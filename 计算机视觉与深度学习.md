# 计算机视觉与深度学习

## 深度学习

### 深度学习介绍

#### 线性整流函数（ReLU函数）

通常意义下，线性整流函数指代数学中的斜坡函数，即
$$
f(x)=max(0,x)
$$
而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出
$$
max(0,W^Tx+b)
$$
到下一层神经元或作为整个神经网络的输出。

#### 神经网络介绍

神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。

#### 监督学习

监督学习也称为带标签的学习方式。监督学习是`从标记的训练数据`来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。

#### 结构化数据vs非结构化数据

结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。

#### 深度学习的准确率

取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。

#### Sigmoid函数

sigmoid函数也叫`Logistic`函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：

优点：平滑、易于求导。

缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现`梯度消失`的情况，从而无法完成深层网络的训练。

Sigmoid函数的公式如下：
$$
S(x)=\frac{1}{1+e^{-x}}
$$
函数图形如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img3.jpg)



### 深度学习基础

> 为了方便学习：
>
> 1.使用$(x,y)$来表示一个单独的样本
>
> 2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in \{0,1\}$代表标签$y$值为0或1
>
> 3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
>
> 4.$m=m_{train}+m_{test}$
>
> 5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&|&&|\\ x^{\left( 1\right)  }&x^{\left( 2\right)  }&\cdots &x^{\left( m\right)  }\\ |&|&&|\end{matrix} \right]  $，$m$是训练集样本的个数。
>
> 6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right)  }&y^{\left( 2\right)  }&\cdots &y^{\left( m\right)  }\end{matrix} \right]  $,$Y\in\R^{1\times m}$

#### Logistic回归

Logistic回归通常用于二元分类问题。

它通常的做法是将`sigmoid函数`作用于线性回归：
$$
\hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}}
$$
这会使得$\hat{y}$的范围在0~1之间

梯度下降法中的`损失函数`如下：
$$
L(\hat{y},y)=\frac12(\hat{y}-y)^2
$$
Logistic回归中使用的`损失函数`如下：
$$
L(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))
$$
当$y=1$时，$L(\hat{y},y)=-y\log\hat{y}$，为了使损失函数较小，$\hat{y}$必须比较大，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于1；当$y=0$，$L(\hat{y},y)=-\log(1-\hat{y})$，$\hat{y}$必须比较小，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于0。

>损失函数是在单个训练样本中定义的，在全体训练样本上的表现是由代价函数来定义的。



代价函数的定义：
$$
\begin{split}
J(W,b)&=\frac{1}{m}\sum^{m}_{i\  =\  1} L\left( \hat{y}^{\left( i\right)  } ,y^{\left( i\right)  }\right) \\&=-\frac{1}{m}\sum^m_{i=1}[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]\quad \quad \quad
\text{其中}\hat{y}^{(i)}\text{代表的是预测值}，y^{(i)}代表的是真实值
\end{split}
$$

#### 梯度下降法

我们可以将梯度下降法用下图来表示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img1.jpg)

梯度下降法所做的事就是从初始点开始让$J(W,b)$朝`最陡的下坡方向`走一步,迭代次数不定。

其中$W$的迭代更新公式如下：
$$
W:=W-\alpha\frac{\partial J(W,b)}{\partial W} \quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial W} 代表该点的W对应的导数
\\b:=b-\alpha\frac{\partial J(W,b)}{\partial b}\quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial b} 代表该点的b对应的导数
$$
这样会使$W$和$b$一步一步得接近使得$J(W,b)$最小的值。

> 那么m个样本的梯度下降如何来表示呢？

其实就是对$J(W,b)$函数分别对$W$和$b$求偏导,得到全局的梯度值。

> $W$和$b$的迭代过程：
>
> 1.对$W$和$b$设定初值，计算$J(W,b)$
>
> 2.通过$J(W,b)$对参数求偏导
>
> 3.使用$W$和$b$的原值减去学习率乘以偏导来迭代更新值
>
> 4.重复1~3步骤

#### 向量化技术

如果不使用向量化技术，在面对巨大的数据集时，你会用非常多的循环去解决迭代的问题，这往往会降低代码运行的速度。向量化技术使得这种计算过程变得更加快速。

比如$f=W^T$,如果$W$有n个维度，不使用向量化一般需要用长度为n的for循环遍历求解，向量化之后则用矩阵来求解，看一段`Python`代码：

```python
import numpy as np
import time

a = np.random.rand(1000000)
b = np.random.rand(1000000)

# 非向量化使用循环
c = 0
tic = time.time()
for i in range(1000000):
  c += a[i]*b[i]
toc = time.time()
print("使用循环做法花费的时间为" + str(1000*(toc - tic)) + "ms")

# 使用向量化技术
tic = time.time()
c = np.dot(a, b)
toc = time.time()
print("使用向量化技术花费的时间为" + str(1000*(toc - tic)) + "ms")

# 运行结果如下：（保留一位小数）
# 使用循环做法花费时间为474.3ms
# 使用向量化技术花费的时间为1.5ms
```

> 我们在编写神经网络的时候，尽量要避免使用for循环

再举个例子：
$$
v=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  ==>u=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  
$$
可以这样编程（尽量使用numpy）:

```python
import numpy as np

# np.random.randint(a, b, size=(c, d)):
# 注：a-b表示生成[a,b]数的范围，后面size表示生成矩阵的大小
n = 10000
v = np.random.randint(10,11,(1,n))

# 原始方法（for循环）
u = np.zero((n,1))
for i in range(n):
  u[i]=math.exp(v[i])
  
# 使用numpy的内置函数，能比原来快很多
u = np.exp(v)
# 同样还有np.log() np.abs() np.maximum(v,0) v**2 1/v
```

使用numpy简易表示Logistic回归的一轮迭代：

```python
# Z = w^T*X+b
Z = np.dot(w.T,X)+b
# A = sigmoid(Z) 
def sigmoid_func(Z):
	return 1/(1+np.exp(-z))
A = sigmoid_func(Z)
dZ = A - Y
dw = 1/m * dZ
db = 1/m * np.sum(dZ)
w = w - a * dw # a代表学习率
b = b - a * db 
```

#### Python中的广播

|            | 苹果 | 牛肉  | 鸡蛋 | 土豆 |
| ---------- | ---- | ----- | ---- | ---- |
| 碳水化合物 | 56.0 | 0.0   | 4.4  | 68.0 |
| 蛋白质     | 1.2  | 104.0 | 52.0 | 8.0  |
| 脂肪       | 1.8  | 135.0 | 99.0 | 0.9  |

求每种食物的每项指标占比：

```python
import numpy as np

A = np.array([[56.0, 0.0, 4.4, 68.0],
              [1.2, 104.0, 52.0, 8.0],
              [1.8, 135.0, 99.0 0.9]])

# 对矩阵进行竖直方向求和
cal = A.sum(axis=0)
# reshape是O(1)操作，放心使用
# 这里的广播是将3*4的矩阵除以1*4的矩阵，然后进行自动广播
percentage = 100*A/cal.reshape(1,4)
```

再举一个特殊的例子：

```python
import numpy as np

A = np.array([[1],
       				[2],
              [3],
              [4]])
# 广播
A = A + 100
print(A)
# 结果：
# [[101]
#  [102]
#  [103]
#  [104]]
```

如上所示，广播的规则如下：

一个$m\times n$的矩阵`加减乘除`一个$1\times n$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times n$的矩阵`加减乘除`一个$m\times 1$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times 1$的矩阵`加减乘除`一个常数，python就会自动把它复制成$m\times 1$的矩阵

一个$1\times m$的矩阵`加减乘除`一个常数，python就会自动把它复制成$1\times m$的矩阵

#### numpy的使用

```python
import numpy as np

# 并不是一个向量，而是一个秩为1的数组
a = np.random.randn(5)
print(a)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(a.shape)
# (5,)
print(a.T)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(np.dot(a,a.T))
# 6.23019719213342

b = np.random.randn(5,1)
print(b)
# [[ 1.83847239]
#  [ 0.43958321]
#  [-0.87437944]
#  [ 0.70296355]
#  [-0.1833722 ]]
print(b.T)
# [[ 1.83847239  0.43958321 -0.87437944  0.70296355 -0.1833722 ]]
print(np.dot(b,b.T))
# [[ 3.37998075  0.8081616  -1.60752246  1.29237908 -0.33712473]
#  [ 0.8081616   0.1932334  -0.38436252  0.30901097 -0.08060734]
#  [-1.60752246 -0.38436252  0.7645394  -0.61465687  0.16033688]
#  [ 1.29237908  0.30901097 -0.61465687  0.49415775 -0.12890397]
#  [-0.33712473 -0.08060734  0.16033688 -0.12890397  0.03362536]]
```

从上面的例子可以看出，我们构建向量时尽量构建b这种类型的向量，不要使用数组，可以避免不必要的错误。



为了我们程序的运行正确，少点bug，可以使用assert声明函数。Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况。语法为`assert (表达式)`。

其中`np.squeeze()`可以将数组变成一个向量。

#### 作业一

使用numpy手写Logistic回归，这里只写回归部分，数据处理部分略过：

两个偏导数公式如下：

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$

$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$

```python
import numpy as np
# ----------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# ----------------------------------
def initialize_with_zeros(dim):
    """
    Argument:
    dim -- 输入数据的维度
    
    Returns:
    w -- 初始化维度为(dim, 1)的向量
    b -- 初始化标量
    """
    w = np.zeros((dim,1))
    b = 0
  
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
  
    return w, b
# ----------------------------------
def propagate(w, b, X, Y):
    """
    代价函数
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
  
    Returns:
    cost -- 代价
    dw -- 损失相对于 w 的梯度，因此维度与 w 相同
    db -- 损失相对于 b 的梯度，因此维度与 b 相同
    """
  
    m = X.shape[1] # 样本数量
    A = sigmoid(np.dot(w.T, X) + b) # 预测值
    cost = -(1.0/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
  
    dw = (1.0/m) * np.dot(X, (A - Y).T)
    db = (1.0/m) * np.sum(A - Y)
  
    assert(dw.shape == w.shape)
    assert(db.shape == b.shape)
    cost = np.squeeze(cost) # 将数组转化为向量（这里为防止bug）
    assert(cost.shape == ())
  
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
# ----------------------------------
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    w和b的迭代优化
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
    num_iterations -- 优化迭代的次数
    learning_rate -- 学习率
    print_cost -- 如果为true，每迭代100次打印一次损失
  
    Returns:
    params -- 包含权重 w 和偏差 b 的字典
    grads -- 包含权重梯度和相对于成本函数的偏差梯度的字典
    costs -- 优化期间计算的所有成本的列表，这将用于绘制学习曲线。
    """
  
    costs = []
    for i in range(num_iterations):
        grads, cost = propagate(w, b, X, Y)
    
        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

        if i % 100:
            costs.append(cost)

        if print_cost and i % 100 == 0:
            print("Cost after iteration %i:%f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads
# ----------------------------------
def predict(w, b, X):
    """
    使用学习的逻辑回归参数 (w, b) 预测标签是 0 还是 1

    Arguments:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 样本数量)维度的数据
    
    Returns:
    Y_prediction -- 一个numpy数组（向量），包含X中示例的所有预测（0/1）
    """
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):
        if A[0, i] > 0.5:
            Y_prediction[0, i] = 1
        else:
            Y_prediction[0, i] = 0
    
    assert(Y_prediction.shape == (1, m))

    return Y_prediction
# ----------------------------------
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    构建逻辑回归模型

    Arguments:
    X_train -- 训练样本 shape:(num_px * num_px * 3, m_train)
    Y_train -- 训练标签 shape:(1, m_train)
    X_test -- 测试样本 shape:(num_px * num_px * 3, m_test)
    Y_test -- 测试标签 shape:(1, m_test)
    num_iterations -- 迭代次数 默认为2000
    learning_rate -- 学习率 默认为0.5
    print_cost -- 是否打印代价
    
    Returns:
    d -- 包含模型信息的字典
    """

    w, b = initialize_with_zeros(X_train.shape[0])
    # 训练
    parameter, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
    # 训练结果
    w = parameter["w"]
    b = parameter["b"]
    # 预测
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)
    # 打印预测结果
    print("训练集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("测试集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    d = {"cost": costs,
         "测试集预测正确个数": Y_prediction_test,
         "训练集预测正确个数": Y_prediction_train,
         "w": w,
         "b": b,
         "学习率": learning_rate,
         "迭代轮数": num_iterations}

    return d
  
## 最后就可以使用model函数对已经经过数据处理的训练集和测试集进行训练和预测了

```

### 神经网络编写

#### 神经网络表示

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img2.jpg)

如上图所示，这是一个`双层神经网络`（一般输入层不作为层数）。最左边为输入层，代表单个样本的输入特征数；中间为隐藏层；最右边为输出层，一般代表预测值。中间的隐藏层是代表特征与预测值关系的一些表达式，类似于机器学习中的$W$和$b$。在这个图中，$W$是一个$4\times 3$的矩阵，$b$是一个$4\times 1$的矩阵，4代表隐藏层的个数，3代表输入的特征。

*需要注意的是这里的W和Logistic中讲的W是不一样的：因为这里的W是指整个隐藏层的W，计算时不用转置（4$\times$

3）；而Logostic中的W相当于只有一个节点的W，且计算时需要转置(3$\times$1)。*

#### 神经网络的计算

将每个隐藏层分开单独和左边的输入层结合在一起看，神经网络其实就是多个类似于Logistic回归的结构。

所以上图隐藏层的计算过程如下：
$$
z^{[1]}_1 = {w^{[1]}_1}^{T}x+b^{[1]}_1,a^{[1]}_1=\sigma(z^{[1]}_1)\\
z^{[1]}_2 = {w^{[1]}_2}^{T}x+b^{[1]}_2,a^{[1]}_2=\sigma(z^{[1]}_2)\\
z^{[1]}_3 = {w^{[1]}_3}^{T}x+b^{[1]}_3,a^{[1]}_3=\sigma(z^{[1]}_3)\\
z^{[1]}_4 = {w^{[1]}_4}^{T}x+b^{[1]}_4,a^{[1]}_4=\sigma(z^{[1]}_4)\\
其中[]里代表的数字是第几层,这里是从隐藏层算起\\下标的值代表的是该层的第几个节点\\
\sigma(z)代表激活函数
$$
所以将上面的双层神经网络整个计算过程合并起来就变成了：
$$
第一层：隐藏层\\
z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[1]}=\sigma(z^{[1]})\\
第二层：输出层\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\
a^{[2]}=\sigma(z^{[2]})
$$

#### 多个样本的向量化

上面讲的计算过程是单个样本的计算过程，如果是多个样本，就要使用一个循环来计算。但是前面讲过样本的遍历可以使用向量化技术来加快运算速度。

所以我们把z和a关于样本的多个列合并在一起：
$$
Z^{[1]}=[z^{[1](1)},z^{[1](2)},\dots z^{[1](m)}]\\
A^{[1]}=[a^{[1](1)},a^{[1](2)},\dots a^{[1](m)}]\\
Z^{[2]}=[z^{[2](1)},z^{[2](2)},\dots z^{[2](m)}]\\
A^{[2]}=[a^{[2](1)},a^{[2](2)},\dots a^{[2](m)}]\\
m代表样本的数量,[]的值代表不同的层，行代表不同的节点(也叫隐藏单元)，列代表不同的样本
$$
所以计算过程变为了：
$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})\\
这里的b不需要变是因为python自带的广播技术
$$

#### 多种激活函数

上面我们使用的激活函数为$\sigma(z)$也就是`sigmoid函数`。现在我们要介绍多种激活函数来进行对比：

* $tanh(z)$:

$$
a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\quad \quad 它的取值范围在[-1,1]\\
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img4.jpg)

该函数的特点是所有的数据平均值接近0，如果需要进行该种数据中心化可以使用该函数。

通常来说激活函数选取$tanh(z)$都比使用`sigmoid`函数更好。但有一个例外是输出层，输出层经常使用`sigmoid函数`，或者使用二元分类时，使用`sigmoid函数`。为了表示不同的层之间使用不同的激活函数，我们通常会将激活函数用$g$来表示，使用$g^{[i]}$表示第i层的激活函数。

`sigmoid函数`和$tanh(z)$函数共同的缺点是当$z$的值很大或者很小的时候，函数的斜率很接近0，也就是我们经常会说的梯度消失，拖慢梯度下降算法。

* ReLU：

$$
a=max(0,z)
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img5.jpg)

* Leaky ReLU：
  $$
  a=max(cz,z)\quad \quad  c在这里是一个常数，通常取一个比较小的数，比如0.01或者0.001
  $$
  图像如下：

  ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img6.jpg)

  ReLU的缺点是，当$z$的值为负数的时候，它没有导数值。而Leaky ReLU解决了这个问题。

> 激活函数如何选择？

激活函数的选择经验：

1.如果你在做二元分类时，输出一般为0或1，那么该网络的输出层激活函数选择sigmoid函数较好，其他所有单元都使用ReLU函数。使用ReLU函数最大的好处就是梯度下降比较快，也就是收敛的比较快。

2.有时候特定情况下会使用tanh(z)函数。

3.ReLU函数是最常用的激活函数

> 为什么神经网络需要使用激活函数？

我们来做一个公式推导：
$$
如果不使用激活函数:\\
a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})\\
=W^{'}x+b^{'}
$$
可以看到如果不使用激活函数，无论你使用多庞大的神经网络，都始终在做线性激活函数，这就退化成了线形回归的内容。

#### 激活函数的导数

当你使用神经网络进行反向传播时，需要计算激活函数的斜率或者导数。

* sigmoid函数

$$
a=g(z)=\frac{1}{1+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))=a(1-a)
$$

* tanh函数

$$
a=g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=1-(\frac{e^z-e^{-z}}{e^z+e^{-z}})^2=1-g(z)^2=1-a^2
$$

* ReLU

$$
a=g(z)=max(0,z)\\
g^{'}(z)=\begin{cases}
0,& \text{如果}z<0\\
1,& \text{如果}z>0\\
undefined,&\text{如果}z=0
\end{cases}
$$

* Leaky ReLU

$$
g(z)=max(0.01z,z)\\
g^{'}(z)=\begin{cases}
0.01& \text{如果}z<0\\
1& \text{如果}z>0
\end{cases}
$$

#### 神经网络的梯度下降法

以单隐藏层为例，写出它们的正向传播和反向传播的过程：

* 正向传播：

$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})
$$

* 反向传播：

$$
dZ^{[2]}=A^{[2]}-Y\\
dW^{[2]}=\frac{1}{m}dZ^{[2]}{A^{[1]}}^T\\
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\\
dZ^{[1]}={W^{[2]}}^TdZ^{[2]}*{g^{[1]}}^{'}(Z^{[1]})\\
这里的*代表逐个元素乘积\\
dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T\\
db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)
$$

> np.sum的axis不同时：
>
> 1.np.sum(axis = 0)代表矩阵最外维度相加（如果最外维度为n，可以理解为n个二维矩阵直接相加）
>
> 2.np.sum(axis = 1)代表矩阵中间维度相加（相当于是二维矩阵内部对每列求和）
>
> 3.np.sum(axis = 2)代表矩阵最内维度相加（相当于是二维矩阵内部对每行求和）
>
> keepdims=True是为了保证不输出秩为1的数组

#### 随机初始化

在Logistic回归中，我们把$w$和$b$都初始化为0向量，在神经网络中$W$不能这么初始化为0矩阵。因为这样会导致第一层在做计算时，每个隐藏单元所做的计算都是一模一样的，在反向传播时，不同隐藏单元激活函数的导数$dz^{[1]}_1$和$dz^{[1]}_2$是一样的。

我们的做法一般是对$W$随机初始化:
$$
W^{[1]}=np.random.randn((x,y))\times 0.01\\
b^{[2]}=np.zeros((y,1))\\
0.01代表权重，一般取比较小的值,这样能使梯度下降更快一些\\这在使用sigmoid作为激活函数的网络更为明显（z值太大,导数接近0）；\\x代表输入层的特征数；\\y代表隐藏层的隐藏单元数目。
$$

#### 作业二

写一个双层神经网络(没有数据处理的部分)：

```python
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# -----------------------------------------
def layer_sizes(X, Y):
    """
    Arguments:
    X -- 输入数据 (输入层大小, 样本数量)
    Y -- 标签 (输出层大小, 样本数量)
    
    Returns:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小
    """

    n_x = X.shape[0] # 输入层的大小
    n_h = 4
    n_y = Y.shape[0] # 输出层的大小

    return (n_x, n_h, n_y)


# -----------------------------------------
def initialize_parameters(n_x, n_h, n_y):
    """
    Arguments:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小

    Returns:
    params -- 初始化参数的字典:
              W1 -- weight matrix of shape (n_h, n_x)
              b1 -- bias vector of shape (n_h, 1)
              W2 -- weight matrix of shape (n_y, n_h)
              b2 -- bias vector of shape (n_y, 1)
    """

    np.random.seed(2) # 设置随机种子

    W1 = np.random.randn((n_h, n_x))
    b1 = np.zeros((n_h, 1))
    W2 = np.random.rand((n_y, n_h))
    b2 = np.zeros((n_y, 1))

    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def forward_propagation(X, parameters):
    """
    前向传播计算

    Argument:
    X -- 输入数据 (n_x, m)
    parameters -- 初始化参数的字典 (output of initialization function)
    
    Returns:
    A2 -- 第二层sigmoid激活函数输出的结果
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    """    

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1) # 隐藏层使用tanh激活函数
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2) # 输出层使用sigmoid激活函数

    assert(A2.shape == (1, X.shape[1])) # X.shape[1]代表样本数量

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache

# -----------------------------------------
def compute_cost(A2, Y, parameters):
    """
    计算代价函数 (13)
    
    Arguments:
    A2 -- 第二层sigmoid激活函数输出的结果 维度(1, number of examples)
    Y -- 正确的标签 维度(1, number of examples)
    parameters -- 初始化参数的字典 W1, b1, W2 and b2
    
    Returns:
    cost -- 代价函数结果
    """

    m = Y.shape[1] # 样本数量

    # 计算代价函数
    # np.multiply(X, Y)是指X和Y对应位置两两相乘
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
    cost = -np.sum(logprobs) / m

    cost = np.squeeze(cost) # 确保代价为我们期望的维度
    
    # isinstance() 函数来判断一个对象是否是一个已知的类型
    assert(isinstance(cost, float))

    return cost

# -----------------------------------------
def backward_propagation(parameters, cache, X, Y):
    """
    后向传播
    
    Arguments:
    parameters -- 参数初始化字典 
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    X -- 输入数据 维度(2, number of examples)
    Y -- 正确标签 维度(1, number of examples)
    
    Returns:
    grads -- 参数渐变的字典
    """

    m = X.shape[1]

    W1 = parameters["W1"]
    W2 = parameters["W2"]

    A1 = cache["A1"]
    A2 = cache["A2"]

    # 后向传播计算
    # tanh()函数的导数为 g'(a) = 1 - a^2
    
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # 1-np.power(A1, 2)为tanh的导数
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True)/ m

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads

# -----------------------------------------
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    中间权重向量更新
    
    Arguments:
    parameters -- 更新前的参数 
    grads -- 用于参数更新的逆向传播参数
    
    Returns:
    parameters -- 更新后的参数
    """

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]

    # 权重向量更新
    W1 = W1 - dW1 * learning_rate
    b1 = b1 - db1 * learning_rate
    W2 = W2 - dW2 * learning_rate
    b2 = b2 - db2 * learning_rate

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- 循环迭代的次数
    print_cost -- 如果为True,每1000次打印一次代价
    
    Returns:
    parameters -- 训练好的参数，用于预测
    """

    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]

    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    # 循环
    for i in range(0, num_iterations):
        # 计算前向传播
        A2, cache = forward_propagation(X, parameters)
        
        # 计算代价
        cost = compute_cost(X, parameters)

        # 计算后向传播
        grads = backward_propagation(parameters, cache, X, Y)

        # 更新权向量
        parameters = update_parameters(parameters, grads) # 学习率直接设置为默认值1.2

        if print_cost and (i % 1000):
            print("第i次迭代之后的代价为:" + str(cost))

    return parameters


# ----------------------------------------- 
def predict(parameters, X):
    """
    通过训练好的权重来预测X的类型
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- 输入数据 维度 (n_x, m)
    
    Returns
    predictions -- 模型预测的结果 (red: 0 / blue: 1)
    """

    A2, cache = forward_propagation(X, parameters)
    prediction = (A2 > 0.5) # sigmoid函数的判别方式

    return prediction

# -----------------------------------------
# 使用：

# 构建双层神经网络模型
parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)

# 绘制决策边界
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0, :])
plt.title("Decision Boundary for hidden layer size " + str(4))
```

### 多层的深层神经网络

#### 神经网络的表示

1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4

2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量

3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$

#### 深层网络中的前向传播

神经网络中每层的前向传播过程：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})\\
l代表层数
$$
**如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。**

#### 核对矩阵中的维数

如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。

通过一个具体的网络来手动计算一下维度：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img7.jpg)

可以写出该网络的部分参数如下：
$$
n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1
$$
由于前向传播的公式为：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})
$$

> 需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。

1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。

2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$

所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。

*可以总结出来的是：*
$$
W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\\
b^{[l]}的维度一定是n^{[l]}\times 1
$$
*同理在反向传播时：*
$$
dW和W的维度必须保持一致，db必须和b保持一致
$$
因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。

#### 参数vs超参数

参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$

超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。

#### 作业三

一个神经网络工作原理的模型如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img8.jpg)

多层网络模型的前向传播和后向传播过程如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img9.jpg)

* 实现一个L层神经网络

```python
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward
import numpy as np


# ------------------------------------------
def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- 包含网络中每一层的维度的Python List
    
    Returns:
    parameters -- Python参数字典 "W1", "b1", ..., "WL", "bL":
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims) # 网络层数

    for i in range(1, L):
        parameters["W" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01
        parameters["b" + str(i)] = np.zeros(layer_dims[i], 1)

        assert(parameters["W" + str(i)].shape == (layer_dims[i], layer_dims[i - 1]))
        assert(parameters["b" + str(i)].shape == (layer_dims[i], 1))

    return parameters

# ------------------------------------------
def linear_forward(A, W, b):
    """
    实现前向传播的线性部分

    Arguments:
    A -- 来自上一层的激活结果 (或者为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)

    Returns:
    Z -- 激活函数的输入或称为预激活参数 
    cache -- Python参数字典包含"A", "W" and "b" ; stored for computing the backward pass efficiently
    """

    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))

    cache = (A, W, b)

    return Z, cache

# ------------------------------------------
def linear_activation_forward(A_prev, W, b, activation):
    """
    实现 线性——>激活层 的前向传播

    Arguments:
    A_prev -- 来自上层的激活结果 (或为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)
    activation -- 当前隐藏层使用的激活函数: "sigmoid" or "relu"

    Returns:
    A -- 激活函数的输出,也称为激活后值 
    cache -- Python字典包含 "线性缓存" and "激活缓存";
             stored for computing the backward pass efficiently
    """

    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    assert(A.shape == (W.shape[0], A.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache

# ------------------------------------------
# 为了实现L层神经网络更加方便，需要将前L-1层的激活函数设置为ReLU，最后一层输出层激活函数设置为Sigmoid
def L_model_forward(X, parameters):
    """
    实现前向传播： the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    
    Arguments:
    X -- 初始数据, numpy array of shape (输入层大小, 样本数量)
    parameters -- 初始化deep网络的参数输出
    
    Returns:
    AL -- 上一层激活后的值
    caches -- cache的列表:
                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """

    caches = []
    A = X
    L = len(parameters) // 2
    # 前L-1层为relu激活函数
    for i in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters["W" + str(i)], parameters["b" + str(i)], "relu")
        caches.append(cache)
    # 最后一层为sigmoid激活函数
    AL, cache = linear_activation_forward(A, parameters["W" + str(L)], parameters["b" + str(L)], "sigmoid")
    caches.append(cache)

    assert(AL.shape == (1, X.shape[1]))

    return AL, caches

# ------------------------------------------
def compute_cost(AL, Y):
    """
    计算代价函数 使用Logistic回归中使用的代价函数

    Arguments:
    AL -- 对应于标签的预测概率向量, shape (1, 样本数)
    Y -- 正确的样本 (for example: containing 0 if non-cat, 1 if cat), shape (1, 样本数)

    Returns:
    cost -- 交叉熵代价
    """ 

    m = Y.shape[1]
    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), (1 - Y).T)) / m

    cost = np.squeeze(cost) # 使得cost的维度是我们想要的（比如将[[17]]变成17）
    assert(cost.shape == ())

    return cost

# ------------------------------------------
def linear_backward(dZ, cache):
    """
    单层实现反向传播的线性部分(l层)

    Arguments:
    dZ -- 代价函数对于线性输出的梯度 (l层)
    cache -- 元组(A_prev, W, b) 来自当前层的前向传播

    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m 
    dA_prev = np.dot(W.T, dZ)

    assert(dA_prev.shape == A_prev.shape)
    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)

    return dA_prev, dW, db

# ------------------------------------------
def linear_activation_backward(dA, cache, activation):
    """
    实现 线性——>激活 过程的反向传播
    
    Arguments:
    dA -- 当前层l激活后的梯度
    cache -- 元组 (linear_cache, activation_cache) 为了有效计算后向传播而存储
    activation -- 当前层的激活函数, stored as a text string: "sigmoid" or "relu"
    
    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度,和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db

# ------------------------------------------
def L_model_backward(AL, Y, caches):
    """
    前L-1层为ReLU激活函数,最后一层为sigmoid函数的后向传播实现
    
    Arguments:
    AL -- 前向传播输出的概率向量 (L_model_forward())
    Y -- 真实值的向量 (containing 0 if non-cat, 1 if cat)
    caches -- 包含cache的列表:
                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
    
    Returns:
    grads -- 带有渐变值的字典
             grads["dA" + str(l)] = ... 
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ... 
    """

    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # 经过这一行转化，Y的维度和AL维度相同

    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    current_cache = caches[L - 1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation="sigmoid")

    for i in reversed(range(L-1)):
        current_cache = caches[i]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(i + 2)], current_cache, activation="relu")
        grads["dA" + str(i + 1)] = dA_prev_temp
        grads["dW" + str(i + 1)] = dW_temp
        grads["db" + str(i + 1)] = db_temp

    return grads

# ------------------------------------------
def update_parameters(parameters, grads, learning_rate):
    """
    使用梯度下降更新参数
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients, output of L_model_backward
    
    Returns:
    parameters -- 更新后的参数字典
                  parameters["W" + str(l)] = ... 
                  parameters["b" + str(l)] = ...
    """   

    L = len(parameters) // 2 # 神经网络中的层数

    for i in range(1, L + 1):
        parameters["W" + str(i)] -= learning_rate * grads["dW" + str(i)]
        parameters["b" + str(i)] -= learning_rate * grads["db" + str(i)]

    return parameters



```



### 有效运行神经网络

深度学习网络是一个需要迭代得到结果的模型。它的超参数调整过程：想法->编码->实验->修改想法，需要不断地尝试，才能学习到调参的经验。

#### 训练集和测试集划分

我们一般将数据分为三个部分：

1.训练集：为训练模型准备的数据

2.验证集：通过交叉验证集选择最优模型

3.测试集：对模型进行评估

> 划分训练测试集最常见的比例是什么？

如果明确指定验证集，一般训练和测试集的比例为7:3；如果指定验证集，那么一般比例为6:2:2。这一般在数据量在10000条以下都是最好的划分比例。但在大数据时代，如果数据总量比较大，比如是百万条，那么验证集和测试集的比例还需要减少，比如98:1:1，也是合理的。

*需要特别注意的是：要保证验证集和测试集来自同一分布，这样会使得你的机器学习算法变得更快。*

#### 偏差和方差

根据数据集的分布状况，可以分为以下三种：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img10.jpg)

如上图，最左边使用Logistic回归，没有大部分正确分类，属于“欠拟合”的状况；最右边使用比较复杂的神经网络，完整地分出了两个类别，属于”过拟合“的情况（因为部分输入数据是不合理的）；中间只有和别数据分类错误，这叫“适度拟合”，是我们比较追求的一种状态。

上图这种只有一个或者两个特征的二维数据中，可以绘制数据，将偏差和方差可视化。但在多维空间数据中，绘制数据和可视化分割边界无法实现。

> 我们在多维空间通常通过两个个指标,来研究偏差和方差：训练集误差和测试集误差。为了便于研究，假设人眼分辨的错误率为0，这也被称为基本误差或者最优误差；假设训练集和验证集来自同一分布。如果训练集误差为0.01，测试集误差为0.11，这种情况很有可能是我们过度拟合了训练集，称之为高方差，对应于上图最右边的情况；如果训练集误差为0.15，测试集误差为0.16，这种情况很有可能是我们训练数据的时候欠拟合，称之为高偏差，对应于上图最左边的情况；如果训练集误差为0.15，测试集误差为0.3，偏差和方差都比较高，这种情况是因为你的算法模型并不适合这个任务，需要改变模型；如果训练集误差为0.005，测试集误差为0.01，偏差和方差都很低，是分类效果比较好的情况，对应上图中间的情况。

#### 模型评估调优的过程

* 首先，将进行训练之后的模型用于评估训练集的性能，如果误差高，代表欠拟合。那么你要采取的方法可能是增加训练时间或者增大网络结构或者是选择一个新网络，先将偏差降下来，拟合训练数据。（这在基本误差不同的情况下会有所不同）
* 如果模型的训练集性能很高后，可以将其用于评估测试集，如果误差高，代表过拟合。那么你要采取的方法最好是采用更多的训练数据，如果无法获得更多数据，可以通过正则化来减少过拟合，降低方差，有时候也会通过替换网络结构来实现。

#### 正则化

如果你的模型在评估是，由比较高的方差，然而你又不能拿到更多的数据集，那么我们最先想到的办法可能是正则化。

正则化有助于避免数据过度拟合，减少网络误差。

* Logistic正则化

假设我们的目标是找到$w,b$来使得$J(w,b)$达到最小值，且使用逻辑回归的代价函数，那么
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})
$$
我们在该函数中加入正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert_2}^2\\
{\Vert w \Vert_2}^2=\sum^{n_x}_{j=1}w_{j}^2=w^Tw \quad 这被称为L2正则化
$$
同理也会有L正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}{\Vert w \Vert_1}\\
{\Vert w \Vert_1}=\sum^{n_x}_{j=1}\lvert w_j \rvert \quad 这被称为L1正则化
$$
通常我们都会使用`L2正则化`来实现降低方差的效果。

>那么$\lambda$的值我们需要如何确定呢？

我们通常使用验证集或者交叉验证来配置$\lambda$参数，不过首先要考虑训练集之间的权衡，把参数$w,b$正常值设为较小的值，避免过拟合，不断调整超参数$\lambda$的值来减小方差。

*需要特别说明的是在编写代码的时候，python语言中lambda是一个保留字段，所以编程时我们通常使用lambd来代替lambda。*

* 神经网络正则化

$$
J(W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]})=\frac1m\sum^n_{i=1}L(\hat{y}^{(i)},y^{(i)})+ \frac{\lambda}{2m}\sum^L_{l=1}{\Vert W^{[l]} \Vert}^2\\
{\Vert W^{[l]} \Vert}^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W_{i,j}^{[l]})^2\quad 该矩阵范数被称为是弗罗贝尼乌斯范数\\
W的维度是(n^{[l]},n^{[l-1]})
$$

如果$J(W,b)$发生了改变，那么反向传播的$dW^{[l]}$也要发生变化：

在`backprop`计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值，这样做的结果是的$W^{[l]}$会比没有正则化之前更小。因此`L2范数`也被称为`权重衰减`。

#### 正则化如何预防过拟合

如果正则化的参数$\lambda$如果设置的过大，$W^{[l]}$会变得更小，就会导致每层上的部分$w$的权重 会接近0，这相当于将部分隐藏单元给去除了，复杂的神经网络会退化称为一 个很深但是又很简单的网络，导致从过拟合直接变成欠拟合的状态。

 一个合适的$\lambda$值能够使模型的性能从过拟合到适度拟合。

>$\lambda$越大，得到迭代的$W$就越小，这相当于让部分隐藏单元的影响变小，降低模型的拟合程度，方差减小。 
>
>总体来说，正则化的效果其实就是将复杂的网络线性化，如果$\lambda$的取值设置的比较好，能达到适度拟合的效果。

#### dropout正则化

`dropout正则化`是指如果一个网络存在过拟合的情况下，可以将所有节点（隐藏单元）设置一个删除概率为0.5，那么保留该隐藏单元的概率`keep-prob`也为0.5，这个值是可以改变的，然后随机消除一些节点并删除该节点进出的连线，得到一个节点更少，规模更小的网络。如下图所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img11.jpg)

实现Dropout的方法很多，最常用的是`inverted dropout`。以一个深层神经网络的某一个隐藏层为例来解释怎么进行Dropout正则化。首先假设对于第$l$层，其激活函数为$a^{[l]}$，我们设置的保留概率`keep_prob`等于0.8，这意味着该隐藏层的所有神经元以0.8的概率得到保留。

可以将`inverted dropout`方法归纳为四步：

>1.根据`keep_prob`生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，`Dl = np.rndom.randn(Al.shape[0], Al.shape[1])`
>
>2.将$d^{[l]}$转化为0-1矩阵， `Dl = Dl < keep_prob`
>
>3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， `Al = np.muiltiply(Al, Dl)`
>
>4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以`keep_prob`,`Al = Al / keep_prob`

需要注意的是，在反向传播的时候，也需要像上面一样进行dropout操作，和前向传播关闭相同的神经元。即对于某一层的$da^{[l]}$，应该进行以下计算：

> 1.`dAl = dAl * Dl` 
>
> 2.`dAl = dAl / keep_prob`

另一点是，Dropout正则化只在训练阶段实施，在测试阶段只需要利用训练好的参数进行正向预测，而不需要进行神经元的随机失活。

三层网络的前向传播`dropout`示例代码：

```python
keep_prob = 0.8
def foward(X):
    # 3层neural network的前向传播
    A1 = np.maximum(0, np.dot(W1, X) + b1)  # 计算第一层网络的输出
    D1 = (np.random.rand(*A1.shape) < keep_prob)  # 以keep_prob为标准，判断该层结点哪些可以保留
    Z1 = np.multiply(A1, D1)     #dropout
    Z1 /= keep_prob   # 为了期望的一致，除以keep_prob

    A2 = np.maximum(0, np.dot(W2, Z1) + b2)
    D2 = (np.random.rand(*A2.shape) < keep_prob)
    Z2 = np.multiply(A2, D2) 
    Z2 /= keep_prob

    out = np.dot(W3, Z2) + b3

```

那么我们如何在不同的层设置不同的`keep_prob`，我们可以在不太会发生过拟合现象的地方设置`keep_prob`为1，在容易发生过拟合的地方将`keep_prob`设置的低一点。

使用`drop_out`正则化的缺点是不能定义明确的代价函数，那么我们使用的方法一般是先将`drop_out`关闭，确保该网络的代价函数是递减的，再打开`drop_out`进行学习。

#### 数据扩容

在我们数据不够的情况下，可以在已有的数据的基础上将数据进行稍作改变来增加数据集。举个例子，一张包含猫咪的图片，我们可以将其扩容为几张：

> 1.将猫的图片进行水平翻转，得到一张新的图片
>
> 2.将猫的图片进行局部放大，并进行裁剪

*但是最重要的一点就是经过调整后的图片，必须确保它还是一只猫。*

如果输入的是一个数字，我们可以将这些数字进行轻微扭曲，旋转，将其加入数据集。

* Early stopping：一种防止过拟合的方法。

使用该方法可以绘制训练集的误差和验证集的误差，可以用来预防过拟合。

>L2正则化和early stopping的对比：
>
>1.L2正则化训练神经网络的时间可能很长。这导致超参数搜索空间更容易分解
>
>2.L2正则化可能许多正则化参数$\lambda$的值，计算代价太高。而early stopping只运行一次梯度下降，你就可以找到W的最大值，中间值，最小值，无需多次迭代。

#### 归一化输入

如图，给定输入数据的散点图：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img12.jpg" style="zoom:67%;" />

归一化需要两个步骤：

* 1.零均值化

$$
u=\frac1m\sum_{i=1}^mX^{(i)}\\
x:=X-u
$$

意思是移动训练集，直至它完成零均值化，结果如下：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img13.jpg" style="zoom:67%;" />

* 2.归一化方差

如上图可以看到，特征$x_1$的方差比特征$x_2$大得多。使用如下公式迭代：
$$
\sigma^2=\frac1m\sum_{i=1}^mx^{(i)}**2\\
x/=\sigma^2
$$
结果如下：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img14.jpg" style="zoom:67%;" />

*需要注意的是：我们在训练集和验证集或者测试集上需要设置相同的$u$和$\sigma^2$*

*那么为什么要使用归一化呢？*

归一化会使得不同的特征的起始$W$范围较为接近，使得它们的代价函数有如下图的转变：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img15.jpg)

这样可以减少迭代，使得梯度下降法使用更大的步长。

#### 梯度消失和梯度爆炸

在深度神经网络中梯度是不稳定的，可能会变得非常小，也可能会变得非常大，这就是梯度消失和梯度爆炸。

假设$W^{[1]}$比单位矩阵稍微大一点，那么在深度神经网络中，激活函数将会是指数级增长 ；相反，假设$W^{[1]}$比单位矩阵稍微小一些，激活函数会呈现指数级下降的趋势。

那么如何解决如上的问题呢？

一般使用权重初始化来解决：$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{1}{n^{[l-1]}})$，其中$shape$代表该层$W$矩阵的维度，$n^{[l-1]}$代表上一层。如果激活函数使用了`ReLU`激活函数，$\frac{1}{n^{[l-1]}}$可以变成$\frac{2}{n^{[l-1]}}$效果更好；如果使用`tanh`激活函数，使用$\frac{1}{n^{[l-1]}}$；有时候也会看到使用$np.sqrt(\frac{1}{n^{[l-1]}+n^{[l]}})$。

#### 梯度检验

关于梯度的数值逼近，一般使用`双边误差`公式，即
$$
f^{'}(\theta)=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}
$$
我们通常会通过`梯度检验`来验证backprop过程的正确实施：

首先，我们需要将$W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]}$重新组合成为一个大的向量$\theta$；同样的，在反向传播的过程也需要将$dW^{[1]},db^{[1]},\dots,dW^{[L]},db^{[L]}$重新组合成为一个大的向量$d\theta$。
$$
J(\theta)=J(\theta_1,\theta_2,\dots)
$$
然后对于每个$i$：
$$
{d\theta_{approx}}^{[i]}=\lim_{\epsilon\rightarrow0}\frac{J(\theta_1,\theta_2,\dots,\theta_{i+\epsilon},\dots)-J(\theta_1,\theta_2,\dots,\theta_{i-\epsilon},\dots)}{2\epsilon}\approx d\theta^{[i]}=\frac{\partial J}{\partial \theta_i}
$$
做完计算之后，需要做的就是验证是否：
$$
d\theta_{approx}\approx d\theta
$$
如果上述两个量差值的二范数在$10^{-7}$量级，这代表导数逼近很有可能是正确的；如果在$10^{-5}$，就需要担心是否是正确的；如果在$10^{-3}$，那说明你的梯度下降传播程序出现了bug。

> 注意事项：
>
> 1.梯度检验是非常耗费时间的，在训练的时候不使用梯度检验，只有在debug的时候使用。
>
> 2.如果算法的梯度检验失败，要检查每一项，找出bug。
>
> 3.如果代价函数包含了正则项$\frac{\lambda}{2m}{\Vert w \Vert_2}^2$，那么$d\theta_{approx}$也需要多加一个正则项。
>
> 4.梯度检验和dropout正则化不能同时使用。所以在梯度检验的时候，需要先关闭dropout正则化。

#### 作业四

* N维梯度检验：

其工作原理如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img16.jpg)

前向传播和后向传播示例代码：

```python
import numpy as np
from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector

#-------------------------------------------------
def forward_propagation_n(X, Y, parameters):
    """
    Implements the forward propagation (and computes the cost) presented in Figure 3.
    
    Arguments:
    X -- training set for m examples
    Y -- labels for m examples 
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                    W1 -- weight matrix of shape (5, 4)
                    b1 -- bias vector of shape (5, 1)
                    W2 -- weight matrix of shape (3, 5)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    
    Returns:
    cost -- the cost function (logistic cost for one example)
    """
    
    # retrieve parameters
    m = X.shape[1]
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    # Cost
    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)
    cost = 1./m * np.sum(logprobs)
    
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    
    return cost, cache
  
#-------------------------------------------------
def backward_propagation_n(X, Y, cache):
    """
    Implement the backward propagation presented in figure 2.
    
    Arguments:
    X -- input datapoint, of shape (input size, 1)
    Y -- true "label"
    cache -- cache output from forward_propagation_n()
    
    Returns:
    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T) * 2
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,
                 "dA2": dA2, "dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dA1": dA1, "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
```

接下去就需要进行梯度检验了！其中涉及的字典转vector的原理图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img17.jpg)

梯度检验的代码如下：

```python
def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n
    
    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
    x -- input datapoint, of shape (input size, 1)
    y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    
    # Set-up variables
    parameters_values, _ = dictionary_to_vector(parameters)
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0]
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))
    
    # Compute gradapprox
    for i in range(num_parameters):
        
        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have to outputs two parameters but we only care about the first one
        ### START CODE HERE ### (approx. 3 lines)
        epsilon = 0.01
        thetaplus = np.copy(parameters_values)                                      # Step 1
        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2
        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   # Step 3
        ### END CODE HERE ###
        
        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        ### START CODE HERE ### (approx. 3 lines)
        thetaminus = np.copy(parameters_values)                                     # Step 1
        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2        
        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  # Step 3
        ### END CODE HERE ###
        
        # Compute gradapprox[i]
        ### START CODE HERE ### (approx. 1 line)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2*epsilon)
        ### END CODE HERE ###
    
    # Compare gradapprox to backward propagation gradients by computing difference.
    ### START CODE HERE ### (approx. 1 line)
    # np.linalg.norm()的作用是求二范数
    numerator = np.linalg.norm(grad)                                           # Step 1'
    denominator = np.linalg.norm(gradapprox)                                         # Step 2'
    difference = np.linalg.norm(grad - gradapprox) / (numerator + denominator)                                          # Step 3'
    ### END CODE HERE ###

    if difference > 1e-7:
        print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
    else:
        print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")
    
    return difference
```

* 权重初始化解决梯度消失和梯度爆炸问题

> 下列代码会通过三种初始化的方式进行对比：
>
> 1.将W和b都设置为0向量
>
> 2.随机设置W和b
>
> 3.在随机设置的基础上进行权重初始化

代码如下：

加载初始数据：

```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation
from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# load image dataset: blue/red dots in circles
train_X, train_Y, test_X, test_Y = load_dataset()
```

结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img18.jpg)

初始化模型：

```python
def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
    learning_rate -- learning rate for gradient descent 
    num_iterations -- number of iterations to run gradient descent
    print_cost -- if True, print the cost every 1000 iterations
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
    Returns:
    parameters -- parameters learnt by the model
    """
        
    grads = {}
    costs = [] # to keep track of the loss
    m = X.shape[1] # number of examples
    layers_dims = [X.shape[0], 10, 5, 1]
    
    # Initialize parameters dictionary.
    if initialization == "zeros":
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == "random":
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == "he":
        parameters = initialize_parameters_he(layers_dims)

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        a3, cache = forward_propagation(X, parameters)
        
        # Loss
        cost = compute_loss(a3, Y)

        # Backward propagation.
        grads = backward_propagation(X, Y, cache)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
            costs.append(cost)
            
    # plot the loss
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```

W为0向量初始化：

```python
def initialize_parameters_zeros(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    
    for i in range(1, L):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.zeros((layers_dims[i], layers_dims[i - 1]))
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###
    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_zeros([3,2,1])
parameters = model(train_X, train_Y, initialization = "zeros")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
def plot_decision_boundary(model, X, y):
    #import pdb;pdb.set_trace()
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    y = y.reshape(X[0,:].shape)#must reshape,otherwise confliction with dimensions
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)
    plt.show()

#---------------------------------------------
plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img19.jpg)

W为随机向量初始化：

```python
def initialize_parameters_random(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
    parameters = {}
    L = len(layers_dims)            # integer representing the number of layers
    
    for i in range(1, L):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * 10
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###

    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_random([3, 2, 1])
parameters = model(train_X, train_Y, initialization = "random")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
plt.title("Model with large random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img20.jpg)

W为权重初始化后的结果，因为这里使用的ReLU激活函数，W在初始化后需要变为$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{2}{n^{[l-1]}})$：

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1 # integer representing the number of layers
     
    for i in range(1, L + 1):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.random.randn(layers_dims[i],layers_dims[i - 1]) * np.sqrt(2.0 / layers_dims[i - 1])
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###
        
    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_he([2, 4, 1])
parameters = model(train_X, train_Y, initialization = "he")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
plt.title("Model with He initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img21.jpg)

**可以看到第三种的分类效果最好！**

* L2正则化和dropout正则化的效果问题

加载数据：

```python
import numpy as np
import matplotlib.pyplot as plt
from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec
from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters
import sklearn
import sklearn.datasets
import scipy.io
from testCases import *

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

train_X, train_Y, test_X, test_Y = load_2D_dataset()
```

数据的分布如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img27.jpg)

使用无正则化模型，进行梯度下降：

```python
# 这里其实已经包含了L2正则化和dropout正则化的情况，只默认情况下的参数不使用正则化
def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
    learning_rate -- learning rate of the optimization
    num_iterations -- number of iterations of the optimization loop
    print_cost -- If True, print the cost every 10000 iterations
    lambd -- regularization hyperparameter, scalar
    keep_prob - probability of keeping a neuron active during drop-out, scalar.
    
    Returns:
    parameters -- parameters learned by the model. They can then be used to predict.
    """
        
    grads = {}
    costs = []                            # to keep track of the cost
    m = X.shape[1]                        # number of examples
    layers_dims = [X.shape[0], 20, 3, 1]  # 网络结构为三层网络结构，分别有20，3，1个节点 
    
    # Initialize parameters dictionary.
    parameters = initialize_parameters(layers_dims)

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
        elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)
        
        # Cost function
        if lambd == 0:
            cost = compute_cost(a3, Y)
        else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)
            
        # Backward propagation.
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache)
        elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
        elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 10000 iterations
        if print_cost and i % 10000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
        if print_cost and i % 1000 == 0:
            costs.append(cost)
    
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (x1,000)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
  
  
#----------------------------------------------------
parameters = model(train_X, train_Y)
print ("On the training set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model without regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

训练集准确率为0.948，测试集准确率为0.915。分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img28.jpg)

使用L2正则化，前向传播公式变化如下：

$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}$$
To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$

在编程的时候，需要使用`np.sum(np.square(W^[l]))`，然后将所有项加起来，乘上$\frac{\lambda}{2m}$

后向传播时，在`backprop`计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值。

```python
# 前向传播
def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost
    
    ### START CODE HERE ### (approx. 1 line)
    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2*m)
    ### END CODER HERE ###
    
    cost = cross_entropy_cost + L2_regularization_cost
    
    return cost
  
#----------------------------------------------------
# 后向传播
def backward_propagation_with_regularization(X, Y, cache, lambd):
    """
    Implements the backward propagation of our baseline model to which we added an L2 regularization.
    
    Arguments:
    X -- input dataset, of shape (input size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation()
    lambd -- regularization hyperparameter, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    
    ### START CODE HERE ### (approx. 1 line)
    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd * W3 / m
    ### END CODE HERE ###
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd * W2 / m
    ### END CODE HERE ###
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW1 = 1./m * np.dot(dZ1, X.T) + lambd * W1 / m
    ### END CODE HERE ###
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
  
#----------------------------------------------------
# 使用L2正则化进行梯度下降
parameters = model(train_X, train_Y, lambd = 0.7)
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model with L2-regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

使用L2正则化，训练集的准确率为0.938，测试集为0.93，分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img29.jpg)

使用dropout正则化，在前向传播的时候分为4步：

> 1.根据`keep_prob`生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，`Dl = np.rndom.randn(Al.shape[0], Al.shape[1])`
>
> 2.将$d^{[l]}$转化为0-1矩阵， `Dl = Dl < keep_prob`
>
> 3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， `Al = np.muiltiply(Al, Dl)`
>
> 4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以`keep_prob`,`Al = Al / keep_prob`

后向传播也做修改，即对于某一层的$da^{[l]}$，应该进行以下计算：

> 1.`dAl = dAl * Dl` 
>
> 2.`dAl = dAl / keep_prob`

代码如下：

```python
# 前向传播
def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
    """
    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                    W1 -- weight matrix of shape (20, 2)
                    b1 -- bias vector of shape (20, 1)
                    W2 -- weight matrix of shape (3, 20)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    A3 -- last activation value, output of the forward propagation, of shape (1,1)
    cache -- tuple, information stored for computing the backward propagation
    """
    
    np.random.seed(1)
    
    # retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]
    
    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. 
    D1 = np.random.rand(*A1.shape)                    # Step 1: initialize matrix D1 = np.random.rand(..., ...)
    D1 = D1 < keep_prob                               # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
    A1 = np.multiply(A1, D1)                          # Step 3: shut down some neurons of A1
    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    ### START CODE HERE ### (approx. 4 lines)
    D2 = np.random.rand(*A2.shape)                    # Step 1: initialize matrix D2 = np.random.rand(..., ...)
    D2 = D2 < keep_prob                               # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)
    A2 = np.multiply(A2, D2)                          # Step 3: shut down some neurons of A2
    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
    
    return A3, cache
  
  
#----------------------------------------------
# 后向传播
def backward_propagation_with_dropout(X, Y, cache, keep_prob):
    """
    Implements the backward propagation of our baseline model to which we added dropout.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation_with_dropout()
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """
    
    m = X.shape[1]
    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    dA2 = np.dot(W3.T, dZ3)
    ### START CODE HERE ### (≈ 2 lines of code)
    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob       # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    ### START CODE HERE ### (≈ 2 lines of code)
    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
    dA1 = dA1 / keep_prob       # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
  
#-------------------------------------------------------------------
# 使用dropout正则化来梯度下降
parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)

print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model with dropout")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

dropout正则化在训练集的预测准确率为0.929，测试集的准确率为0.95。分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img30.jpg)

### 优化算法

#### Mini-batch梯度下降法

假设我们的样本数量为500w个，那么在进行梯度下降之前，我们需要先将500w个数据整合成一个大的向量$X$。Mini-batch的做法为将500w个样本按照每个子集为1000个样本等分。每个子集标记为$X^{\left\{ 1\right\}  }X^{\left\{ 2\right\}  },\dots,X^{\left\{ 5000\right\}  }$。相应的，除了需要拆分$X$，也需要拆分标签$Y$，拆分的方法和$X$相同。

**Mini-batch的原理是将同时原本对所有样本和标签同时进行梯度下降转变为同时只对一个子集进行梯度下降处理，处理5000次**。需要注意代价函数也要改变，因为每次训练的样本个数改变了。

当你的**训练集大小很大**的时候，mini-batch梯度下降法比batch梯度下降法运行地更快。



batch梯度下降法和Mini-batch梯度下降法的代价随迭代的图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img22.jpg)

右边的图像出现波动的原因是：每次实现梯度下降的样本集不同，可能$X^{\left\{ 1\right\}  }$和$Y^{\left\{ 1\right\}  }$需要花费的代价更大，而$X^{\left\{ 2\right\}  }$和$Y^{\left\{ 2\right\}  }$花费的代价更少，从而形成一个噪声的现象。

那么mini-bash的大小如何决定呢？

> 先看两种极端情况：
>
> 如果子集的大小为m，那么mini-bash梯度下降就变成了batch梯度下降；
>
> 如果子集的大小为1，那么mini-bash梯度下降就变成了`随机梯度下降法`，每个样本都是一个子集；
>
> batch梯度下降每次下降的噪声会小一点，幅度会大一点（这里的噪声是指梯度下降的方向偏离目标）；而随机梯度下降大部分时间会向着全局最小值逼近，但有时候会远离最小值（刚好该样本是一个''坏''样本），随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。
>
> batch梯度下降在训练数据很大的时候，单次训练迭代时间过长，如果训练数据量较小的情况下效果较好；而随机梯度下降单次迭代很快，但却无法使用向量化技术对运算进行加速。我们的目的就是选择一个不大不小的size，使得我们的学习速率达到最快（梯度下降）。
>
> 最优的情况就是，单次选取的size大小的数据分布比较符合整体数据的分布，这样使得学习速率和运行效率都比较高。

#### 指数加权平均

指数加权平均也称指数加权移动平均，通过它可以来计算局部的平均值，来描述数值的变化趋势，下面通过一个温度的例子来详细介绍一下。

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img23.jpg)

上图是温度随时间变化的图像，我们通过温度的局部平均值（移动平均值）来描述温度的变化趋势，计算公式如下：
$$
v_t=\beta v_{t-1}+(1-\beta)\theta_{t}\\
v_0=0\\
v_1=0.9v_0+0.1\theta_1\\
v_2=0.9v_1+0.1\theta_2\\
\theta 代表当天的温度，v代表局部平均值
$$
当$\beta$为0.9时，可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。

当$\beta$变得越小，移动平均值的波动越大。

通过上面的公式往下推到，可以得到$v_{100}$的表达式：
$$
v_{100}=0.1\times\theta_{100}+0.1\times0.9\times\theta_{99}+\dots+0.1\times0.9^{99}\times\theta_1\\
=0.1\times\sum_{i=1}^{100}0.9^{100-i}\times\theta_i
$$
当$\epsilon=1-\beta$时，$(1-\beta)^{\frac{1}{\epsilon}}\approx\frac{1}{e}\approx\frac{1}{1-\beta}$，所以可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。

> 简单来说，普通的加权求平均值的方法每一项的权重是$\frac{1}{n}$，指数加权平均每一项的权重是指数递减的。

* 指数加权平均的偏差修正

由于我们初始设置的$v_0$为0，这样会使前面几个$v_1,v_2\dots$的值与实际值相比偏小，我们通常会采取以下的办法来修正偏差：
$$
v_t=\frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t}
$$
这样修正的效果为随着t的增加，分母越来越接近1。相当于时间越短，修正的幅度越大，所以这个公式主要是为了修正早期的偏差。

#### 动量梯度下降法

我们将上面所说的`指数加权平均`的做法应用于神经网络的反向传播过程，如下：
$$
V_{dW}=\beta V_{dW}+(1-\beta)dW\\
V_{db}=\beta V_{db}+(1-\beta)db\\
W:=W-\alpha V_{dW},b:=b-\alpha V_{db}
$$
这样做可以减缓梯度下降的幅度，因为梯度下降不一定朝着最快的方向前进。如下图所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img24.jpg)

原本为蓝色的梯度下降会变成红色，纵轴摆动的方向变小了且上下摆动的幅度均值大概为0。这样一来，即使我增加学习率或者步长也不会出现紫色线这种偏离函数的情况。

*$\beta$最常用的值为0.9，按照道理来说需要加上偏差修正。但实际上不会这么做，因为经过10次迭代之后，移动平均已经过了初始阶段，不再是一个具有偏差的预测值。*

#### RMSprop算法

通过前面的算法可知，我们加快学习效率的方法是增加$W$方向的学习速率（图中的水平方向），降低$b$方向的学习速率（垂直方向）。公式如下：
$$
S_{dW}=\beta S_{dW}+(1-\beta)(dW)^2\\
S_{db}=\beta S_{db}+(1-\beta)(db)^2\\
W:=W-\alpha\frac{dW}{\sqrt{S_{dW}}+\epsilon}\\
b:=b-\alpha\frac{db}{\sqrt{S_{db}}+\epsilon}\\
式中，S_{dW}和S_{db}表示权重W和偏置值b在t−1轮迭代中的梯度动量\\
超参数β一般取值为0.9，学习率\alpha一般取值为 0.001，ε是防止分母为零，一般去10^{-8}。
$$
我们会希望$S_{db}$较大，$S_{dW}$较小，这样可以使得$W$方向的变化更大，$b$方向的变化更小。结果变化如下图：

 ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img25.jpg)

#### Adam算法

Adam算法是将动量梯度下降法和RMSprop算法结合起来，公式如下：
$$
首先初始化V_{dW}=0,S_{dW}=0,V_{db}=0,S_{db}=0\\
在第t次迭代时：\\
使用当前的mini-batch计算dW,db\\
V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW,V_{db}=\beta_1V_{db}+(1-\beta)db\quad 动量梯度下降\\
S_{dW}=\beta_2S_{dW}+(1-\beta_2)(dW)^2,S_{db}=\beta_2S_{db}+(1-\beta_2)(db)^2\quad SMSprob\\
V_{dW}^{correct}=\frac{V_{dW}}{1-(\beta_1)^t},V_{db}^{correct}=\frac{V_{db}}{1-(\beta_1)^t}\\
S_{dW}^{correct}=\frac{S_{dW}}{1-(\beta_2)^t},S_{db}^{correct}=\frac{S_{db}}{1-(\beta_2)^t}\\
W:=W-\alpha\frac{V_{dW}^{correct}}{\sqrt{S_{dW}^{corret}}+\epsilon}\\
b:=b-\alpha\frac{V_{db}^{correct}}{\sqrt{S_{db}^{corret}}+\epsilon}\\
$$
**Adam算法被证明具有更强的普适性，适用于更加广泛的结构。**

其中上述算法中的超参数使用值：
$$
\alpha是一个需要不断调整的值\\
\beta_1推荐值为0.9\\
\beta_2推荐值为0.999\\
\epsilon推荐值为10^{-8}
$$

#### 学习率衰减

假设使用一个mini-batch的梯度下降方法，梯度下降会出现噪声，最后不会收敛，而是会在最小值之间波动。通过学习率衰减的办法，可以使得在梯度下降到最小值附近，波动的幅度变得很小。如下图所示（从蓝线到绿线的变化）：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img26.jpg)

学习率的设置公式如下：
$$
\alpha=\frac{1}{1+衰减率\times 代数}\alpha_0\\ 
这里的代数是指迭代的次数,衰减率和\alpha是一个需要调整的参数。
$$

#### 作业五

分别使用`mini-batch`，`动量梯度下降`，`Adam`算法对梯度下降进行加速。

```python
# 导入库
import numpy as np
import matplotlib.pyplot as plt
import scipy.io
import math
import sklearn
import sklearn.datasets

from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation
from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset
from testCases import *

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# 参数更新
def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Update parameters using one step of gradient descent
    
    Arguments:
    parameters -- python dictionary containing your parameters to be updated:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients to update each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    learning_rate -- the learning rate, scalar.
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(parameters) // 2 # number of layers in the neural networks

    # Update rule for each parameter
    for i in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(i + 1)] -= learning_rate * grads["dW" + str(i + 1)]
        parameters["b" + str(i + 1)] -= learning_rate * grads["db" + str(i + 1)]
        ### END CODE HERE ###
        
    return parameters
  
# mini-batch
def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[1]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y) 洗牌
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1,m))

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size) 处理最后一个mini-batch小于64的情况。
    if m % mini_batch_size != 0:
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches
  
  
# 动量梯度下降：1.初始化参数 2.更新参数
def initialize_velocity(parameters):
    """
    Initializes the velocity as a python dictionary with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    
    Returns:
    v -- python dictionary containing the current velocity.
                    v['dW' + str(l)] = velocity of dWl
                    v['db' + str(l)] = velocity of dbl
    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    
    # Initialize velocity
    for i in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        v['dW' + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        v['db' + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
        ### END CODE HERE ###
        
    return v
  
 
def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Update parameters using Momentum
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- python dictionary containing the current velocity:
                    v['dW' + str(l)] = ...
                    v['db' + str(l)] = ...
    beta -- the momentum hyperparameter, scalar
    learning_rate -- the learning rate, scalar
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- python dictionary containing your updated velocities
    """

    L = len(parameters) // 2 # number of layers in the neural networks
    
    # Momentum update for each parameter
    for i in range(L):
        
        ### START CODE HERE ### (approx. 4 lines)
        # compute velocities
        v['dW' + str(i + 1)] = beta * v['dW' + str(i + 1)] + (1 - beta) * grads['dW' + str(i + 1)]
        v['db' + str(i + 1)] = beta * v['db' + str(i + 1)] + (1 - beta) * grads['db' + str(i + 1)]
        # update parameters
        parameters['W' + str(i + 1)] -= learning_rate * v['dW' + str(i + 1)]
        parameters['b' + str(i + 1)] -= learning_rate * v['db' + str(i + 1)]
        ### END CODE HERE ###
        
    return parameters, v
  
  
# Adma算法： 1.初始化V[dW],V[db],S[dW],S[db] 2.更新参数
def initialize_adam(parameters) :
    """
    Initializes v and s as two python dictionaries with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters["W" + str(l)] = Wl
                    parameters["b" + str(l)] = bl
    
    Returns: 
    v -- python dictionary that will contain the exponentially weighted average of the gradient.
                    v["dW" + str(l)] = ...
                    v["db" + str(l)] = ...
    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.
                    s["dW" + str(l)] = ...
                    s["db" + str(l)] = ...

    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    s = {}
    
    # Initialize v, s. Input: "parameters". Outputs: "v, s".
    for i in range(L):
    ### START CODE HERE ### (approx. 4 lines)
        v["dW" + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        v["db" + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
        s["dW" + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        s["db" + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
    ### END CODE HERE ###
    
    return v, s
  
  
def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,
                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):
    """
    Update parameters using Adam
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    learning_rate -- the learning rate, scalar.
    beta1 -- Exponential decay hyperparameter for the first moment estimates 
    beta2 -- Exponential decay hyperparameter for the second moment estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates

    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    """
    
    L = len(parameters) // 2                 # number of layers in the neural networks
    v_corrected = {}                         # Initializing first moment estimate, python dictionary
    s_corrected = {}                         # Initializing second moment estimate, python dictionary
    
    # Perform Adam update on all parameters
    for i in range(L):
        # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".
        ### START CODE HERE ### (approx. 2 lines)
        v["dW" + str(i + 1)] = beta1 * v["dW" + str(i + 1)] + (1 - beta1) * grads["dW" + str(i + 1)]
        v["db" + str(i + 1)] = beta1 * v["db" + str(i + 1)] + (1 - beta1) * grads["db" + str(i + 1)]
        ### END CODE HERE ###

        # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        v_corrected["dW" + str(i + 1)] = v["dW" + str(i + 1)] / (1 - beta1 ** t)
        v_corrected["db" + str(i + 1)] = v["db" + str(i + 1)] / (1 - beta1 ** t)
        ### END CODE HERE ###

        # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".
        ### START CODE HERE ### (approx. 2 lines)
        s["dW" + str(i + 1)] = beta2 * s["dW" + str(i + 1)] + (1 - beta2) * np.multiply(grads["dW" + str(i + 1)], grads["dW" + str(i + 1)])
        s["db" + str(i + 1)] = beta2 * s["db" + str(i + 1)] + (1 - beta2) * np.multiply(grads["db" + str(i + 1)], grads["db" + str(i + 1)])
        ### END CODE HERE ###

        # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        s_corrected["dW" + str(i + 1)] = s["dW" + str(i + 1)] / (1 - beta2 ** t)
        s_corrected["db" + str(i + 1)] = s["db" + str(i + 1)] / (1 - beta2 ** t)
        ### END CODE HERE ###

        # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(i + 1)] -= learning_rate * v_corrected["dW" + str(i + 1)] / (epsilon + np.sqrt(s_corrected["dW" + str(i + 1)]))
        parameters["b" + str(i + 1)] -= learning_rate * v_corrected["db" + str(i + 1)] / (epsilon + np.sqrt(s_corrected["db" + str(i + 1)]))
        ### END CODE HERE ###

    return parameters, v, s
  
 
```

使用不同算法加速的模型进行预测：

```python
# 加载数据
train_X, train_Y = load_dataset()
```

数据分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img31.jpg)

建立预测模型并使用三种不同的算法进行加速预测模型：

```python
def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,
          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):
    """
    3-layer neural network model which can be run in different optimizer modes.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    layers_dims -- python list, containing the size of each layer
    learning_rate -- the learning rate, scalar.
    mini_batch_size -- the size of a mini batch
    beta -- Momentum hyperparameter
    beta1 -- Exponential decay hyperparameter for the past gradients estimates 
    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates
    num_epochs -- number of epochs
    print_cost -- True to print the cost every 1000 epochs

    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(layers_dims)             # number of layers in the neural networks
    costs = []                       # to keep track of the cost
    t = 0                            # initializing the counter required for Adam update
    seed = 10                        # For grading purposes, so that your "random" minibatches are the same as ours
    
    # Initialize parameters
    parameters = initialize_parameters(layers_dims)

    # Initialize the optimizer
    if optimizer == "gd":
        pass # no initialization required for gradient descent
    elif optimizer == "momentum":
        v = initialize_velocity(parameters)
    elif optimizer == "adam":
        v, s = initialize_adam(parameters)
    
    # Optimization loop
    for i in range(num_epochs):
        
        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch
        seed = seed + 1
        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)

        for minibatch in minibatches:

            # Select a minibatch
            (minibatch_X, minibatch_Y) = minibatch

            # Forward propagation
            a3, caches = forward_propagation(minibatch_X, parameters)

            # Compute cost
            cost = compute_cost(a3, minibatch_Y)

            # Backward propagation
            grads = backward_propagation(minibatch_X, minibatch_Y, caches)

            # Update parameters
            if optimizer == "gd":
                parameters = update_parameters_with_gd(parameters, grads, learning_rate)
            elif optimizer == "momentum":
                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)
            elif optimizer == "adam":
                t = t + 1 # Adam counter
                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,
                                                               t, learning_rate, beta1, beta2,  epsilon)
        
        # Print the cost every 1000 epoch
        if print_cost and i % 1000 == 0:
            print ("Cost after epoch %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)
                
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('epochs (per 100)')
    plt.title("Learning rate = " + str(learning_rate))
    plt.show()

    return parameters


# 画图
def plot_decision_boundary(model, X, y):
    #import pdb;pdb.set_trace()
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    y = y.reshape(X[0,:].shape)#must reshape,otherwise confliction with dimensions
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)
    plt.show()
 

```

mini-batch：

```python
#--------------------------------------------- mini-batch
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "gd")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Gradient Descent optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)

```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img32.jpg)

准确率为0.79，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img33.jpg)

动量下降(with mini-batch)：

```python
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = "momentum")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Momentum optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img34.jpg)

准确率为0.79，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img35.jpg)

Adma算法（with mini-batch）：

```python
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "adam")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Adam optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img36.jpg)

准确率为0.94，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img37.jpg)

### 超参数调试

#### 调试处理

超参数一般有：学习率$\alpha$；动量梯度下降的$\beta$；Adma算法的$\beta_1,\beta_2,\epsilon$；神经网络的层数layers；隐藏层的数量；学习率衰减；`mini-bash size`等。

我们一般优先选择调试学习率$\alpha$，其次是隐藏层数量，`mini-batch size`和动量下降中的$\beta$。再其次调整的参数就是layers和学习率衰减了。Adma算法的三个参数一般设置为$\beta1=0.9,\beta_2 = 0.999,\epsilon=10^{-8}$，我们一般不会调整它。

在深度学习中，我们一般会通过矩阵随机取值的方式来调参，如下图：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img38.jpg" style="zoom:50%;" />

长宽分别代表超参数1，2的取值。一般会在矩阵内`随机取25个点来查看效果`，因为这样会得到25个不同的超参数1和25个不同的超参数2。

但如果为3个参数，那么我们可以在一个立方体内随机选择点。

超参数调试的技巧是从`粗糙到精细的过程`：经过粗略的调整，发现在某区域内效果较好，那么我们要做的是，放大这块区域，更加密集的取值，来获取最优点。

#### 为超参数选择合适的范围

上面所说的随机取值并不是在有效值范围内的随机均匀取值，而是选择合适的步进值来探究超参数。对于隐藏层和隐藏单元的数量，随机均匀取值是合理的，但对某些参数的不合理的。

假设你认为学习率$\alpha$的取值范围是0.0001～1，取值范围内随机均匀取值会将90%的值集中在0.1到1里面，不合理。**对于这种情况，一般先按0.0001，0.001，0.01，0.1，1作为分界点，在分界点之间随机均匀取值。**

在Python中你可以这样做：

```python
import numpy as np

r = -4 * np.random.rand()
a = np.power(10, r)
# 更多的取值情况为10^a~10^b
# 通过计算得到a和b的值，然后我们将r取值变为a~b之间的随机取值。
```

还有就是动量梯度下降$\beta$的取值，它意味着指数加权平均的大小，一般通过$1-\beta$来取范围。但因为是$1-\beta$，需要将排列顺序颠倒。

```python
import numpy as np

# r在[-3, -1]内
h = -1
l = -3
r = (h - l) * np.random.rand() + l
beta = 1 - np.power(10, r)
```

#### 超参数训练方式：Panda VS Caviar

如果没有很大的算力的情况下，可以根据天数来调整超参数来观察效果（每次只能运行一个模型）。（Panda）

在算力足够的情况下，同时训练不同的几个模型。（Caviar）

#### 归一化网络

前面介绍过的输入归一化，可以使梯度下降时，更加不容易偏离方向，增大步长，加快学习速度。但这只针对了输入层，如果我们对所有隐藏层进行归一化，可以大大加快学习速率，这就是我们平常所说的`batch归一化`。实际做法一般是对$Z^{[l]}$进行归一化处理，做法如下：
$$
对于某一层的Z^{[l]}，有z^{(1)},z^{(2)},\dots,z^{(m)}\\
u=\frac1m\sum_{i=1}^mz^{(i)}\\
\sigma^2=\frac1m\sum_{i=1}^m(z^{(i)}-u)^2\\
z_{norm}^{(i)}=\frac{z^{(i)}-u}{\sqrt{\sigma^2+\epsilon}}\\
这样Z的每一个分量z值都已经标准化了，平均值为0，方差为1;\\
但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有不同的分布会有意义。\\
\tilde{z}^{(i)}=\gamma z^{(i)}_{norm}+\beta\\
这里的\gamma和\beta的作用是让你可以构造其他平均值和方差的隐藏单元值\\
如果\gamma=\sqrt{\sigma^2+\epsilon},\beta=u\quad则\tilde{z}^{(i)}=z^{(i)}_{normal}
$$
*已经知道了如何对Z进行归一化，那么如何将其应用于神经网络中呢？*

BatchNormal神经网络参数计算的过程如下：
$$
X \xrightarrow{W^{[1]},b^{[1]}}Z^{[1]} \xrightarrow[Batch Normal(BN)]{\gamma^{[1]},\beta^{[1]}}\tilde{z}^{[i]}\rightarrow a^{[1]}=g^{[1]}(\tilde{z}^{[i]}) \xrightarrow{W^{[2]},b^{[2]}}Z^{[2]}\rightarrow\dots
$$
*需要注意的是这里的$\gamma$和$\beta$是和$W$，$b$一样的参数，而不是超参数。所以在反向传播时，也需要计算$d\beta,d\gamma$来更新$\beta$和$\gamma$的值。而且常数项$b$代表变化后的Z平均值离0有多远，但是标准化之后的$z^{(i)}_{norm}$均值必定为0，所以我们可以直接将$b$去掉，转而使用后面的$\beta$参数来定义。*

*Batch Normal为什么奏效？*

> 用一个例子来解释就是：你的训练集都是黑猫或者其他动物，假设他们都分布于某一侧，将训练出来的模型去预测花色猫，效果就不好。花猫分布于另一侧，这样相当于不同于黑猫的x分布。你不能期待分布在左边的数据训练出来的模型能预测右边的数据。假设将左侧的数据进行Batch Normal，相当于人为将其分布均匀，比较能适应新的数据集，防止了Covarite shift。
>
> 放在深层神经网络来看就是前层参数的变化会影响后层的参数，归一化降低了这种影响，尽量只保持特征值带来的波动。

*Batch Normal和dropout正则化*

>dropout正则化对隐藏单元进行随机删除，从而引入噪声，防止对某个神经单元过于依赖。而Batch Normal在每个mini-batch的均值方差与整体的均值方差不一致，引入加性噪声和乘性噪声，导致轻微的正则化。如果想减少这种影响，可以将mini-batch的值设置的更大，从而减少带来的噪声，减少正则化的效果！

*测试时的Batch Normal*

> 因为测试时是单个单个进行测试的，不能直接进行batch normal，因为无法知道均值和方差。所以我们使用训练集来估算均值和方差。通常估算的方法是通过指数加权平均来粗略估算均值和方差。



#### softmax回归

假设我们的分类类别是多个而不是两个，相当于我们的输出层单元个数变成了多个，数量就是你要分出的类别数C。这时，输出层会变成C个概率值，输入为$x$的情况下，输出为$p(类别|x)$，所以输出层是维度为（C，1）的矩阵。

softmax激活函数公式如下：
$$
假设Z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]},L层有C个隐藏单元，所以Z^{[L]}的维度为（C，1）\\
t=e^{(Z^{[L]})}\\
a^{[L]}=\frac{t}{\sum_{j=1}^{C}t_j},a_j^{[L]}=\frac{t_i}{\sum_{j=1}^{C}t_j}\\
a^{[L]}的维度为（C，1），可以看出softmax激活函数的特点为输入和输出都为（C，1）
$$
*使用softmax训练一个softmax分类器*

> 需要注意的是Softmax回归其实就是Logistic回归的推广，当Softmax回归的类C=2，就变成了Logistic回归。

softmax网络的损失函数为：
$$
L(\hat{y},y)=-\sum_{j=1}^Cy_j\log{\hat{y_j}}
$$
代价函数为：
$$
J=\frac1m\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})
$$
**特别需要注意的是，softmax因为是一个多分类的问题，输入的$y$标签也是一个向量，$Y$会变成一个矩阵。比如有猫，狗，牛，蛇，那么蛇的样本标签$y=[0,0,0,1]^{T}$，向量化技术后，整体样本标签会成为一个（C，m）的矩阵。**

后向传播的公式：
$$
dz^{[L]}=\hat{y}-y\\
$$

#### 深度学习框架

深度学习框架在市面上有很多，我们该如何选择？

* 1.便于编程
* 2.运行速度
* 3.是否开源

#### 作业六

* tensorflow练习

导入需要的库：

```python
import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.python.framework import ops
from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict

%matplotlib inline
np.random.seed(1)
```

计算损失函数的例子：

```python
y_hat = tf.constant(36, name='y_hat')            # Define y_hat 常量. Set to 36.
y = tf.constant(39, name='y')                    # Define y. Set to 39

loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss

init = tf.global_variables_initializer()         # When init is run later (session.run(init)),
                                                 # the loss variable will be initialized and ready to be computed
with tf.Session() as session:                    # Create a session and print the output
    session.run(init)                            # Initializes the variables
    print(session.run(loss))                     # Prints the loss
```

会话机制：

```python
a = tf.constant(2)
b = tf.constant(10)
c = tf.multiply(a,b)
print(c)

sess = tf.Session()
print(sess.run(c))
```

往函数喂数据:

```python
# Change the value of x in the feed_dict

x = tf.placeholder(tf.int64, name = 'x')
print(sess.run(2 * x, feed_dict = {x: 3}))
sess.close()
```

初始化神经网络模型参数：

```python
# GRADED FUNCTION: linear_function

def linear_function():
    """
    Implements a linear function: 
            Initializes W to be a random tensor of shape (4,3)
            Initializes X to be a random tensor of shape (3,1)
            Initializes b to be a random tensor of shape (4,1)
    Returns: 
    result -- runs the session for Y = WX + b 
    """
    
    np.random.seed(1)
    
    ### START CODE HERE ### (4 lines of code)
    X = tf.constant(np.random.randn(3, 1), name="X")
    W = tf.constant(np.random.randn(4, 3), name="W")
    b = tf.constant(np.random.randn(4, 1), name="b")
    Y = tf.add(tf.matmul(W, X), b)
    ### END CODE HERE ### 
    
    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate
    
    ### START CODE HERE ###
    sess = tf.Session()
    result = sess.run(Y)
    ### END CODE HERE ### 
    
    # close the session 
    sess.close()

    return result
```

激活函数：

```python
# GRADED FUNCTION: sigmoid

def sigmoid(z):
    """
    Computes the sigmoid of z
    
    Arguments:
    z -- input value, scalar or vector
    
    Returns: 
    results -- the sigmoid of z
    """
    
    ### START CODE HERE ### ( approx. 4 lines of code)
    # Create a placeholder for x. Name it 'x'.
    x = tf.placeholder(tf.float32, name="x")

    # compute sigmoid(x)
    sigmoid = tf.sigmoid(x)

    # Create a session, and run it. Please use the method 2 explained above. 
    # You should use a feed_dict to pass z's value to x. 
    with tf.Session() as sess:
        # Run session and call the output "result"
        result = sess.run(sigmoid, feed_dict={x:z})
    
    ### END CODE HERE ###
    
    return result
```

计算代价函数：

```python
# GRADED FUNCTION: cost

def cost(logits, labels):
    """
    Computes the cost using the sigmoid cross entropy
    
    Arguments:
    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)
    labels -- vector of labels y (1 or 0) 
    
    Note: What we've been calling "z" and "y" in this class are respectively called "logits" and "labels" 
    in the TensorFlow documentation. So logits will feed into z, and labels into y. 
    
    Returns:
    cost -- runs the session of the cost (formula (2))
    """
    
    ### START CODE HERE ### 
    
    # Create the placeholders for "logits" (z) and "labels" (y) (approx. 2 lines)
    z = tf.placeholder(tf.float32, name="z")
    y = tf.placeholder(tf.float32, name="y")
    
    # Use the loss function (approx. 1 line)
    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z, labels = y)
    
    # Create a session (approx. 1 line). See method 1 above.
    sess = tf.Session()
    
    # Run the session (approx. 1 line).
    cost = sess.run(cost, feed_dict={z:logits, y:labels})
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    
    return cost
```

独热编码：

```python
# GRADED FUNCTION: one_hot_matrix

def one_hot_matrix(labels, C):
    """
    Creates a matrix where the i-th row corresponds to the ith class number and the jth column
                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) 
                     will be 1. 
                     
    Arguments:
    labels -- vector containing the labels 
    C -- number of classes, the depth of the one hot dimension
    
    Returns: 
    one_hot -- one hot matrix
    """
    
    ### START CODE HERE ###
    
    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
    C = tf.constant(value = C, name="C")
    
    # Use tf.one_hot, be careful with the axis (approx. 1 line)
    one_hot_matrix = tf.one_hot(labels, C, axis=0)
    
    # Create the session (approx. 1 line)
    sess = tf.Session()
    
    # Run the session (approx. 1 line)
    one_hot = sess.run(one_hot_matrix)
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    
    return one_hot
```

初始化零一向量：

```python
# GRADED FUNCTION: ones

def ones(shape):
    """
    Creates an array of ones of dimension shape
    
    Arguments:
    shape -- shape of the array you want to create
        
    Returns: 
    ones -- array containing only ones
    """
    
    ### START CODE HERE ###
    
    # Create "ones" tensor using tf.ones(...). (approx. 1 line)
    ones = tf.ones(shape)
    
    # Create the session (approx. 1 line)
    sess = tf.Session()
    
    # Run the session to compute 'ones' (approx. 1 line)
    ones = sess.run(ones)
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    return ones
```



### 网络模型的优化

#### ML策略

当你采用一个分类器，得到的准确率为90%，如何提高它的准确率呢？以下有这么多的方法可以使用：

* 收集更多的数据
* 提高训练集的多样性（反例集）
* 尝试使用优化算法
* 使用规模更大或者更小的网络
* 使用正则化
* 修改网络结构
  * 修改激活函数
  * 改变隐藏层单元的数目
  * ...

修改的方法有很多，但是需要选择一个正确的并不容易。

#### 正交化

如果将神经网络比作一个可调节的电视机，调整电视机的各种按钮来改变电视机的布局，色彩，亮度就如同在神经网络中调整各种超参数来查看神经网络的效果。

对于一个电视来说，我们需要慢慢调整（一个开关一个开关调整）才能将电视机调好，那么这种一个超参数调整，只能改变其神经网络的某个性质的形式就叫做正交化。

#### 单一数字评估指标

理解一下两个概念，`Precision`和`Recall`。

`Precison`：中文为查准率，预测为真的模型中，有多少样本是真的，它的占比值。对于一个猫分类器来说就是，模型预测为猫的类型样本中，有多少占比为真正的猫。

`Recall`:中文为召回率，对于所有样本标签为真的情况下，有多少占比是你的模型正确预测出来的。对于一个猫分类器来说就是，在所有标签为真猫的样本中，有多少占比是你的模型正确预测的。

对于一个分类器来说，两个指标都同等重要，所以我们需要找到一个结合查准率和召回率的指标，也就是所谓的`F1分数`，公式如下：
$$
F1=\frac{2}{\frac1p+\frac1R}\\
其中p代表查准率，R代表召回率，计算方式在数学上称为调和平均数
$$
这样算出来的`F1分数`来判断网络的性能更加合理，称这种形式为单一数字评估指标。

再举一个例子，如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img39.jpg)

算法在不同地区的错误率都不同，无法统一进行比较，那我们就将其整合为一个值，就是做平均之后再做比较。

#### 满足和优化指标

对于某些多个指标的情况，我们想建立`单一实数评估指标`是非常困难的。比如，准确率和运行时间，想要合并为一个成本指标是很困难的，因为我们并不知道两个指标哪个更重要，或者说两个指标之间的关联性。所以我们可以采用`满足和优化指标`，在该例中就是在满足运行时间在多少毫秒之内，优化准确率，这样就可以找到最优的那个算法。

#### 划分训练/开发/测试集

* 划分dev/test sets：

假设你要开发一个猫分类器，需要在不同区域进行运营：美国，英国，中国，印度，澳大利亚等。你如何设立开发集和测试集呢？

如果你在8个区域中随机选取4个区域作为开发集，4个区域作为测试集，效果可能是非常糟糕的。我们选取的原则是让开发集和测试集尽量来自同一分布。所以我们的做法一般是将所有数据随机洗牌，放入开发集和测试集，这样开发集和测试集都有来自8个区域的数据。

* 开发集和测试的大小：

在机器学习中，会有一条七三分的比例划分训练集和测试集，如果有开发集，则会按照6:2:2的比例来划分训练集，开发集，测试集。但在如今的数据量在百万级别的情况下，这样做会更加合理：98%作为训练集，1%开发集，%1测试集。

* 训练集，开发集和测试集的目的：

开发集是为了指定产品的优化目标，训练集决定了你向优化目标迭代的速度，测试集是为了评估投产系统的性能。

#### 何时改变开发_测试集和指标

假设一个猫分类器，使用了算法A和算法B，它们都分别采用分类错误率来衡量算法的性能。算法A的错误率为3%，算法B的错误率为5%，但是算法A在错误分类的图片里，将色情图片分为了猫的图片，这是用户不能接受的。所以在这种情况下，即使你的算法在开发集上的指标更好，依然被认为是不好的产品。**当你的指标错误地预测算法，就是你需要修改开发_测试集和指标的时候**。

> 那么如何修改指标？

我们可能会对目标`错误率`设置一个$W$来赋予不同的图片以不同的权重：
$$
W^{(i)}=\begin{cases}
1,&如果x^{(i)}不是色情图片\\
10,&如果x^{(i)}是色情图片
\end{cases}
$$
*需要注意的是在目标函数上加上$W$，也需要修改归一化常数*

#### 贝叶斯误差和可避免偏差

假设一个两个不同的模型，在训练集和开发集上的错误率如下：

一个问题的错误率是有一个贝叶斯误差（理论最小误差），在大多数时候人类识别的错误率是接近贝叶斯误差的。

|                | 问题1的错误率 | 问题2 |
| :------------: | :-----------: | :---: |
|     训练集     |      8%       |  8%   |
|     开发集     |      10%      |  10%  |
| 人类错误率相近 |      1%       | 7.5%  |

在训练集和人类错误率之间的差值被称为可避免偏差，训练集和开发集之间的误差被认为是需要更加优化方差。问题1，训练集与人类错误率相差较远，是训练欠拟合，可以加大训练网络或者迭代时间等来缩小；问题2中的训练集8%和测试集10%，略微过拟合，可以用正则化来降低拟合程度。

> 那么如果要把人类错误率作为贝叶斯错误的替代，该如何定义人类错误率呢？

假设一个医学图像分类的例子：

* 普通的人类在该任务上达到了3%的错误率。

* 普通的医生能达到1%的错误率
* 经验丰富的医生能达到0.7%的错误率
* 一个经验丰富的医生团队能达到0.5%的错误率

那么在上述的例子中，该如何界定人类水平错误率呢？

在我们的定义中，人类水平错误率应该逼近理论错误率的极限，那么我们就将最低的0.5%来估计`贝叶斯错误率`。

#### 改善模型的表现

改善模型的表现，需要满足很多条件：

1.首先，你的算法需要对训练集的拟合很好，这可以看成是你做到的`可避免偏差`很低。

2.在训练集中做的很好的情况下，然后推广到开发集和测试集的效果也很好，也就是`方差`不是太大。

* 降低可避免偏差的方法：
  * 训练更大的模型
  * 训练更久，或者使用更好的优化算法来加快训练速度
  * 寻找更好的神经网络架构（比如RNN，CNN）/寻找一组更好的超参数
* 降低方差的方法：
  * 使用更多数据
  * 正则化
  * 数据扩增
  * 修改网络架构/寻找不同的超参数组合

### 修改模型的错误

#### 误差分析

如果你的猫分类器识别错误了100张图片，你的识别错误率为10%。如果只有5张图片错误识别成狗，那么针对狗做算法优化最多从10%优化到9.5%；如果有50张图片错误识别成狗，那么针对狗做算法优化可以从10%优化到5%。很显然，对于第一种情况优化方式是不合理的，第二种情况优化方式是合理的。

一般会在开发集的预测错误样本上进行误差分析，有时候人工进行误差分析是很有效果的。

#### 清除标注错误的数据

如果你的训练数据集出现标注错误的数据，比如一张狗的图片被你标记成了猫。其实对于数据较多的数据集，有少量数据标注错误其实对结果影响不大。深度学习算法对随机标注错误不敏感，但是对于系统性的标注错误就很敏感。

如果你的开发集或测试集出现标注错误的数据，就要看情况了。如果严重影响了对你开发集上评估算法的能力，那应该花时间去修复标记错误。其实也是通过错误预测的样本进行分析，看看有多少错误是因为标注数据导致的，如果占比很低，可以不进行错误的修改，如果占比较高，可以进行错误的修改。

#### 在不同的划分上训练并测试

目前很多深度学习团队可能将训练集和开发集分开，来自不同的数据分布。**那么如何解决两种数据分布不同的数据对算法性能的影响呢？**

* 第一种做法是将两组数据合并在一起：假设你有200000张高清的猫的图片和10000张模糊的不专业的猫的图片，合并之后就有210000张图片。进行随机洗牌后，训练集包含205000张图片，开发集和测试集各包含2500张图片。这样做的好处是数据集都来自于同一分布，更好管理；缺点是开发集的样本大部分为你从网上下载的高清图片，并不是你关注的数据分布，显然这是不合理的。
* 第二种做法是将200000张网图和5000/2500张不专业的图一起作为训练集，然后5000张不专业图1:1用作开发集和测试集。优点是你的开发集的数据分布是你想要的，缺点是你的训练集分布和开发测试集分布不同。

*根据经验，一般选择第二种做法会更加好！*

 #### 不匹配数据划分的偏差和方差

我们已经学会了如何在同一分布数据上进行偏差分析和方差分析！假设训练集与开发测试集分布不同，我们又需要如何分析偏差和方差呢？

假设训练集和开发集的照片来自不同分布，如果训练集误差为1%，开发集误差为10%。那么久不能简单的说算法过拟合，泛化能力不强。这里的开发集误差高的原因可能有两个：1.是因为训练集在训练的过程中过拟合（方差过高）2.是因为开发集的数据来自和训练集不同的分布。

>如何分辨哪种因素影响了开发集的高误差呢？

我们通常的做法是将训练集再划分，比如9:1，将0.9作为训练集，0.1作为开发集，这样相当于是同分布的训练集和开发集。那么总体的数据划分如下：

将原本的训练集的90%作为训练集，将10%作为train-dev集，将原来的开发测试集作为dev-test集。然后将train-dev集和dev-test集上的错误率进行对比，来看看到底是哪个因素影响了开发集的高误差，从而确定我们的优化方向。

* 如果你的训练集和train-dev集误差率相差很多，说明你的算法还存在问题。
* 上面一条的误差不大，而train-dev集误差率和dev-test集误差相差较大，说明是因为数据分布不同造成了高误差。
* 如果两个差距都相差很大，说明两个因素都有影响，均需要优化。
* 假设开发集和测试集的误差相差很大，那么算法可能在开发集上过拟合了，需要在开发集上收集更多的数据。
* 如果出现了训练集和train-dev集上表现不好，而在dev-test集上表现更好，那么可能是训练集的数据更难识别，更难处理的原因。

#### 解决数据不匹配

解决数据不匹配，首先需要了解训练集和开发集的之间区别（错误分析）。找到区别后，可以根据区别来模拟数据，也可以在训练集里加入更多类似于开发集的数据。

#### 迁移学习

假设你有一个已经训练好的语音识别模型，你想将它应用于其他地方，可以使用迁移学习的方法。做法是将输出层去掉，在原始的神经网络的基础上加上一层或者多层的神经网络节点。

>那么什么时候使用迁移学习是比较合适的呢？

当你在某个领域学习到的数据和知识应用到另一个领域，而正好这个领域你的数据集比较少，那么你可能能够使用迁移学习来完成这个目标。*使用迁移学习有一个重要的前提是：如果想提高任务B的性能，任务A和任务B都要有相同的输入x，任务A比任务B的数据多，迁移学习才是有意义的。*

#### 多任务学习

如果对一张图片你需要识别多个物体时，你输入的$x^{(i)}$对应的$y^{(i)}$可能有更高的维数（假设你需要识别4个类，那么维度就变成4$\times$1）。

那么假设对于一个4个维度的网络，我们的损失函数如何定义：
$$
\hat{y}^{(i)}=\frac1m\sum_{i=1}^m\sum_{j=1}^4L(\hat{y_j}^{(i)},y_j^{(i)})\\
这里的L是指logsitc损失
$$

> 多任务学习与softmax回归有什么区别？
>
> softmax将单个标签分配给单个样本，实现多分类问题
>
> 多任务学习则是一个图片对应多个标签，每一个标签都是一个多维矩阵。

*虽然多任务学习可以使用多个网络单任务来完成，但有时，多任务网络会比单任务多网络的性能更好，而且多任务学习可以解决只有部分物体被标记的情况（比如这张图片中漏标了一个，只对做了标记的进行求和）。*

>多任务学习提升算法性能的原理：
>
>其实多任务学习可以看成是多次迁移学习的代替。在许多物体的识别的低网络迭代部分是相似的，对于其他物体识别特征的学习的效果相当于增加该物体识别学习的样本数量。（前提是其他任务的样本比该任务的样本多，才能提升该任务识别的性能）。

#### 端到端的深度学习

传统的学习流程(以语音识别为例)：
$$
X(audio)\xrightarrow{MFCC}特征\xrightarrow{机器学习算法}音位\rightarrow单词\rightarrow Y(听写文本)
$$
而在端到端的学习中，简化成为了黑盒模型：
$$
audio\xrightarrow{单个神经网络} 听写文本
$$
*端到端学习被证明比传统的学习流程更好，但只限于数据量比较大的情况下。*

> 如何选择端到端学习？
>
> 1.如果你的数据量很大，可以采用端到端学习
>
> 2.如果你没有特别多的东西，还是需要人为的组件来支撑（特征提取等）

### 卷积操作

计算机视觉的例子：

* 图片分类
* 目标检测
* 神经网络实现图片转化迁移

但因为图片的像素高时，每张图片都很是一个很大的维度，如何处理大量的高像素图片呢？这可能就要用到卷积神经网络了。

#### 边缘检测示例

我们通过边缘检测的例子来看卷积操作是如何进行的。

在边缘检测的例子里，卷积核从左到右每列为{1, 1, 1},{0, 0, 0},{-1, -1, -1}。

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img40.jpg)

如图所示，左边是一个6$\times$6的矩阵，中间是一个3$\times$3的卷积核，”*“代表的是卷积操作，但在Python中一般代表乘积的操作，需要注意区分。将卷积核与左边颜色加深的矩阵一一对应相乘后再相加，得到右边绿色的数值。让卷积核在模板上进行移动卷积，可以得到一个4$\times$4的矩阵。假设原图像的维度为m$\times$n，卷积核的大小为a$\times$a，那么得到的矩阵大小为(m-a+1)$\times$(n-a+1)。

对应编程的函数：

> python：conv_forward
>
> tensorflow：tf.nn.conv2d
>
> PyTorch：torch.nn.Conv2d

再举个更加明显的例子，如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img41.jpg)

可以看到很明显的就是，如果左边的图片没有变化，与卷积核进行卷积就会得到0，如果选取的区域图片有变化，那么得到的结果就有正值。上述的过程看一看成如下示意图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img42.jpg)

*将最左边的变化的部分在最右边得到的结果显现出来。*

#### 更多的边缘检测内容

在上节例子中的卷积核可以为我们检测由亮到暗的一个过渡的过程，如果是由暗到亮，那么输出的就是负值。

如果我们把上节检测垂直边缘的卷积核旋转一下，从第一行到第三行分别是{1, 1, 1},{0, 0, 0},{-1, -1, -1}，这就变成了一个水平的边缘检测。

列举几个3$\times$3滤波器：

* Sobel滤波器：第一列到第三列分别为：{1, 2, 1},{0, 0, 0},{-1, -2, -1}。
* Scharr滤波器：第一列到第三列分别为：{3, 10, 3},{0, 0, 0},{-3, -10, -3}。

#### Padding

在进行卷积的过程中，会发现很容易将图像最边缘的部分给忽略掉。一般的做法是在进行卷积之前，会在周围再填充一层像素，这样m$\times$n的矩阵就变成了(m+2)$\times$(n+2)的矩阵，这样在每层神经网络里进行卷积，不会损失掉边缘的特征。

填充一层就代表padding=1，填充两层代表padding=2...

* Valid卷积，其卷积过程如下：

$$
n\times n\quad*\quad f\times f\xrightarrow{no\quad padding}(n-f+1)\times (n-f+1)
$$

* Same卷积，顾名思义就是卷积前后的大小是相同的，其卷积过程如下：

$$
n\times n\quad*\quad f\times f\xrightarrow[p=\frac{f-1}{2}]{padding=p}n\times n\\
f=2p+1,所以我们的f只取奇数
$$

*上述过程中所有填充的像素一般使用0来填充！*

#### 卷积步长

卷积步长指得是，卷积模板每次移动的距离，一般用英文stride表示。如果步长表示为$s$，那么卷积后的矩阵大小就变了：
$$
n\times n\quad *\quad f\times f\xrightarrow{padding=p,stride=s}\lfloor \frac{n+2p-f}{s}+1 \rfloor \times \lfloor \frac{n+2p-f}{s}+1 \rfloor
$$

> 深度学习的卷积vs数学上的卷积
>
> 前面所讲的所有操作都属于深度学习上的卷积，其实这严格意义上来讲不是一种卷积。真正的在数学上的卷积先要对卷积核进行翻转操作，但在深度学习的文献中，我们将直接把深度学习上的伪卷积称为卷积。

#### 三维卷积

我们知道彩色的图像像素矩阵是一个三维矩阵，其中第三个维度为3，代表RGB三个通道。那么如果图像是三维的（假设为6$\times$6$\times$3），那么我们使用的卷积核也应该是三维的（假设为3$\times$3$\times$3），第三个维度与图像的第三个维度是相同的，代表三个通道，但输出图像的结果的形状为4$\times$4$\times$1。

*为什么输出的图像是只有一个通道的呢？*

可以看一下如下的图来理解：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img43.jpg)

将三个通道的卷积核看成一个长方体，在三通道图像矩阵上进行滑动得到输出。这样做的好处是可以对不同的通道进行不同的卷积模版设置，参数设置不同，可以得到不同的特征检测器。

为了对一个图像同时进行不同特征的选择，我们可以同时用不同的卷积核对图像做卷积，同时得到不同的输出（每个输出都是单通道的），我们将几个输出堆叠在一起，就形成了多维矩阵。注意有几个卷积核，得到图像输出的第三个维度就有几个。

#### 单层卷积网络

学会了三维卷积之后，其实就可以使用三维卷积来表示单层卷积网络的结构了，下面用一个例子来表示过程：
$$
input：6\times6\times3\rightarrow\begin{cases}
*\quad 3\times3\times3的卷积核 \rightarrow二维图像输出y_1\rightarrow ReLU(y_1+b_1) \\
*\quad 3\times3\times3的卷积核 \rightarrow二维图像输出y_2\rightarrow ReLU(y_2+b_2)
\end{cases}\\
\rightarrow4\times4\times2\quad 这里的2根据单层卷积核的数量进行更改
$$
*和普通的神经网络相比，卷积核相当于普通神经网络中的权重$w$，偏差同样为$b$。一般在使用卷积层时，我们使用$f^{[l]}$来表示第$l$层的卷积核大小；使用$p^{[l]}=padding$代表使用有效填充，也就是`same padding`，使输入输出前后的图像高度和宽度一致；使用$s^{[l]}$来代表$l$层上的卷积步长；$l$层的输入则变为$n^{[l-1]}_H\times n^{[l-1]}_W\times n^{[l-1]}_c$，$l$层的输出则变为$n^{[l]}_H\times n^{[l]}_W\times n^{[l]}_c$，其中$n^{[l]}_H=\lfloor \frac{n^{[l-1]}_H+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor$，$n^{[l]}_W=\lfloor \frac{n^{[l-1]}_W+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \rfloor$*，第$l$层的卷积核的维度为$f^{[l]}\times f^{[l]}\times n^{[l-1]}_c$，第$l$层的激活值$a^{[l]}$的维度为$n^{[l]}_H\times n^{[l]}_W\times n^{[l]}_c$，如果采用批量梯度下降$m$个样本的激活值$A^{[l]}$的维度为$m\times n^{[l]}_H\times n^{[l]}_W\times n^{[l]}_c$，批量梯度下降的$W$的维度为$f^{[l]}\times f^{[l]}\times n^{[l-1]}_c\times n^{[l]}_c$。

#### 简单卷积网络示例

假设一张$39\times 39$的彩色图像，输入一个三层卷积网络，它的卷积过程如下，所有`padding方式`都采用`no padding`，卷积步长初始为1：

* 初始：

$$
n^{[0]}_H=n^{[0]}_W=39,n^{[0]}_c=3\\
$$

* 第一层卷积核：

$$
卷积核大小为f^{[1]}\times f^{[1]}\times n^{[0]}_c,f^{[1]}=3,n^{[0]}_c=3\\
卷积核的数量为n^{[1]}_c=10，卷积步长s^{[1]}=1\\
经过卷积和激活之后a^{[1]}的维度为n^{[1]}_H\times n^{[1]}_W\times n^{[1]}_c，其中n^{[1]}_H=\lfloor \frac{n^{[0]}_H+2p^{[1]}-f^{[1]}}{s^{[1]}}+1 \rfloor=37,同理n^{[1]}_W=37,所以维度为37\times37\times10
$$

* 第二层卷积核：

$$
卷积核大小为f^{[2]}\times f^{[2]}\times n^{[1]}_c,f^{[2]}=5,n^{[1]}_c=10\\
卷积核的数量为n^{[2]}_c=20,卷积步长s^{[2]}=2\\
经过卷积和激活之后a^{[2]}的维度为n^{[2]}_H\times n^{[2]}_W\times n^{[2]}_c，其中n^{[2]}_H=\lfloor \frac{n^{[1]}_H+2p^{[2]}-f^{[2]}}{s^{[2]}}+1 \rfloor=17,同理n^{[2]}_W=17,所以维度为17\times17\times20
$$

* 第三层卷积核：

$$
卷积核大小为f^{[3]}\times f^{[3]}\times n^{[2]}_c,f^{[3]}=5,n^{[2]}_c=20\\
卷积核的数量为n^{[3]}_c=40,卷积步长s^{[3]}=2\\
经过卷积和激活之后a^{[3]}的维度为n^{[3]}_H\times n^{[3]}_W\times n^{[3]}_c，其中n^{[3]}_H=\lfloor \frac{n^{[2]}_H+2p^{[3]}-f^{[3]}}{s^{[3]}}+1 \rfloor=7,同理n^{[3]}_W=7,所以维度为7\times7\times40
$$

然后将$a^{[3]}$矩阵中所有的值处理成为一个长向量，并放进`logistic层`或者`softmax层`得到预测值。

#### 池化层

* 举一个最大池化层的例子：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img44.jpg)

如上一个$4\times 4$的矩阵，将其分成4个部分，在每个部分取最大值，得到一个$2\times 2$的矩阵，这就是一个最大池化的例子。

最大池化只有一组超参数，并没有参数需要进行学习。超参数分别为$f=2,s=2$代表如果类似比作卷积核，卷积核的大小为2，卷积步长为2。如果输入的矩阵是3维的，输出也是3维的，比如输入$5\times5\times2$,输出也是$3\times3\times2$。

* 平均池化：

平均池化的原理和最大池化相同，只是将池化的方式变成了求平均值。

> 我们在阅读文献时，在计算神经网络使用的层数时，只计算具有参数的层数，只具有超参数的层不参与计算！
>
> 对于卷积神经网络的搭建，一般使用别人文章搭建好的超参数来修改，不要自己设置！
>
> 一般来说随着卷积神经网络的层数深入，$n_H$和$n_W$会越来越小，$n_c$会原来越大。

常见的卷积神经网络的结构：

conv-pool-conv-pool-FC-FC-FC-softmax

### 卷积神经网络示例探究

#### 经典网络

* LeNet-5：识别手写数字的经典网络

`LeNet-5`是针对灰度图片进行训练的。其网络结构如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img52.jpg)

当时早期一般使用平均池化，在现在的版本中，最后输出$\hat{y}$之前会有一个softmax回归层，参数大约为60000个，网络结构较为简单。

* AlexNet

这是针对计算机视觉大赛做的模型，其网络结构如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img53.jpg)

其中图片矩阵的宽和高不变的地方都是因为使用padding填充使得图片的宽高不变。相比LeNet-5网络，该网络的参数大约有6000万个参数。

* VGG-16

网络结构如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img54.jpg)

*注意其中的卷积$\times$2代表使用这个卷积层2次。*可以看到它是一个非常深的网络，总共包含大约1.38亿个参数。

#### 残差网络

* 残差块

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img55.jpg)

残差块是在一个深层的网络内，将前层与比较深的后层建立联系。上图以一个双线性层举例，从最上面的结构变成了下面的结构。

从公式上来说，线性层的公式为：
$$
z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}\\
a^{[l+1]}=g(z^{[l+1]})\\
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}\\
a^{[l+2]}=g(z^{[l+2]})
$$
而残差块则是将最后一个激活函数给修改了，也就是修改了上面的最后一个公式：
$$
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
$$
相当于加上了一个前层的激活值进行新的激活，这被称为是一个`short cut`，如果这种连接跨越了很多层，你可能还能听到一个术语叫`skip connection`，中文意思叫远跳连接。

我使用深度神经网络进行训练的时候会发现，随着网络深度的提升，训练的错误率会先下降后上升。而残差网络的出现，解决了这一问题，即使100层的网络也不会出现错误率提升的现象。这使得我们在训练深度网络的时候，解决了梯度消失和梯度爆炸的问题。



> 那么为什么残差网络的做法能够达到这样的效果呢？

$$
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})=a(W^{[l+2]}a^{[l+1]}+b^{[l+2]}+a^{[l]})\\
假设当到l+2层时，W^{[l+2]}和b^{[l+2]}都变成0，a^{[l+2]}=a^{[l]}\\
相当于在该层时出现了梯度消失/梯度爆炸,那么此时残差网络，就可以使网络停止在出现该问题之间。
$$

*如果$l$层和$l+2$层的维度不同，则需要对公式做一个变形*：
$$
a^{[l+2]}=g(z^{[l+2]}+W_sa^{[l]})\\
W_s使用来转换维度的，padding用0来填充。
$$



#### 网络中的网络 & 1$\times$1卷积

对于单通道的图片，1$\times$1卷积没什么用。假设你有一张多通道的图片，比如32通道，使用一个1$\times$1$\times$32的卷积核进行卷积，然后应用ReLU激活函数。这样设计的原因和在卷积的每个通道上相当于设计了一个全连接层。

1$\times$1网络的应用场景如下：假设你需要将一个$28\times28\times192$的图片转化成为$28\times28\times32$的图片，在通常情况下，需要使用padding=same的32个192通道的卷积核实现，但因为padding是有损的。这时候$1\times1\times192$的卷积核就派上用场了。

#### 谷歌Inception网络介绍

GoogLeNet是2014年Christian Szegedy提出的一种全新的深度学习结构，在这之前的AlexNet、VGG等结构都是通过增大网络的深度（层数）来获得更好的训练效果，但层数的增加会带来很多负作用，比如overfit、梯度消失、梯度爆炸等。inception的提出则从另一种角度来提升训练结果：能更高效的利用计算资源，在相同的计算量下能提取到更多的特征，从而提升训练结果。

inception结构示例如下所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img56.jpg)



> 首先我们先来举个例子看一下，1$\times$1的卷积网络可以打打降低成本。

假设一个$28\times28\times192$的网络直接通过32个$5\times5$的卷积核进行`padding=same`的变化，计算成本为$28\times28\times32\times5\times5\times192=1.2$亿次乘法运算。

如果我们在中间加若干个$1\times1\times192$的卷积核做过渡，假设加了16个卷积核，那么计算成本则变为$28\times28\times16\times192+28\times28\times32\times5\times5\times16=1240$万次乘法运算，大约缩减到了$\frac{1}{10}$。

如果对inceotion中的每一个网络都做$1\times1$的卷积，那么上图中的结构变化成如下结构：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img57.jpg)

这就是一个完整的inception模块。

#### 迁移学习

我们如何使用别人训练好的权重参数运用到自己的网络中呢？

以ImageNet数据集的网络为例，因为它是一个1000分类的问题，如何运用到我们自己的4分类问题上呢？最好的办法就是将原有的softmax层去掉，设计自己的softmax层。*还有就是训练的过程中，那些预先训练好的参数是不参与你后续的训练的：这有两种实现方式，第一种就是通过框架来调整参数，使得前面的参数不参与前向和后向传播；第二种方式就是将x输入，在训练好的参数的计算下，保存输出的结果，用于后续自己的训练。*

> 但如果你的数据集是一个很小的数据集，该如何做呢？

那么我们可能需要只冻结预训练网络的前层网络，构建自己的输出单元，训练后层网络。或者你可以将后层的网络换成自己的网络。

> 那么如果你的数据集比较大，如何做呢？

我们不需要冻结任何的网络，直接将预训练好的网络当做初始化，修改输出单元，直接进行训练。

#### 数据增强

数据增强的方式如下：

* 垂直镜像对称
* 随机裁剪
* 局部扭曲
* 色彩转化： 会使你的模型对颜色更具鲁棒性。

