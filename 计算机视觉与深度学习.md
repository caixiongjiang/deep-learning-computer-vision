# 计算机视觉与深度学习

## 深度学习

### 深度学习介绍

#### 线性整流函数（ReLU函数）

通常意义下，线性整流函数指代数学中的斜坡函数，即
$$
f(x)=max(0,x)
$$
而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出
$$
max(0,W^Tx+b)
$$
到下一层神经元或作为整个神经网络的输出。

#### 神经网络介绍

神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。

#### 监督学习

监督学习也称为带标签的学习方式。监督学习是`从标记的训练数据`来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。

#### 结构化数据vs非结构化数据

结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。

#### 深度学习的准确率

取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。

#### Sigmoid函数

sigmoid函数也叫`Logistic`函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：

优点：平滑、易于求导。

缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现`梯度消失`的情况，从而无法完成深层网络的训练。

Sigmoid函数的公式如下：
$$
S(x)=\frac{1}{1+e^{-x}}
$$
函数图形如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img3.jpg)



### 深度学习基础

> 为了方便学习：
>
> 1.使用$(x,y)$来表示一个单独的样本
>
> 2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in \{0,1\}$代表标签$y$值为0或1
>
> 3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
>
> 4.$m=m_{train}+m_{test}$
>
> 5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&|&&|\\ x^{\left( 1\right)  }&x^{\left( 2\right)  }&\cdots &x^{\left( m\right)  }\\ |&|&&|\end{matrix} \right]  $，$m$是训练集样本的个数。
>
> 6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right)  }&y^{\left( 2\right)  }&\cdots &y^{\left( m\right)  }\end{matrix} \right]  $,$Y\in\R^{1\times m}$

#### Logistic回归

Logistic回归通常用于二元分类问题。

它通常的做法是将`sigmoid函数`作用于线性回归：
$$
\hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}}
$$
这会使得$\hat{y}$的范围在0~1之间

梯度下降法中的`损失函数`如下：
$$
L(\hat{y},y)=\frac12(\hat{y}-y)^2
$$
Logistic回归中使用的`损失函数`如下：
$$
L(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))
$$
当$y=1$时，$L(\hat{y},y)=-y\log\hat{y}$，为了使损失函数较小，$\hat{y}$必须比较大，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于1；当$y=0$，$L(\hat{y},y)=-\log(1-\hat{y})$，$\hat{y}$必须比较小，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于0。

>损失函数是在单个训练样本中定义的，在全体训练样本上的表现是由代价函数来定义的。



代价函数的定义：
$$
\begin{split}
J(W,b)&=\frac{1}{m}\sum^{m}_{i\  =\  1} L\left( \hat{y}^{\left( i\right)  } ,y^{\left( i\right)  }\right) \\&=-\frac{1}{m}\sum^m_{i=1}[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]\quad \quad \quad
\text{其中}\hat{y}^{(i)}\text{代表的是预测值}，y^{(i)}代表的是真实值
\end{split}
$$

#### 梯度下降法

我们可以将梯度下降法用下图来表示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img1.jpg)

梯度下降法所做的事就是从初始点开始让$J(W,b)$朝`最陡的下坡方向`走一步,迭代次数不定。

其中$W$的迭代更新公式如下：
$$
W:=W-\alpha\frac{\partial J(W,b)}{\partial W} \quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial W} 代表该点的W对应的导数
\\b:=b-\alpha\frac{\partial J(W,b)}{\partial b}\quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial b} 代表该点的b对应的导数
$$
这样会使$W$和$b$一步一步得接近使得$J(W,b)$最小的值。

> 那么m个样本的梯度下降如何来表示呢？

其实就是对$J(W,b)$函数分别对$W$和$b$求偏导,得到全局的梯度值。

> $W$和$b$的迭代过程：
>
> 1.对$W$和$b$设定初值，计算$J(W,b)$
>
> 2.通过$J(W,b)$对参数求偏导
>
> 3.使用$W$和$b$的原值减去学习率乘以偏导来迭代更新值
>
> 4.重复1~3步骤

#### 向量化技术

如果不使用向量化技术，在面对巨大的数据集时，你会用非常多的循环去解决迭代的问题，这往往会降低代码运行的速度。向量化技术使得这种计算过程变得更加快速。

比如$f=W^T$,如果$W$有n个维度，不使用向量化一般需要用长度为n的for循环遍历求解，向量化之后则用矩阵来求解，看一段`Python`代码：

```python
import numpy as np
import time

a = np.random.rand(1000000)
b = np.random.rand(1000000)

# 非向量化使用循环
c = 0
tic = time.time()
for i in range(1000000):
  c += a[i]*b[i]
toc = time.time()
print("使用循环做法花费的时间为" + str(1000*(toc - tic)) + "ms")

# 使用向量化技术
tic = time.time()
c = np.dot(a, b)
toc = time.time()
print("使用向量化技术花费的时间为" + str(1000*(toc - tic)) + "ms")

# 运行结果如下：（保留一位小数）
# 使用循环做法花费时间为474.3ms
# 使用向量化技术花费的时间为1.5ms
```

> 我们在编写神经网络的时候，尽量要避免使用for循环

再举个例子：
$$
v=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  ==>u=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  
$$
可以这样编程（尽量使用numpy）:

```python
import numpy as np

# np.random.randint(a, b, size=(c, d)):
# 注：a-b表示生成[a,b]数的范围，后面size表示生成矩阵的大小
n = 10000
v = np.random.randint(10,11,(1,n))

# 原始方法（for循环）
u = np.zero((n,1))
for i in range(n):
  u[i]=math.exp(v[i])
  
# 使用numpy的内置函数，能比原来快很多
u = np.exp(v)
# 同样还有np.log() np.abs() np.maximum(v,0) v**2 1/v
```

使用numpy简易表示Logistic回归的一轮迭代：

```python
# Z = w^T*X+b
Z = np.dot(w.T,X)+b
# A = sigmoid(Z) 
def sigmoid_func(Z):
	return 1/(1+np.exp(-z))
A = sigmoid_func(Z)
dZ = A - Y
dw = 1/m * dZ
db = 1/m * np.sum(dZ)
w = w - a * dw # a代表学习率
b = b - a * db 
```

#### Python中的广播

|            | 苹果 | 牛肉  | 鸡蛋 | 土豆 |
| ---------- | ---- | ----- | ---- | ---- |
| 碳水化合物 | 56.0 | 0.0   | 4.4  | 68.0 |
| 蛋白质     | 1.2  | 104.0 | 52.0 | 8.0  |
| 脂肪       | 1.8  | 135.0 | 99.0 | 0.9  |

求每种食物的每项指标占比：

```python
import numpy as np

A = np.array([[56.0, 0.0, 4.4, 68.0],
              [1.2, 104.0, 52.0, 8.0],
              [1.8, 135.0, 99.0 0.9]])

# 对矩阵进行竖直方向求和
cal = A.sum(axis=0)
# reshape是O(1)操作，放心使用
# 这里的广播是将3*4的矩阵除以1*4的矩阵，然后进行自动广播
percentage = 100*A/cal.reshape(1,4)
```

再举一个特殊的例子：

```python
import numpy as np

A = np.array([[1],
       				[2],
              [3],
              [4]])
# 广播
A = A + 100
print(A)
# 结果：
# [[101]
#  [102]
#  [103]
#  [104]]
```

如上所示，广播的规则如下：

一个$m\times n$的矩阵`加减乘除`一个$1\times n$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times n$的矩阵`加减乘除`一个$m\times 1$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times 1$的矩阵`加减乘除`一个常数，python就会自动把它复制成$m\times 1$的矩阵

一个$1\times m$的矩阵`加减乘除`一个常数，python就会自动把它复制成$1\times m$的矩阵

#### numpy的使用

```python
import numpy as np

# 并不是一个向量，而是一个秩为1的数组
a = np.random.randn(5)
print(a)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(a.shape)
# (5,)
print(a.T)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(np.dot(a,a.T))
# 6.23019719213342

b = np.random.randn(5,1)
print(b)
# [[ 1.83847239]
#  [ 0.43958321]
#  [-0.87437944]
#  [ 0.70296355]
#  [-0.1833722 ]]
print(b.T)
# [[ 1.83847239  0.43958321 -0.87437944  0.70296355 -0.1833722 ]]
print(np.dot(b,b.T))
# [[ 3.37998075  0.8081616  -1.60752246  1.29237908 -0.33712473]
#  [ 0.8081616   0.1932334  -0.38436252  0.30901097 -0.08060734]
#  [-1.60752246 -0.38436252  0.7645394  -0.61465687  0.16033688]
#  [ 1.29237908  0.30901097 -0.61465687  0.49415775 -0.12890397]
#  [-0.33712473 -0.08060734  0.16033688 -0.12890397  0.03362536]]
```

从上面的例子可以看出，我们构建向量时尽量构建b这种类型的向量，不要使用数组，可以避免不必要的错误。



为了我们程序的运行正确，少点bug，可以使用assert声明函数。Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况。语法为`assert (表达式)`。

其中`np.squeeze()`可以将数组变成一个向量。

#### 作业一

使用numpy手写Logistic回归，这里只写回归部分，数据处理部分略过：

两个偏导数公式如下：

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$

$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$

```python
import numpy as np
# ----------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# ----------------------------------
def initialize_with_zeros(dim):
    """
    Argument:
    dim -- 输入数据的维度
    
    Returns:
    w -- 初始化维度为(dim, 1)的向量
    b -- 初始化标量
    """
    w = np.zeros((dim,1))
    b = 0
  
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
  
    return w, b
# ----------------------------------
def propagate(w, b, X, Y):
    """
    代价函数
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
  
    Returns:
    cost -- 代价
    dw -- 损失相对于 w 的梯度，因此维度与 w 相同
    db -- 损失相对于 b 的梯度，因此维度与 b 相同
    """
  
    m = X.shape[1] # 样本数量
    A = sigmoid(np.dot(w.T, X) + b) # 预测值
    cost = -(1.0/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
  
    dw = (1.0/m) * np.dot(X, (A - Y).T)
    db = (1.0/m) * np.sum(A - Y)
  
    assert(dw.shape == w.shape)
    assert(db.shape == b.shape)
    cost = np.squeeze(cost) # 将数组转化为向量（这里为防止bug）
    assert(cost.shape == ())
  
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
# ----------------------------------
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    w和b的迭代优化
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
    num_iterations -- 优化迭代的次数
    learning_rate -- 学习率
    print_cost -- 如果为true，每迭代100次打印一次损失
  
    Returns:
    params -- 包含权重 w 和偏差 b 的字典
    grads -- 包含权重梯度和相对于成本函数的偏差梯度的字典
    costs -- 优化期间计算的所有成本的列表，这将用于绘制学习曲线。
    """
  
    costs = []
    for i in range(num_iterations):
        grads, cost = propagate(w, b, X, Y)
    
        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

        if i % 100:
            costs.append(cost)

        if print_cost and i % 100 == 0:
            print("Cost after iteration %i:%f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads
# ----------------------------------
def predict(w, b, X):
    """
    使用学习的逻辑回归参数 (w, b) 预测标签是 0 还是 1

    Arguments:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 样本数量)维度的数据
    
    Returns:
    Y_prediction -- 一个numpy数组（向量），包含X中示例的所有预测（0/1）
    """
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):
        if A[0, i] > 0.5:
            Y_prediction[0, i] = 1
        else:
            Y_prediction[0, i] = 0
    
    assert(Y_prediction.shape == (1, m))

    return Y_prediction
# ----------------------------------
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    构建逻辑回归模型

    Arguments:
    X_train -- 训练样本 shape:(num_px * num_px * 3, m_train)
    Y_train -- 训练标签 shape:(1, m_train)
    X_test -- 测试样本 shape:(num_px * num_px * 3, m_test)
    Y_test -- 测试标签 shape:(1, m_test)
    num_iterations -- 迭代次数 默认为2000
    learning_rate -- 学习率 默认为0.5
    print_cost -- 是否打印代价
    
    Returns:
    d -- 包含模型信息的字典
    """

    w, b = initialize_with_zeros(X_train.shape[0])
    # 训练
    parameter, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
    # 训练结果
    w = parameter["w"]
    b = parameter["b"]
    # 预测
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)
    # 打印预测结果
    print("训练集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("测试集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    d = {"cost": costs,
         "测试集预测正确个数": Y_prediction_test,
         "训练集预测正确个数": Y_prediction_train,
         "w": w,
         "b": b,
         "学习率": learning_rate,
         "迭代轮数": num_iterations}

    return d
  
## 最后就可以使用model函数对已经经过数据处理的训练集和测试集进行训练和预测了

```

### 神经网络编写

#### 神经网络表示

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img2.jpg)

如上图所示，这是一个`双层神经网络`（一般输入层不作为层数）。最左边为输入层，代表单个样本的输入特征数；中间为隐藏层；最右边为输出层，一般代表预测值。中间的隐藏层是代表特征与预测值关系的一些表达式，类似于机器学习中的$W$和$b$。在这个图中，$W$是一个$4\times 3$的矩阵，$b$是一个$4\times 1$的矩阵，4代表隐藏层的个数，3代表输入的特征。

*需要注意的是这里的W和Logistic中讲的W是不一样的：因为这里的W是指整个隐藏层的W，计算时不用转置（4$\times$

3）；而Logostic中的W相当于只有一个节点的W，且计算时需要转置(3$\times$1)。*

#### 神经网络的计算

将每个隐藏层分开单独和左边的输入层结合在一起看，神经网络其实就是多个类似于Logistic回归的结构。

所以上图隐藏层的计算过程如下：
$$
z^{[1]}_1 = {w^{[1]}_1}^{T}x+b^{[1]}_1,a^{[1]}_1=\sigma(z^{[1]}_1)\\
z^{[1]}_2 = {w^{[1]}_2}^{T}x+b^{[1]}_2,a^{[1]}_2=\sigma(z^{[1]}_2)\\
z^{[1]}_3 = {w^{[1]}_3}^{T}x+b^{[1]}_3,a^{[1]}_3=\sigma(z^{[1]}_3)\\
z^{[1]}_4 = {w^{[1]}_4}^{T}x+b^{[1]}_4,a^{[1]}_4=\sigma(z^{[1]}_4)\\
其中[]里代表的数字是第几层,这里是从隐藏层算起\\下标的值代表的是该层的第几个节点\\
\sigma(z)代表激活函数
$$
所以将上面的双层神经网络整个计算过程合并起来就变成了：
$$
第一层：隐藏层\\
z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[1]}=\sigma(z^{[1]})\\
第二层：输出层\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\
a^{[2]}=\sigma(z^{[2]})
$$

#### 多个样本的向量化

上面讲的计算过程是单个样本的计算过程，如果是多个样本，就要使用一个循环来计算。但是前面讲过样本的遍历可以使用向量化技术来加快运算速度。

所以我们把z和a关于样本的多个列合并在一起：
$$
Z^{[1]}=[z^{[1](1)},z^{[1](2)},\dots z^{[1](m)}]\\
A^{[1]}=[a^{[1](1)},a^{[1](2)},\dots a^{[1](m)}]\\
Z^{[2]}=[z^{[2](1)},z^{[2](2)},\dots z^{[2](m)}]\\
A^{[2]}=[a^{[2](1)},a^{[2](2)},\dots a^{[2](m)}]\\
m代表样本的数量,[]的值代表不同的层，行代表不同的节点(也叫隐藏单元)，列代表不同的样本
$$
所以计算过程变为了：
$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})\\
这里的b不需要变是因为python自带的广播技术
$$

#### 多种激活函数

上面我们使用的激活函数为$\sigma(z)$也就是`sigmoid函数`。现在我们要介绍多种激活函数来进行对比：

* $tanh(z)$:

$$
a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\quad \quad 它的取值范围在[-1,1]\\
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img4.jpg)

该函数的特点是所有的数据平均值接近0，如果需要进行该种数据中心化可以使用该函数。

通常来说激活函数选取$tanh(z)$都比使用`sigmoid`函数更好。但有一个例外是输出层，输出层经常使用`sigmoid函数`，或者使用二元分类时，使用`sigmoid函数`。为了表示不同的层之间使用不同的激活函数，我们通常会将激活函数用$g$来表示，使用$g^{[i]}$表示第i层的激活函数。

`sigmoid函数`和$tanh(z)$函数共同的缺点是当$z$的值很大或者很小的时候，函数的斜率很接近0，也就是我们经常会说的梯度消失，拖慢梯度下降算法。

* ReLU：

$$
a=max(0,z)
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img5.jpg)

* Leaky ReLU：
  $$
  a=max(cz,z)\quad \quad  c在这里是一个常数，通常取一个比较小的数，比如0.01或者0.001
  $$
  图像如下：

  ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img6.jpg)

  ReLU的缺点是，当$z$的值为负数的时候，它没有导数值。而Leaky ReLU解决了这个问题。

> 激活函数如何选择？

激活函数的选择经验：

1.如果你在做二元分类时，输出一般为0或1，那么该网络的输出层激活函数选择sigmoid函数较好，其他所有单元都使用ReLU函数。使用ReLU函数最大的好处就是梯度下降比较快，也就是收敛的比较快。

2.有时候特定情况下会使用tanh(z)函数。

3.ReLU函数是最常用的激活函数

> 为什么神经网络需要使用激活函数？

我们来做一个公式推导：
$$
如果不使用激活函数:\\
a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})\\
=W^{'}x+b^{'}
$$
可以看到如果不使用激活函数，无论你使用多庞大的神经网络，都始终在做线性激活函数，这就退化成了线形回归的内容。

#### 激活函数的导数

当你使用神经网络进行反向传播时，需要计算激活函数的斜率或者导数。

* sigmoid函数

$$
a=g(z)=\frac{1}{1+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))=a(1-a)
$$

* tanh函数

$$
a=g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=1-(\frac{e^z-e^{-z}}{e^z+e^{-z}})^2=1-g(z)^2=1-a^2
$$

* ReLU

$$
a=g(z)=max(0,z)\\
g^{'}(z)=\begin{cases}
0,& \text{如果}z<0\\
1,& \text{如果}z>0\\
undefined,&\text{如果}z=0
\end{cases}
$$

* Leaky ReLU

$$
g(z)=max(0.01z,z)\\
g^{'}(z)=\begin{cases}
0.01& \text{如果}z<0\\
1& \text{如果}z>0
\end{cases}
$$

#### 神经网络的梯度下降法

以单隐藏层为例，写出它们的正向传播和反向传播的过程：

* 正向传播：

$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})
$$

* 反向传播：

$$
dZ^{[2]}=A^{[2]}-Y\\
dW^{[2]}=\frac{1}{m}dZ^{[2]}{A^{[1]}}^T\\
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\\
dZ^{[1]}={W^{[2]}}^TdZ^{[2]}*{g^{[1]}}^{'}(Z^{[1]})\\
这里的*代表逐个元素乘积\\
dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T\\
db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)
$$

> np.sum的axis不同时：
>
> 1.np.sum(axis = 0)代表矩阵最外维度相加（如果最外维度为n，可以理解为n个二维矩阵直接相加）
>
> 2.np.sum(axis = 1)代表矩阵中间维度相加（相当于是二维矩阵内部对每列求和）
>
> 3.np.sum(axis = 2)代表矩阵最内维度相加（相当于是二维矩阵内部对每行求和）
>
> keepdims=True是为了保证不输出秩为1的数组

#### 随机初始化

在Logistic回归中，我们把$w$和$b$都初始化为0向量，在神经网络中$W$不能这么初始化为0矩阵。因为这样会导致第一层在做计算时，每个隐藏单元所做的计算都是一模一样的，在反向传播时，不同隐藏单元激活函数的导数$dz^{[1]}_1$和$dz^{[1]}_2$是一样的。

我们的做法一般是对$W$随机初始化:
$$
W^{[1]}=np.random.randn((x,y))\times 0.01\\
b^{[2]}=np.zeros((y,1))\\
0.01代表权重，一般取比较小的值,这样能使梯度下降更快一些\\这在使用sigmoid作为激活函数的网络更为明显（z值太大,导数接近0）；\\x代表输入层的特征数；\\y代表隐藏层的隐藏单元数目。
$$

#### 作业二

写一个双层神经网络(没有数据处理的部分)：

```python
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# -----------------------------------------
def layer_sizes(X, Y):
    """
    Arguments:
    X -- 输入数据 (输入层大小, 样本数量)
    Y -- 标签 (输出层大小, 样本数量)
    
    Returns:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小
    """

    n_x = X.shape[0] # 输入层的大小
    n_h = 4
    n_y = Y.shape[0] # 输出层的大小

    return (n_x, n_h, n_y)


# -----------------------------------------
def initialize_parameters(n_x, n_h, n_y):
    """
    Arguments:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小

    Returns:
    params -- 初始化参数的字典:
              W1 -- weight matrix of shape (n_h, n_x)
              b1 -- bias vector of shape (n_h, 1)
              W2 -- weight matrix of shape (n_y, n_h)
              b2 -- bias vector of shape (n_y, 1)
    """

    np.random.seed(2) # 设置随机种子

    W1 = np.random.randn((n_h, n_x))
    b1 = np.zeros((n_h, 1))
    W2 = np.random.rand((n_y, n_h))
    b2 = np.zeros((n_y, 1))

    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def forward_propagation(X, parameters):
    """
    前向传播计算

    Argument:
    X -- 输入数据 (n_x, m)
    parameters -- 初始化参数的字典 (output of initialization function)
    
    Returns:
    A2 -- 第二层sigmoid激活函数输出的结果
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    """    

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1) # 隐藏层使用tanh激活函数
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2) # 输出层使用sigmoid激活函数

    assert(A2.shape == (1, X.shape[1])) # X.shape[1]代表样本数量

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache

# -----------------------------------------
def compute_cost(A2, Y, parameters):
    """
    计算代价函数 (13)
    
    Arguments:
    A2 -- 第二层sigmoid激活函数输出的结果 维度(1, number of examples)
    Y -- 正确的标签 维度(1, number of examples)
    parameters -- 初始化参数的字典 W1, b1, W2 and b2
    
    Returns:
    cost -- 代价函数结果
    """

    m = Y.shape[1] # 样本数量

    # 计算代价函数
    # np.multiply(X, Y)是指X和Y对应位置两两相乘
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
    cost = -np.sum(logprobs) / m

    cost = np.squeeze(cost) # 确保代价为我们期望的维度
    
    # isinstance() 函数来判断一个对象是否是一个已知的类型
    assert(isinstance(cost, float))

    return cost

# -----------------------------------------
def backward_propagation(parameters, cache, X, Y):
    """
    后向传播
    
    Arguments:
    parameters -- 参数初始化字典 
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    X -- 输入数据 维度(2, number of examples)
    Y -- 正确标签 维度(1, number of examples)
    
    Returns:
    grads -- 参数渐变的字典
    """

    m = X.shape[1]

    W1 = parameters["W1"]
    W2 = parameters["W2"]

    A1 = cache["A1"]
    A2 = cache["A2"]

    # 后向传播计算
    # tanh()函数的导数为 g'(a) = 1 - a^2
    
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # 1-np.power(A1, 2)为tanh的导数
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True)/ m

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads

# -----------------------------------------
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    中间权重向量更新
    
    Arguments:
    parameters -- 更新前的参数 
    grads -- 用于参数更新的逆向传播参数
    
    Returns:
    parameters -- 更新后的参数
    """

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]

    # 权重向量更新
    W1 = W1 - dW1 * learning_rate
    b1 = b1 - db1 * learning_rate
    W2 = W2 - dW2 * learning_rate
    b2 = b2 - db2 * learning_rate

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- 循环迭代的次数
    print_cost -- 如果为True,每1000次打印一次代价
    
    Returns:
    parameters -- 训练好的参数，用于预测
    """

    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]

    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    # 循环
    for i in range(0, num_iterations):
        # 计算前向传播
        A2, cache = forward_propagation(X, parameters)
        
        # 计算代价
        cost = compute_cost(X, parameters)

        # 计算后向传播
        grads = backward_propagation(parameters, cache, X, Y)

        # 更新权向量
        parameters = update_parameters(parameters, grads) # 学习率直接设置为默认值1.2

        if print_cost and (i % 1000):
            print("第i次迭代之后的代价为:" + str(cost))

    return parameters


# ----------------------------------------- 
def predict(parameters, X):
    """
    通过训练好的权重来预测X的类型
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- 输入数据 维度 (n_x, m)
    
    Returns
    predictions -- 模型预测的结果 (red: 0 / blue: 1)
    """

    A2, cache = forward_propagation(X, parameters)
    prediction = (A2 > 0.5) # sigmoid函数的判别方式

    return prediction

# -----------------------------------------
# 使用：

# 构建双层神经网络模型
parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)

# 绘制决策边界
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0, :])
plt.title("Decision Boundary for hidden layer size " + str(4))
```

### 多层的深层神经网络

#### 神经网络的表示

1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4

2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量

3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$

#### 深层网络中的前向传播

神经网络中每层的前向传播过程：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})\\
l代表层数
$$
**如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。**

#### 核对矩阵中的维数

如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。

通过一个具体的网络来手动计算一下维度：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img7.jpg)

可以写出该网络的部分参数如下：
$$
n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1
$$
由于前向传播的公式为：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})
$$

> 需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。

1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。

2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$

所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。

*可以总结出来的是：*
$$
W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\\
b^{[l]}的维度一定是n^{[l]}\times 1
$$
*同理在反向传播时：*
$$
dW和W的维度必须保持一致，db必须和b保持一致
$$
因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。

#### 参数vs超参数

参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$

超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。

#### 作业三

一个神经网络工作原理的模型如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img8.jpg)

多层网络模型的前向传播和后向传播过程如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img9.jpg)

* 实现一个L层神经网络

```python
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward
import numpy as np


# ------------------------------------------
def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- 包含网络中每一层的维度的Python List
    
    Returns:
    parameters -- Python参数字典 "W1", "b1", ..., "WL", "bL":
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims) # 网络层数

    for i in range(1, L):
        parameters["W" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01
        parameters["b" + str(i)] = np.zeros(layer_dims[i], 1)

        assert(parameters["W" + str(i)].shape == (layer_dims[i], layer_dims[i - 1]))
        assert(parameters["b" + str(i)].shape == (layer_dims[i], 1))

    return parameters

# ------------------------------------------
def linear_forward(A, W, b):
    """
    实现前向传播的线性部分

    Arguments:
    A -- 来自上一层的激活结果 (或者为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)

    Returns:
    Z -- 激活函数的输入或称为预激活参数 
    cache -- Python参数字典包含"A", "W" and "b" ; stored for computing the backward pass efficiently
    """

    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))

    cache = (A, W, b)

    return Z, cache

# ------------------------------------------
def linear_activation_forward(A_prev, W, b, activation):
    """
    实现 线性——>激活层 的前向传播

    Arguments:
    A_prev -- 来自上层的激活结果 (或为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)
    activation -- 当前隐藏层使用的激活函数: "sigmoid" or "relu"

    Returns:
    A -- 激活函数的输出,也称为激活后值 
    cache -- Python字典包含 "线性缓存" and "激活缓存";
             stored for computing the backward pass efficiently
    """

    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    assert(A.shape == (W.shape[0], A.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache

# ------------------------------------------
# 为了实现L层神经网络更加方便，需要将前L-1层的激活函数设置为ReLU，最后一层输出层激活函数设置为Sigmoid
def L_model_forward(X, parameters):
    """
    实现前向传播： the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    
    Arguments:
    X -- 初始数据, numpy array of shape (输入层大小, 样本数量)
    parameters -- 初始化deep网络的参数输出
    
    Returns:
    AL -- 上一层激活后的值
    caches -- cache的列表:
                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """

    caches = []
    A = X
    L = len(parameters) // 2
    # 前L-1层为relu激活函数
    for i in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters["W" + str(i)], parameters["b" + str(i)], "relu")
        caches.append(cache)
    # 最后一层为sigmoid激活函数
    AL, cache = linear_activation_forward(A, parameters["W" + str(L)], parameters["b" + str(L)], "sigmoid")
    caches.append(cache)

    assert(AL.shape == (1, X.shape[1]))

    return AL, caches

# ------------------------------------------
def compute_cost(AL, Y):
    """
    计算代价函数 使用Logistic回归中使用的代价函数

    Arguments:
    AL -- 对应于标签的预测概率向量, shape (1, 样本数)
    Y -- 正确的样本 (for example: containing 0 if non-cat, 1 if cat), shape (1, 样本数)

    Returns:
    cost -- 交叉熵代价
    """ 

    m = Y.shape[1]
    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), (1 - Y).T)) / m

    cost = np.squeeze(cost) # 使得cost的维度是我们想要的（比如将[[17]]变成17）
    assert(cost.shape == ())

    return cost

# ------------------------------------------
def linear_backward(dZ, cache):
    """
    单层实现反向传播的线性部分(l层)

    Arguments:
    dZ -- 代价函数对于线性输出的梯度 (l层)
    cache -- 元组(A_prev, W, b) 来自当前层的前向传播

    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m 
    dA_prev = np.dot(W.T, dZ)

    assert(dA_prev.shape == A_prev.shape)
    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)

    return dA_prev, dW, db

# ------------------------------------------
def linear_activation_backward(dA, cache, activation):
    """
    实现 线性——>激活 过程的反向传播
    
    Arguments:
    dA -- 当前层l激活后的梯度
    cache -- 元组 (linear_cache, activation_cache) 为了有效计算后向传播而存储
    activation -- 当前层的激活函数, stored as a text string: "sigmoid" or "relu"
    
    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度,和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db

# ------------------------------------------
def L_model_backward(AL, Y, caches):
    """
    前L-1层为ReLU激活函数,最后一层为sigmoid函数的后向传播实现
    
    Arguments:
    AL -- 前向传播输出的概率向量 (L_model_forward())
    Y -- 真实值的向量 (containing 0 if non-cat, 1 if cat)
    caches -- 包含cache的列表:
                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
    
    Returns:
    grads -- 带有渐变值的字典
             grads["dA" + str(l)] = ... 
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ... 
    """

    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # 经过这一行转化，Y的维度和AL维度相同

    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    current_cache = caches[L - 1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation="sigmoid")

    for i in reversed(range(L-1)):
        current_cache = caches[i]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(i + 2)], current_cache, activation="relu")
        grads["dA" + str(i + 1)] = dA_prev_temp
        grads["dW" + str(i + 1)] = dW_temp
        grads["db" + str(i + 1)] = db_temp

    return grads

# ------------------------------------------
def update_parameters(parameters, grads, learning_rate):
    """
    使用梯度下降更新参数
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients, output of L_model_backward
    
    Returns:
    parameters -- 更新后的参数字典
                  parameters["W" + str(l)] = ... 
                  parameters["b" + str(l)] = ...
    """   

    L = len(parameters) // 2 # 神经网络中的层数

    for i in range(1, L + 1):
        parameters["W" + str(i)] -= learning_rate * grads["dW" + str(i)]
        parameters["b" + str(i)] -= learning_rate * grads["db" + str(i)]

    return parameters



```



### 有效运行神经网络

深度学习网络是一个需要迭代得到结果的模型。它的超参数调整过程：想法->编码->实验->修改想法，需要不断地尝试，才能学习到调参的经验。

#### 训练集和测试集划分

我们一般将数据分为三个部分：

1.训练集：为训练模型准备的数据

2.验证集：通过交叉验证集选择最优模型

3.测试集：对模型进行评估

> 划分训练测试集最常见的比例是什么？

如果明确指定验证集，一般训练和测试集的比例为7:3；如果指定验证集，那么一般比例为6:2:2。这一般在数据量在10000条以下都是最好的划分比例。但在大数据时代，如果数据总量比较大，比如是百万条，那么验证集和测试集的比例还需要减少，比如98:1:1，也是合理的。

*需要特别注意的是：要保证验证集和测试集来自同一分布，这样会使得你的机器学习算法变得更快。*

#### 偏差和方差

根据数据集的分布状况，可以分为以下三种：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img10.jpg)

如上图，最左边使用Logistic回归，没有大部分正确分类，属于“欠拟合”的状况；最右边使用比较复杂的神经网络，完整地分出了两个类别，属于”过拟合“的情况（因为部分输入数据是不合理的）；中间只有和别数据分类错误，这叫“适度拟合”，是我们比较追求的一种状态。

上图这种只有一个或者两个特征的二维数据中，可以绘制数据，将偏差和方差可视化。但在多维空间数据中，绘制数据和可视化分割边界无法实现。

> 我们在多维空间通常通过两个个指标,来研究偏差和方差：训练集误差和测试集误差。为了便于研究，假设人眼分辨的错误率为0，这也被称为基本误差或者最优误差；假设训练集和验证集来自同一分布。如果训练集误差为0.01，测试集误差为0.11，这种情况很有可能是我们过度拟合了训练集，称之为高方差，对应于上图最右边的情况；如果训练集误差为0.15，测试集误差为0.16，这种情况很有可能是我们训练数据的时候欠拟合，称之为高偏差，对应于上图最左边的情况；如果训练集误差为0.15，测试集误差为0.3，偏差和方差都比较高，这种情况是因为你的算法模型并不适合这个任务，需要改变模型；如果训练集误差为0.005，测试集误差为0.01，偏差和方差都很低，是分类效果比较好的情况，对应上图中间的情况。

#### 模型评估调优的过程

* 首先，将进行训练之后的模型用于评估训练集的性能，如果误差高，代表欠拟合。那么你要采取的方法可能是增加训练时间或者增大网络结构或者是选择一个新网络，先将偏差降下来，拟合训练数据。（这在基本误差不同的情况下会有所不同）
* 如果模型的训练集性能很高后，可以将其用于评估测试集，如果误差高，代表过拟合。那么你要采取的方法最好是采用更多的训练数据，如果无法获得更多数据，可以通过正则化来减少过拟合，降低方差，有时候也会通过替换网络结构来实现。

#### 正则化

如果你的模型在评估是，由比较高的方差，然而你又不能拿到更多的数据集，那么我们最先想到的办法可能是正则化。

正则化有助于避免数据过度拟合，减少网络误差。

* Logistic正则化

假设我们的目标是找到$w,b$来使得$J(w,b)$达到最小值，且使用逻辑回归的代价函数，那么
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})
$$
我们在该函数中加入正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert_2}^2\\
{\Vert w \Vert_2}^2=\sum^{n_x}_{j=1}w_{j}^2=w^Tw \quad 这被称为L2正则化
$$
同理也会有L正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}{\Vert w \Vert_1}\\
{\Vert w \Vert_1}=\sum^{n_x}_{j=1}\lvert w_j \rvert \quad 这被称为L1正则化
$$
通常我们都会使用`L2正则化`来实现降低方差的效果。

>那么$\lambda$的值我们需要如何确定呢？

我们通常使用验证集或者交叉验证来配置$\lambda$参数，不过首先要考虑训练集之间的权衡，把参数$w,b$正常值设为较小的值，避免过拟合，不断调整超参数$\lambda$的值来减小方差。

*需要特别说明的是在编写代码的时候，python语言中lambda是一个保留字段，所以编程时我们通常使用lambd来代替lambda。*

* 神经网络正则化

$$
J(W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]})=\frac1m\sum^n_{i=1}L(\hat{y}^{(i)},y^{(i)})+ \frac{\lambda}{2m}\sum^L_{l=1}{\Vert W^{[l]} \Vert}^2\\
{\Vert W^{[l]} \Vert}^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W_{i,j}^{[l]})^2\quad 该矩阵范数被称为是弗罗贝尼乌斯范数\\
W的维度是(n^{[l]},n^{[l-1]})
$$

如果$J(W,b)$发生了改变，那么反向传播的$dW^{[l]}$也要发生变化：

在`backprop`计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值，这样做的结果是的$W^{[l]}$会比没有正则化之前更小。因此`L2范数`也被称为`权重衰减`。

#### 正则化如何预防过拟合

如果正则化的参数$\lambda$如果设置的过大，$W^{[l]}$会变得更小，就会导致每层上的部分$w$的权重 会接近0，这相当于将部分隐藏单元给去除了，复杂的神经网络会退化称为一 个很深但是又很简单的网络，导致从过拟合直接变成欠拟合的状态。

 一个合适的$\lambda$值能够使模型的性能从过拟合到适度拟合。

>$\lambda$越大，得到迭代的$W$就越小，这相当于让部分隐藏单元的影响变小，降低模型的拟合程度，方差减小。 
>
>总体来说，正则化的效果其实就是将复杂的网络线性化，如果$\lambda$的取值设置的比较好，能达到适度拟合的效果。

#### dropout正则化

`dropout正则化`是指如果一个网络存在过拟合的情况下，可以将所有节点（隐藏单元）设置一个删除概率为0.5，那么保留该隐藏单元的概率`keep-prob`也为0.5，这个值是可以改变的，然后随机消除一些节点并删除该节点进出的连线，得到一个节点更少，规模更小的网络。如下图所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img11.jpg)

实现Dropout的方法很多，最常用的是`inverted dropout`。以一个深层神经网络的某一个隐藏层为例来解释怎么进行Dropout正则化。首先假设对于第$l$层，其激活函数为$a^{[l]}$，我们设置的保留概率`keep_prob`等于0.8，这意味着该隐藏层的所有神经元以0.8的概率得到保留。

可以将`inverted dropout`方法归纳为四步：

>1.根据`keep_prob`生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，`Dl = np.rndom.randn(Al.shape[0], Al.shape[1])`
>
>2.将$d^{[l]}$转化为0-1矩阵， `Dl = Dl < keep_prob`
>
>3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， `Al = np.muiltiply(Al, Dl)`
>
>4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以`keep_prob`,`Al = Al / keep_prob`

需要注意的是，在反向传播的时候，也需要像上面一样进行dropout操作，和前向传播关闭相同的神经元。即对于某一层的$da^{[l]}$，应该进行以下计算：

> 1.`dAl = dAl * Dl` 
>
> 2.`dAl = dAl / keep_prob`

另一点是，Dropout正则化只在训练阶段实施，在测试阶段只需要利用训练好的参数进行正向预测，而不需要进行神经元的随机失活。

三层网络的前向传播`dropout`示例代码：

```python
keep_prob = 0.8
def foward(X):
    # 3层neural network的前向传播
    A1 = np.maximum(0, np.dot(W1, X) + b1)  # 计算第一层网络的输出
    D1 = (np.random.rand(*A1.shape) < keep_prob)  # 以keep_prob为标准，判断该层结点哪些可以保留
    Z1 = np.multiply(A1, D1)     #dropout
    Z1 /= keep_prob   # 为了期望的一致，除以keep_prob

    A2 = np.maximum(0, np.dot(W2, Z1) + b2)
    D2 = (np.random.rand(*A2.shape) < keep_prob)
    Z2 = np.multiply(A2, D2) 
    Z2 /= keep_prob

    out = np.dot(W3, Z2) + b3

```

那么我们如何在不同的层设置不同的`keep_prob`，我们可以在不太会发生过拟合现象的地方设置`keep_prob`为1，在容易发生过拟合的地方将`keep_prob`设置的低一点。

使用`drop_out`正则化的缺点是不能定义明确的代价函数，那么我们使用的方法一般是先将`drop_out`关闭，确保该网络的代价函数是递减的，再打开`drop_out`进行学习。

#### 数据扩容

在我们数据不够的情况下，可以在已有的数据的基础上将数据进行稍作改变来增加数据集。举个例子，一张包含猫咪的图片，我们可以将其扩容为几张：

> 1.将猫的图片进行水平翻转，得到一张新的图片
>
> 2.将猫的图片进行局部放大，并进行裁剪

*但是最重要的一点就是经过调整后的图片，必须确保它还是一只猫。*

如果输入的是一个数字，我们可以将这些数字进行轻微扭曲，旋转，将其加入数据集。

* Early stopping：一种防止过拟合的方法。

使用该方法可以绘制训练集的误差和验证集的误差，可以用来预防过拟合。

>L2正则化和early stopping的对比：
>
>1.L2正则化训练神经网络的时间可能很长。这导致超参数搜索空间更容易分解
>
>2.L2正则化可能许多正则化参数$\lambda$的值，计算代价太高。而early stopping只运行一次梯度下降，你就可以找到W的最大值，中间值，最小值，无需多次迭代。

#### 归一化输入

如图，给定输入数据的散点图：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img12.jpg" style="zoom:67%;" />

归一化需要两个步骤：

* 1.零均值化

$$
u=\frac1m\sum_{i=1}^mX^{(i)}\\
x:=X-u
$$

意思是移动训练集，直至它完成零均值化，结果如下：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img13.jpg" style="zoom:67%;" />

* 2.归一化方差

如上图可以看到，特征$x_1$的方差比特征$x_2$大得多。使用如下公式迭代：
$$
\sigma^2=\frac1m\sum_{i=1}^mx^{(i)}**2\\
x/=\sigma^2
$$
结果如下：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img14.jpg" style="zoom:67%;" />

*需要注意的是：我们在训练集和验证集或者测试集上需要设置相同的$u$和$\sigma^2$*

*那么为什么要使用归一化呢？*

归一化会使得不同的特征的起始$W$范围较为接近，使得它们的代价函数有如下图的转变：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img15.jpg)

这样可以减少迭代，使得梯度下降法使用更大的步长。

#### 梯度消失和梯度爆炸

在深度神经网络中梯度是不稳定的，可能会变得非常小，也可能会变得非常大，这就是梯度消失和梯度爆炸。

假设$W^{[1]}$比单位矩阵稍微大一点，那么在深度神经网络中，激活函数将会是指数级增长 ；相反，假设$W^{[1]}$比单位矩阵稍微小一些，激活函数会呈现指数级下降的趋势。

那么如何解决如上的问题呢？

一般使用权重初始化来解决：$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{1}{n^{[l-1]}})$，其中$shape$代表该层$W$矩阵的维度，$n^{[l-1]}$代表上一层。如果激活函数使用了`ReLU`激活函数，$\frac{1}{n^{[l-1]}}$可以变成$\frac{2}{n^{[l-1]}}$效果更好；如果使用`tanh`激活函数，使用$\frac{1}{n^{[l-1]}}$；有时候也会看到使用$np.sqrt(\frac{1}{n^{[l-1]}+n^{[l]}})$。

#### 梯度检验

关于梯度的数值逼近，一般使用`双边误差`公式，即
$$
f^{'}(\theta)=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}
$$
我们通常会通过`梯度检验`来验证backprop过程的正确实施：

首先，我们需要将$W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]}$重新组合成为一个大的向量$\theta$；同样的，在反向传播的过程也需要将$dW^{[1]},db^{[1]},\dots,dW^{[L]},db^{[L]}$重新组合成为一个大的向量$d\theta$。
$$
J(\theta)=J(\theta_1,\theta_2,\dots)
$$
然后对于每个$i$：
$$
{d\theta_{approx}}^{[i]}=\lim_{\epsilon\rightarrow0}\frac{J(\theta_1,\theta_2,\dots,\theta_{i+\epsilon},\dots)-J(\theta_1,\theta_2,\dots,\theta_{i-\epsilon},\dots)}{2\epsilon}\approx d\theta^{[i]}=\frac{\partial J}{\partial \theta_i}
$$
做完计算之后，需要做的就是验证是否：
$$
d\theta_{approx}\approx d\theta
$$
如果上述两个量差值的二范数在$10^{-7}$量级，这代表导数逼近很有可能是正确的；如果在$10^{-5}$，就需要担心是否是正确的；如果在$10^{-3}$，那说明你的梯度下降传播程序出现了bug。

> 注意事项：
>
> 1.梯度检验是非常耗费时间的，在训练的时候不使用梯度检验，只有在debug的时候使用。
>
> 2.如果算法的梯度检验失败，要检查每一项，找出bug。
>
> 3.如果代价函数包含了正则项$\frac{\lambda}{2m}{\Vert w \Vert_2}^2$，那么$d\theta_{approx}$也需要多加一个正则项。
>
> 4.梯度检验和dropout正则化不能同时使用。所以在梯度检验的时候，需要先关闭dropout正则化。

#### 作业四

* N维梯度检验：

其工作原理如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img16.jpg)

前向传播和后向传播示例代码：

```python
import numpy as np
from gc_utils import sigmoid, relu, dictionary_to_vector, vector_to_dictionary, gradients_to_vector

#-------------------------------------------------
def forward_propagation_n(X, Y, parameters):
    """
    Implements the forward propagation (and computes the cost) presented in Figure 3.
    
    Arguments:
    X -- training set for m examples
    Y -- labels for m examples 
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                    W1 -- weight matrix of shape (5, 4)
                    b1 -- bias vector of shape (5, 1)
                    W2 -- weight matrix of shape (3, 5)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    
    Returns:
    cost -- the cost function (logistic cost for one example)
    """
    
    # retrieve parameters
    m = X.shape[1]
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]

    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)

    # Cost
    logprobs = np.multiply(-np.log(A3),Y) + np.multiply(-np.log(1 - A3), 1 - Y)
    cost = 1./m * np.sum(logprobs)
    
    cache = (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3)
    
    return cost, cache
  
#-------------------------------------------------
def backward_propagation_n(X, Y, cache):
    """
    Implement the backward propagation presented in figure 2.
    
    Arguments:
    X -- input datapoint, of shape (input size, 1)
    Y -- true "label"
    cache -- cache output from forward_propagation_n()
    
    Returns:
    gradients -- A dictionary with the gradients of the cost with respect to each parameter, activation and pre-activation variables.
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T) * 2
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 4./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,
                 "dA2": dA2, "dZ2": dZ2, "dW2": dW2, "db2": db2,
                 "dA1": dA1, "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
```

接下去就需要进行梯度检验了！其中涉及的字典转vector的原理图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img17.jpg)

梯度检验的代码如下：

```python
def gradient_check_n(parameters, gradients, X, Y, epsilon = 1e-7):
    """
    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n
    
    Arguments:
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters. 
    x -- input datapoint, of shape (input size, 1)
    y -- true "label"
    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)
    
    Returns:
    difference -- difference (2) between the approximated gradient and the backward propagation gradient
    """
    
    # Set-up variables
    parameters_values, _ = dictionary_to_vector(parameters)
    grad = gradients_to_vector(gradients)
    num_parameters = parameters_values.shape[0]
    J_plus = np.zeros((num_parameters, 1))
    J_minus = np.zeros((num_parameters, 1))
    gradapprox = np.zeros((num_parameters, 1))
    
    # Compute gradapprox
    for i in range(num_parameters):
        
        # Compute J_plus[i]. Inputs: "parameters_values, epsilon". Output = "J_plus[i]".
        # "_" is used because the function you have to outputs two parameters but we only care about the first one
        ### START CODE HERE ### (approx. 3 lines)
        epsilon = 0.01
        thetaplus = np.copy(parameters_values)                                      # Step 1
        thetaplus[i][0] = thetaplus[i][0] + epsilon                                # Step 2
        J_plus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaplus))                                   # Step 3
        ### END CODE HERE ###
        
        # Compute J_minus[i]. Inputs: "parameters_values, epsilon". Output = "J_minus[i]".
        ### START CODE HERE ### (approx. 3 lines)
        thetaminus = np.copy(parameters_values)                                     # Step 1
        thetaminus[i][0] = thetaminus[i][0] - epsilon                               # Step 2        
        J_minus[i], _ = forward_propagation_n(X, Y, vector_to_dictionary(thetaminus))                                  # Step 3
        ### END CODE HERE ###
        
        # Compute gradapprox[i]
        ### START CODE HERE ### (approx. 1 line)
        gradapprox[i] = (J_plus[i] - J_minus[i]) / (2*epsilon)
        ### END CODE HERE ###
    
    # Compare gradapprox to backward propagation gradients by computing difference.
    ### START CODE HERE ### (approx. 1 line)
    # np.linalg.norm()的作用是求二范数
    numerator = np.linalg.norm(grad)                                           # Step 1'
    denominator = np.linalg.norm(gradapprox)                                         # Step 2'
    difference = np.linalg.norm(grad - gradapprox) / (numerator + denominator)                                          # Step 3'
    ### END CODE HERE ###

    if difference > 1e-7:
        print ("\033[93m" + "There is a mistake in the backward propagation! difference = " + str(difference) + "\033[0m")
    else:
        print ("\033[92m" + "Your backward propagation works perfectly fine! difference = " + str(difference) + "\033[0m")
    
    return difference
```

* 权重初始化解决梯度消失和梯度爆炸问题

> 下列代码会通过三种初始化的方式进行对比：
>
> 1.将W和b都设置为0向量
>
> 2.随机设置W和b
>
> 3.在随机设置的基础上进行权重初始化

代码如下：

加载初始数据：

```python
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import sklearn.datasets
from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation
from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# load image dataset: blue/red dots in circles
train_X, train_Y, test_X, test_Y = load_dataset()
```

结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img18.jpg)

初始化模型：

```python
def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = "he"):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)
    learning_rate -- learning rate for gradient descent 
    num_iterations -- number of iterations to run gradient descent
    print_cost -- if True, print the cost every 1000 iterations
    initialization -- flag to choose which initialization to use ("zeros","random" or "he")
    
    Returns:
    parameters -- parameters learnt by the model
    """
        
    grads = {}
    costs = [] # to keep track of the loss
    m = X.shape[1] # number of examples
    layers_dims = [X.shape[0], 10, 5, 1]
    
    # Initialize parameters dictionary.
    if initialization == "zeros":
        parameters = initialize_parameters_zeros(layers_dims)
    elif initialization == "random":
        parameters = initialize_parameters_random(layers_dims)
    elif initialization == "he":
        parameters = initialize_parameters_he(layers_dims)

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        a3, cache = forward_propagation(X, parameters)
        
        # Loss
        cost = compute_loss(a3, Y)

        # Backward propagation.
        grads = backward_propagation(X, Y, cache)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 1000 iterations
        if print_cost and i % 1000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
            costs.append(cost)
            
    # plot the loss
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (per hundreds)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
```

W为0向量初始化：

```python
def initialize_parameters_zeros(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    parameters = {}
    L = len(layers_dims)            # number of layers in the network
    
    for i in range(1, L):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.zeros((layers_dims[i], layers_dims[i - 1]))
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###
    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_zeros([3,2,1])
parameters = model(train_X, train_Y, initialization = "zeros")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
def plot_decision_boundary(model, X, y):
    #import pdb;pdb.set_trace()
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    y = y.reshape(X[0,:].shape)#must reshape,otherwise confliction with dimensions
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)
    plt.show()

#---------------------------------------------
plt.title("Model with Zeros initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img19.jpg)

W为随机向量初始化：

```python
def initialize_parameters_random(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)               # This seed makes sure your "random" numbers will be the as ours
    parameters = {}
    L = len(layers_dims)            # integer representing the number of layers
    
    for i in range(1, L):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.random.randn(layers_dims[i], layers_dims[i - 1]) * 10
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###

    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_random([3, 2, 1])
parameters = model(train_X, train_Y, initialization = "random")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
plt.title("Model with large random initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img20.jpg)

W为权重初始化后的结果，因为这里使用的ReLU激活函数，W在初始化后需要变为$W^{[l]}=np.random.randn(shape)\times np.sqrt(\frac{2}{n^{[l-1]}})$：

```python
def initialize_parameters_he(layers_dims):
    """
    Arguments:
    layer_dims -- python array (list) containing the size of each layer.
    
    Returns:
    parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL":
                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])
                    b1 -- bias vector of shape (layers_dims[1], 1)
                    ...
                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])
                    bL -- bias vector of shape (layers_dims[L], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layers_dims) - 1 # integer representing the number of layers
     
    for i in range(1, L + 1):
        ### START CODE HERE ### (≈ 2 lines of code)
        parameters['W' + str(i)] = np.random.randn(layers_dims[i],layers_dims[i - 1]) * np.sqrt(2.0 / layers_dims[i - 1])
        parameters['b' + str(i)] = np.zeros((layers_dims[i], 1))
        ### END CODE HERE ###
        
    return parameters
  
#---------------------------------------------
parameters = initialize_parameters_he([2, 4, 1])
parameters = model(train_X, train_Y, initialization = "he")
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)

#---------------------------------------------
# 画图
plt.title("Model with He initialization")
axes = plt.gca()
axes.set_xlim([-1.5,1.5])
axes.set_ylim([-1.5,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

分类结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img21.jpg)

**可以看到第三种的分类效果最好！**

* L2正则化和dropout正则化的效果问题

加载数据：

```python
import numpy as np
import matplotlib.pyplot as plt
from reg_utils import sigmoid, relu, plot_decision_boundary, initialize_parameters, load_2D_dataset, predict_dec
from reg_utils import compute_cost, predict, forward_propagation, backward_propagation, update_parameters
import sklearn
import sklearn.datasets
import scipy.io
from testCases import *

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

train_X, train_Y, test_X, test_Y = load_2D_dataset()
```

数据的分布如下图：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img27.jpg)

使用无正则化模型，进行梯度下降：

```python
# 这里其实已经包含了L2正则化和dropout正则化的情况，只默认情况下的参数不使用正则化
def model(X, Y, learning_rate = 0.3, num_iterations = 30000, print_cost = True, lambd = 0, keep_prob = 1):
    """
    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (output size, number of examples)
    learning_rate -- learning rate of the optimization
    num_iterations -- number of iterations of the optimization loop
    print_cost -- If True, print the cost every 10000 iterations
    lambd -- regularization hyperparameter, scalar
    keep_prob - probability of keeping a neuron active during drop-out, scalar.
    
    Returns:
    parameters -- parameters learned by the model. They can then be used to predict.
    """
        
    grads = {}
    costs = []                            # to keep track of the cost
    m = X.shape[1]                        # number of examples
    layers_dims = [X.shape[0], 20, 3, 1]  # 网络结构为三层网络结构，分别有20，3，1个节点 
    
    # Initialize parameters dictionary.
    parameters = initialize_parameters(layers_dims)

    # Loop (gradient descent)

    for i in range(0, num_iterations):

        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.
        if keep_prob == 1:
            a3, cache = forward_propagation(X, parameters)
        elif keep_prob < 1:
            a3, cache = forward_propagation_with_dropout(X, parameters, keep_prob)
        
        # Cost function
        if lambd == 0:
            cost = compute_cost(a3, Y)
        else:
            cost = compute_cost_with_regularization(a3, Y, parameters, lambd)
            
        # Backward propagation.
        assert(lambd==0 or keep_prob==1)    # it is possible to use both L2 regularization and dropout, 
                                            # but this assignment will only explore one at a time
        if lambd == 0 and keep_prob == 1:
            grads = backward_propagation(X, Y, cache)
        elif lambd != 0:
            grads = backward_propagation_with_regularization(X, Y, cache, lambd)
        elif keep_prob < 1:
            grads = backward_propagation_with_dropout(X, Y, cache, keep_prob)
        
        # Update parameters.
        parameters = update_parameters(parameters, grads, learning_rate)
        
        # Print the loss every 10000 iterations
        if print_cost and i % 10000 == 0:
            print("Cost after iteration {}: {}".format(i, cost))
        if print_cost and i % 1000 == 0:
            costs.append(cost)
    
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('iterations (x1,000)')
    plt.title("Learning rate =" + str(learning_rate))
    plt.show()
    
    return parameters
  
  
#----------------------------------------------------
parameters = model(train_X, train_Y)
print ("On the training set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model without regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

训练集准确率为0.948，测试集准确率为0.915。分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img28.jpg)

使用L2正则化，前向传播公式变化如下：

$$J = -\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small  y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} \tag{1}$$
To:
$$J_{regularized} = \small \underbrace{-\frac{1}{m} \sum\limits_{i = 1}^{m} \large{(}\small y^{(i)}\log\left(a^{[L](i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right) \large{)} }_\text{cross-entropy cost} + \underbrace{\frac{1}{m} \frac{\lambda}{2} \sum\limits_l\sum\limits_k\sum\limits_j W_{k,j}^{[l]2} }_\text{L2 regularization cost} \tag{2}$$

在编程的时候，需要使用`np.sum(np.square(W^[l]))`，然后将所有项加起来，乘上$\frac{\lambda}{2m}$

后向传播时，在`backprop`计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值。

```python
# 前向传播
def compute_cost_with_regularization(A3, Y, parameters, lambd):
    """
    Implement the cost function with L2 regularization. See formula (2) above.
    
    Arguments:
    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    parameters -- python dictionary containing parameters of the model
    
    Returns:
    cost - value of the regularized loss function (formula (2))
    """
    m = Y.shape[1]
    W1 = parameters["W1"]
    W2 = parameters["W2"]
    W3 = parameters["W3"]
    
    cross_entropy_cost = compute_cost(A3, Y) # This gives you the cross-entropy part of the cost
    
    ### START CODE HERE ### (approx. 1 line)
    L2_regularization_cost = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2*m)
    ### END CODER HERE ###
    
    cost = cross_entropy_cost + L2_regularization_cost
    
    return cost
  
#----------------------------------------------------
# 后向传播
def backward_propagation_with_regularization(X, Y, cache, lambd):
    """
    Implements the backward propagation of our baseline model to which we added an L2 regularization.
    
    Arguments:
    X -- input dataset, of shape (input size, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation()
    lambd -- regularization hyperparameter, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """
    
    m = X.shape[1]
    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    
    ### START CODE HERE ### (approx. 1 line)
    dW3 = 1./m * np.dot(dZ3, A2.T) + lambd * W3 / m
    ### END CODE HERE ###
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    
    dA2 = np.dot(W3.T, dZ3)
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW2 = 1./m * np.dot(dZ2, A1.T) + lambd * W2 / m
    ### END CODE HERE ###
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    ### START CODE HERE ### (approx. 1 line)
    dW1 = 1./m * np.dot(dZ1, X.T) + lambd * W1 / m
    ### END CODE HERE ###
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
  
#----------------------------------------------------
# 使用L2正则化进行梯度下降
parameters = model(train_X, train_Y, lambd = 0.7)
print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model with L2-regularization")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

使用L2正则化，训练集的准确率为0.938，测试集为0.93，分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img29.jpg)

使用dropout正则化，在前向传播的时候分为4步：

> 1.根据`keep_prob`生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，`Dl = np.rndom.randn(Al.shape[0], Al.shape[1])`
>
> 2.将$d^{[l]}$转化为0-1矩阵， `Dl = Dl < keep_prob`
>
> 3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， `Al = np.muiltiply(Al, Dl)`
>
> 4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以`keep_prob`,`Al = Al / keep_prob`

后向传播也做修改，即对于某一层的$da^{[l]}$，应该进行以下计算：

> 1.`dAl = dAl * Dl` 
>
> 2.`dAl = dAl / keep_prob`

代码如下：

```python
# 前向传播
def forward_propagation_with_dropout(X, parameters, keep_prob = 0.5):
    """
    Implements the forward propagation: LINEAR -> RELU + DROPOUT -> LINEAR -> RELU + DROPOUT -> LINEAR -> SIGMOID.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    parameters -- python dictionary containing your parameters "W1", "b1", "W2", "b2", "W3", "b3":
                    W1 -- weight matrix of shape (20, 2)
                    b1 -- bias vector of shape (20, 1)
                    W2 -- weight matrix of shape (3, 20)
                    b2 -- bias vector of shape (3, 1)
                    W3 -- weight matrix of shape (1, 3)
                    b3 -- bias vector of shape (1, 1)
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    A3 -- last activation value, output of the forward propagation, of shape (1,1)
    cache -- tuple, information stored for computing the backward propagation
    """
    
    np.random.seed(1)
    
    # retrieve parameters
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    W3 = parameters["W3"]
    b3 = parameters["b3"]
    
    # LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID
    Z1 = np.dot(W1, X) + b1
    A1 = relu(Z1)
    ### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. 
    D1 = np.random.rand(*A1.shape)                    # Step 1: initialize matrix D1 = np.random.rand(..., ...)
    D1 = D1 < keep_prob                               # Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)
    A1 = np.multiply(A1, D1)                          # Step 3: shut down some neurons of A1
    A1 = A1 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z2 = np.dot(W2, A1) + b2
    A2 = relu(Z2)
    ### START CODE HERE ### (approx. 4 lines)
    D2 = np.random.rand(*A2.shape)                    # Step 1: initialize matrix D2 = np.random.rand(..., ...)
    D2 = D2 < keep_prob                               # Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)
    A2 = np.multiply(A2, D2)                          # Step 3: shut down some neurons of A2
    A2 = A2 / keep_prob                               # Step 4: scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    Z3 = np.dot(W3, A2) + b3
    A3 = sigmoid(Z3)
    
    cache = (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3)
    
    return A3, cache
  
  
#----------------------------------------------
# 后向传播
def backward_propagation_with_dropout(X, Y, cache, keep_prob):
    """
    Implements the backward propagation of our baseline model to which we added dropout.
    
    Arguments:
    X -- input dataset, of shape (2, number of examples)
    Y -- "true" labels vector, of shape (output size, number of examples)
    cache -- cache output from forward_propagation_with_dropout()
    keep_prob - probability of keeping a neuron active during drop-out, scalar
    
    Returns:
    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables
    """
    
    m = X.shape[1]
    (Z1, D1, A1, W1, b1, Z2, D2, A2, W2, b2, Z3, A3, W3, b3) = cache
    
    dZ3 = A3 - Y
    dW3 = 1./m * np.dot(dZ3, A2.T)
    db3 = 1./m * np.sum(dZ3, axis=1, keepdims = True)
    dA2 = np.dot(W3.T, dZ3)
    ### START CODE HERE ### (≈ 2 lines of code)
    dA2 = dA2 * D2              # Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation
    dA2 = dA2 / keep_prob       # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ2 = np.multiply(dA2, np.int64(A2 > 0))
    dW2 = 1./m * np.dot(dZ2, A1.T)
    db2 = 1./m * np.sum(dZ2, axis=1, keepdims = True)
    
    dA1 = np.dot(W2.T, dZ2)
    ### START CODE HERE ### (≈ 2 lines of code)
    dA1 = dA1 * D1              # Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation
    dA1 = dA1 / keep_prob       # Step 2: Scale the value of neurons that haven't been shut down
    ### END CODE HERE ###
    dZ1 = np.multiply(dA1, np.int64(A1 > 0))
    dW1 = 1./m * np.dot(dZ1, X.T)
    db1 = 1./m * np.sum(dZ1, axis=1, keepdims = True)
    
    gradients = {"dZ3": dZ3, "dW3": dW3, "db3": db3,"dA2": dA2,
                 "dZ2": dZ2, "dW2": dW2, "db2": db2, "dA1": dA1, 
                 "dZ1": dZ1, "dW1": dW1, "db1": db1}
    
    return gradients
  
#-------------------------------------------------------------------
# 使用dropout正则化来梯度下降
parameters = model(train_X, train_Y, keep_prob = 0.86, learning_rate = 0.3)

print ("On the train set:")
predictions_train = predict(train_X, train_Y, parameters)
print ("On the test set:")
predictions_test = predict(test_X, test_Y, parameters)
# 画图
plt.title("Model with dropout")
axes = plt.gca()
axes.set_xlim([-0.75,0.40])
axes.set_ylim([-0.75,0.65])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

dropout正则化在训练集的预测准确率为0.929，测试集的准确率为0.95。分类图如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img30.jpg)

### 优化算法

#### Mini-batch梯度下降法

假设我们的样本数量为500w个，那么在进行梯度下降之前，我们需要先将500w个数据整合成一个大的向量$X$。Mini-batch的做法为将500w个样本按照每个子集为1000个样本等分。每个子集标记为$X^{\left\{ 1\right\}  }X^{\left\{ 2\right\}  },\dots,X^{\left\{ 5000\right\}  }$。相应的，除了需要拆分$X$，也需要拆分标签$Y$，拆分的方法和$X$相同。

**Mini-batch的原理是将同时原本对所有样本和标签同时进行梯度下降转变为同时只对一个子集进行梯度下降处理，处理5000次**。需要注意代价函数也要改变，因为每次训练的样本个数改变了。

当你的**训练集大小很大**的时候，mini-batch梯度下降法比batch梯度下降法运行地更快。



batch梯度下降法和Mini-batch梯度下降法的代价随迭代的图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img22.jpg)

右边的图像出现波动的原因是：每次实现梯度下降的样本集不同，可能$X^{\left\{ 1\right\}  }$和$Y^{\left\{ 1\right\}  }$需要花费的代价更大，而$X^{\left\{ 2\right\}  }$和$Y^{\left\{ 2\right\}  }$花费的代价更少，从而形成一个噪声的现象。

那么mini-bash的大小如何决定呢？

> 先看两种极端情况：
>
> 如果子集的大小为m，那么mini-bash梯度下降就变成了batch梯度下降；
>
> 如果子集的大小为1，那么mini-bash梯度下降就变成了`随机梯度下降法`，每个样本都是一个子集；
>
> batch梯度下降每次下降的噪声会小一点，幅度会大一点（这里的噪声是指梯度下降的方向偏离目标）；而随机梯度下降大部分时间会向着全局最小值逼近，但有时候会远离最小值（刚好该样本是一个''坏''样本），随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。
>
> batch梯度下降在训练数据很大的时候，单次训练迭代时间过长，如果训练数据量较小的情况下效果较好；而随机梯度下降单次迭代很快，但却无法使用向量化技术对运算进行加速。我们的目的就是选择一个不大不小的size，使得我们的学习速率达到最快（梯度下降）。
>
> 最优的情况就是，单次选取的size大小的数据分布比较符合整体数据的分布，这样使得学习速率和运行效率都比较高。

#### 指数加权平均

指数加权平均也称指数加权移动平均，通过它可以来计算局部的平均值，来描述数值的变化趋势，下面通过一个温度的例子来详细介绍一下。

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img23.jpg)

上图是温度随时间变化的图像，我们通过温度的局部平均值（移动平均值）来描述温度的变化趋势，计算公式如下：
$$
v_t=\beta v_{t-1}+(1-\beta)\theta_{t}\\
v_0=0\\
v_1=0.9v_0+0.1\theta_1\\
v_2=0.9v_1+0.1\theta_2\\
\theta 代表当天的温度，v代表局部平均值
$$
当$\beta$为0.9时，可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。

当$\beta$变得越小，移动平均值的波动越大。

通过上面的公式往下推到，可以得到$v_{100}$的表达式：
$$
v_{100}=0.1\times\theta_{100}+0.1\times0.9\times\theta_{99}+\dots+0.1\times0.9^{99}\times\theta_1\\
=0.1\times\sum_{i=1}^{100}0.9^{100-i}\times\theta_i
$$
当$\epsilon=1-\beta$时，$(1-\beta)^{\frac{1}{\epsilon}}\approx\frac{1}{e}\approx\frac{1}{1-\beta}$，所以可以将$v_t$看作$\frac{1}{1-\beta}=\frac{1}{1-0.9}=10$天的平均值。

> 简单来说，普通的加权求平均值的方法每一项的权重是$\frac{1}{n}$，指数加权平均每一项的权重是指数递减的。

* 指数加权平均的偏差修正

由于我们初始设置的$v_0$为0，这样会使前面几个$v_1,v_2\dots$的值与实际值相比偏小，我们通常会采取以下的办法来修正偏差：
$$
v_t=\frac{\beta v_{t-1}+(1-\beta)\theta_t}{1-\beta^t}
$$
这样修正的效果为随着t的增加，分母越来越接近1。相当于时间越短，修正的幅度越大，所以这个公式主要是为了修正早期的偏差。

#### 动量梯度下降法

我们将上面所说的`指数加权平均`的做法应用于神经网络的反向传播过程，如下：
$$
V_{dW}=\beta V_{dW}+(1-\beta)dW\\
V_{db}=\beta V_{db}+(1-\beta)db\\
W:=W-\alpha V_{dW},b:=b-\alpha V_{db}
$$
这样做可以减缓梯度下降的幅度，因为梯度下降不一定朝着最快的方向前进。如下图所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img24.jpg)

原本为蓝色的梯度下降会变成红色，纵轴摆动的方向变小了且上下摆动的幅度均值大概为0。这样一来，即使我增加学习率或者步长也不会出现紫色线这种偏离函数的情况。

*$\beta$最常用的值为0.9，按照道理来说需要加上偏差修正。但实际上不会这么做，因为经过10次迭代之后，移动平均已经过了初始阶段，不再是一个具有偏差的预测值。*

#### RMSprop算法

通过前面的算法可知，我们加快学习效率的方法是增加$W$方向的学习速率（图中的水平方向），降低$b$方向的学习速率（垂直方向）。公式如下：
$$
S_{dW}=\beta S_{dW}+(1-\beta)(dW)^2\\
S_{db}=\beta S_{db}+(1-\beta)(db)^2\\
W:=W-\alpha\frac{dW}{\sqrt{S_{dW}}+\epsilon}\\
b:=b-\alpha\frac{db}{\sqrt{S_{db}}+\epsilon}\\
式中，S_{dW}和S_{db}表示权重W和偏置值b在t−1轮迭代中的梯度动量\\
超参数β一般取值为0.9，学习率\alpha一般取值为 0.001，ε是防止分母为零，一般去10^{-8}。
$$
我们会希望$S_{db}$较大，$S_{dW}$较小，这样可以使得$W$方向的变化更大，$b$方向的变化更小。结果变化如下图：

 ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img25.jpg)

#### Adam算法

Adam算法是将动量梯度下降法和RMSprop算法结合起来，公式如下：
$$
首先初始化V_{dW}=0,S_{dW}=0,V_{db}=0,S_{db}=0\\
在第t次迭代时：\\
使用当前的mini-batch计算dW,db\\
V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW,V_{db}=\beta_1V_{db}+(1-\beta)db\quad 动量梯度下降\\
S_{dW}=\beta_2S_{dW}+(1-\beta_2)(dW)^2,S_{db}=\beta_2S_{db}+(1-\beta_2)(db)^2\quad SMSprob\\
V_{dW}^{correct}=\frac{V_{dW}}{1-(\beta_1)^t},V_{db}^{correct}=\frac{V_{db}}{1-(\beta_1)^t}\\
S_{dW}^{correct}=\frac{S_{dW}}{1-(\beta_2)^t},S_{db}^{correct}=\frac{S_{db}}{1-(\beta_2)^t}\\
W:=W-\alpha\frac{V_{dW}^{correct}}{\sqrt{S_{dW}^{corret}}+\epsilon}\\
b:=b-\alpha\frac{V_{db}^{correct}}{\sqrt{S_{db}^{corret}}+\epsilon}\\
$$
**Adam算法被证明具有更强的普适性，适用于更加广泛的结构。**

其中上述算法中的超参数使用值：
$$
\alpha是一个需要不断调整的值\\
\beta_1推荐值为0.9\\
\beta_2推荐值为0.999\\
\epsilon推荐值为10^{-8}
$$

#### 学习率衰减

假设使用一个mini-batch的梯度下降方法，梯度下降会出现噪声，最后不会收敛，而是会在最小值之间波动。通过学习率衰减的办法，可以使得在梯度下降到最小值附近，波动的幅度变得很小。如下图所示（从蓝线到绿线的变化）：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img26.jpg)

学习率的设置公式如下：
$$
\alpha=\frac{1}{1+衰减率\times 代数}\alpha_0\\ 
这里的代数是指迭代的次数,衰减率和\alpha是一个需要调整的参数。
$$

#### 作业五

分别使用`mini-batch`，`动量梯度下降`，`Adam`算法对梯度下降进行加速。

```python
# 导入库
import numpy as np
import matplotlib.pyplot as plt
import scipy.io
import math
import sklearn
import sklearn.datasets

from opt_utils import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation
from opt_utils import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset
from testCases import *

%matplotlib inline
plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# 参数更新
def update_parameters_with_gd(parameters, grads, learning_rate):
    """
    Update parameters using one step of gradient descent
    
    Arguments:
    parameters -- python dictionary containing your parameters to be updated:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients to update each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    learning_rate -- the learning rate, scalar.
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(parameters) // 2 # number of layers in the neural networks

    # Update rule for each parameter
    for i in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(i + 1)] -= learning_rate * grads["dW" + str(i + 1)]
        parameters["b" + str(i + 1)] -= learning_rate * grads["db" + str(i + 1)]
        ### END CODE HERE ###
        
    return parameters
  
# mini-batch
def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):
    """
    Creates a list of random minibatches from (X, Y)
    
    Arguments:
    X -- input data, of shape (input size, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    mini_batch_size -- size of the mini-batches, integer
    
    Returns:
    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)
    """
    
    np.random.seed(seed)            # To make your "random" minibatches the same as ours
    m = X.shape[1]                  # number of training examples
    mini_batches = []
        
    # Step 1: Shuffle (X, Y) 洗牌
    permutation = list(np.random.permutation(m))
    shuffled_X = X[:, permutation]
    shuffled_Y = Y[:, permutation].reshape((1,m))

    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.
    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning
    for k in range(0, num_complete_minibatches):
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k + 1) * mini_batch_size]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    # Handling the end case (last mini-batch < mini_batch_size) 处理最后一个mini-batch小于64的情况。
    if m % mini_batch_size != 0:
        ### START CODE HERE ### (approx. 2 lines)
        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]
        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]
        ### END CODE HERE ###
        mini_batch = (mini_batch_X, mini_batch_Y)
        mini_batches.append(mini_batch)
    
    return mini_batches
  
  
# 动量梯度下降：1.初始化参数 2.更新参数
def initialize_velocity(parameters):
    """
    Initializes the velocity as a python dictionary with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    
    Returns:
    v -- python dictionary containing the current velocity.
                    v['dW' + str(l)] = velocity of dWl
                    v['db' + str(l)] = velocity of dbl
    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    
    # Initialize velocity
    for i in range(L):
        ### START CODE HERE ### (approx. 2 lines)
        v['dW' + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        v['db' + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
        ### END CODE HERE ###
        
    return v
  
 
def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):
    """
    Update parameters using Momentum
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- python dictionary containing the current velocity:
                    v['dW' + str(l)] = ...
                    v['db' + str(l)] = ...
    beta -- the momentum hyperparameter, scalar
    learning_rate -- the learning rate, scalar
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- python dictionary containing your updated velocities
    """

    L = len(parameters) // 2 # number of layers in the neural networks
    
    # Momentum update for each parameter
    for i in range(L):
        
        ### START CODE HERE ### (approx. 4 lines)
        # compute velocities
        v['dW' + str(i + 1)] = beta * v['dW' + str(i + 1)] + (1 - beta) * grads['dW' + str(i + 1)]
        v['db' + str(i + 1)] = beta * v['db' + str(i + 1)] + (1 - beta) * grads['db' + str(i + 1)]
        # update parameters
        parameters['W' + str(i + 1)] -= learning_rate * v['dW' + str(i + 1)]
        parameters['b' + str(i + 1)] -= learning_rate * v['db' + str(i + 1)]
        ### END CODE HERE ###
        
    return parameters, v
  
  
# Adma算法： 1.初始化V[dW],V[db],S[dW],S[db] 2.更新参数
def initialize_adam(parameters) :
    """
    Initializes v and s as two python dictionaries with:
                - keys: "dW1", "db1", ..., "dWL", "dbL" 
                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.
    
    Arguments:
    parameters -- python dictionary containing your parameters.
                    parameters["W" + str(l)] = Wl
                    parameters["b" + str(l)] = bl
    
    Returns: 
    v -- python dictionary that will contain the exponentially weighted average of the gradient.
                    v["dW" + str(l)] = ...
                    v["db" + str(l)] = ...
    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.
                    s["dW" + str(l)] = ...
                    s["db" + str(l)] = ...

    """
    
    L = len(parameters) // 2 # number of layers in the neural networks
    v = {}
    s = {}
    
    # Initialize v, s. Input: "parameters". Outputs: "v, s".
    for i in range(L):
    ### START CODE HERE ### (approx. 4 lines)
        v["dW" + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        v["db" + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
        s["dW" + str(i + 1)] = np.zeros(parameters["W" + str(i + 1)].shape)
        s["db" + str(i + 1)] = np.zeros(parameters["b" + str(i + 1)].shape)
    ### END CODE HERE ###
    
    return v, s
  
  
def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,
                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):
    """
    Update parameters using Adam
    
    Arguments:
    parameters -- python dictionary containing your parameters:
                    parameters['W' + str(l)] = Wl
                    parameters['b' + str(l)] = bl
    grads -- python dictionary containing your gradients for each parameters:
                    grads['dW' + str(l)] = dWl
                    grads['db' + str(l)] = dbl
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    learning_rate -- the learning rate, scalar.
    beta1 -- Exponential decay hyperparameter for the first moment estimates 
    beta2 -- Exponential decay hyperparameter for the second moment estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates

    Returns:
    parameters -- python dictionary containing your updated parameters 
    v -- Adam variable, moving average of the first gradient, python dictionary
    s -- Adam variable, moving average of the squared gradient, python dictionary
    """
    
    L = len(parameters) // 2                 # number of layers in the neural networks
    v_corrected = {}                         # Initializing first moment estimate, python dictionary
    s_corrected = {}                         # Initializing second moment estimate, python dictionary
    
    # Perform Adam update on all parameters
    for i in range(L):
        # Moving average of the gradients. Inputs: "v, grads, beta1". Output: "v".
        ### START CODE HERE ### (approx. 2 lines)
        v["dW" + str(i + 1)] = beta1 * v["dW" + str(i + 1)] + (1 - beta1) * grads["dW" + str(i + 1)]
        v["db" + str(i + 1)] = beta1 * v["db" + str(i + 1)] + (1 - beta1) * grads["db" + str(i + 1)]
        ### END CODE HERE ###

        # Compute bias-corrected first moment estimate. Inputs: "v, beta1, t". Output: "v_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        v_corrected["dW" + str(i + 1)] = v["dW" + str(i + 1)] / (1 - beta1 ** t)
        v_corrected["db" + str(i + 1)] = v["db" + str(i + 1)] / (1 - beta1 ** t)
        ### END CODE HERE ###

        # Moving average of the squared gradients. Inputs: "s, grads, beta2". Output: "s".
        ### START CODE HERE ### (approx. 2 lines)
        s["dW" + str(i + 1)] = beta2 * s["dW" + str(i + 1)] + (1 - beta2) * np.multiply(grads["dW" + str(i + 1)], grads["dW" + str(i + 1)])
        s["db" + str(i + 1)] = beta2 * s["db" + str(i + 1)] + (1 - beta2) * np.multiply(grads["db" + str(i + 1)], grads["db" + str(i + 1)])
        ### END CODE HERE ###

        # Compute bias-corrected second raw moment estimate. Inputs: "s, beta2, t". Output: "s_corrected".
        ### START CODE HERE ### (approx. 2 lines)
        s_corrected["dW" + str(i + 1)] = s["dW" + str(i + 1)] / (1 - beta2 ** t)
        s_corrected["db" + str(i + 1)] = s["db" + str(i + 1)] / (1 - beta2 ** t)
        ### END CODE HERE ###

        # Update parameters. Inputs: "parameters, learning_rate, v_corrected, s_corrected, epsilon". Output: "parameters".
        ### START CODE HERE ### (approx. 2 lines)
        parameters["W" + str(i + 1)] -= learning_rate * v_corrected["dW" + str(i + 1)] / (epsilon + np.sqrt(s_corrected["dW" + str(i + 1)]))
        parameters["b" + str(i + 1)] -= learning_rate * v_corrected["db" + str(i + 1)] / (epsilon + np.sqrt(s_corrected["db" + str(i + 1)]))
        ### END CODE HERE ###

    return parameters, v, s
  
 
```

使用不同算法加速的模型进行预测：

```python
# 加载数据
train_X, train_Y = load_dataset()
```

数据分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img31.jpg)

建立预测模型并使用三种不同的算法进行加速预测模型：

```python
def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,
          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 10000, print_cost = True):
    """
    3-layer neural network model which can be run in different optimizer modes.
    
    Arguments:
    X -- input data, of shape (2, number of examples)
    Y -- true "label" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)
    layers_dims -- python list, containing the size of each layer
    learning_rate -- the learning rate, scalar.
    mini_batch_size -- the size of a mini batch
    beta -- Momentum hyperparameter
    beta1 -- Exponential decay hyperparameter for the past gradients estimates 
    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates 
    epsilon -- hyperparameter preventing division by zero in Adam updates
    num_epochs -- number of epochs
    print_cost -- True to print the cost every 1000 epochs

    Returns:
    parameters -- python dictionary containing your updated parameters 
    """

    L = len(layers_dims)             # number of layers in the neural networks
    costs = []                       # to keep track of the cost
    t = 0                            # initializing the counter required for Adam update
    seed = 10                        # For grading purposes, so that your "random" minibatches are the same as ours
    
    # Initialize parameters
    parameters = initialize_parameters(layers_dims)

    # Initialize the optimizer
    if optimizer == "gd":
        pass # no initialization required for gradient descent
    elif optimizer == "momentum":
        v = initialize_velocity(parameters)
    elif optimizer == "adam":
        v, s = initialize_adam(parameters)
    
    # Optimization loop
    for i in range(num_epochs):
        
        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch
        seed = seed + 1
        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)

        for minibatch in minibatches:

            # Select a minibatch
            (minibatch_X, minibatch_Y) = minibatch

            # Forward propagation
            a3, caches = forward_propagation(minibatch_X, parameters)

            # Compute cost
            cost = compute_cost(a3, minibatch_Y)

            # Backward propagation
            grads = backward_propagation(minibatch_X, minibatch_Y, caches)

            # Update parameters
            if optimizer == "gd":
                parameters = update_parameters_with_gd(parameters, grads, learning_rate)
            elif optimizer == "momentum":
                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)
            elif optimizer == "adam":
                t = t + 1 # Adam counter
                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,
                                                               t, learning_rate, beta1, beta2,  epsilon)
        
        # Print the cost every 1000 epoch
        if print_cost and i % 1000 == 0:
            print ("Cost after epoch %i: %f" %(i, cost))
        if print_cost and i % 100 == 0:
            costs.append(cost)
                
    # plot the cost
    plt.plot(costs)
    plt.ylabel('cost')
    plt.xlabel('epochs (per 100)')
    plt.title("Learning rate = " + str(learning_rate))
    plt.show()

    return parameters


# 画图
def plot_decision_boundary(model, X, y):
    #import pdb;pdb.set_trace()
    # Set min and max values and give it some padding
    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1
    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1
    h = 0.01
    # Generate a grid of points with distance h between them
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    # Predict the function value for the whole grid
    Z = model(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot the contour and training examples
    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)
    plt.ylabel('x2')
    plt.xlabel('x1')
    y = y.reshape(X[0,:].shape)#must reshape,otherwise confliction with dimensions
    plt.scatter(X[0, :], X[1, :], c=y, cmap=plt.cm.Spectral)
    plt.show()
 

```

mini-batch：

```python
#--------------------------------------------- mini-batch
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "gd")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Gradient Descent optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)

```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img32.jpg)

准确率为0.79，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img33.jpg)

动量下降(with mini-batch)：

```python
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, beta = 0.9, optimizer = "momentum")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Momentum optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img34.jpg)

准确率为0.79，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img35.jpg)

Adma算法（with mini-batch）：

```python
# train 3-layer model
layers_dims = [train_X.shape[0], 5, 2, 1]
parameters = model(train_X, train_Y, layers_dims, optimizer = "adam")

# Predict
predictions = predict(train_X, train_Y, parameters)

# Plot decision boundary
plt.title("Model with Adam optimization")
axes = plt.gca()
axes.set_xlim([-1.5,2.5])
axes.set_ylim([-1,1.5])
plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)
```

cost随迭代次数的变化结果如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img36.jpg)

准确率为0.94，分类结果分布如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img37.jpg)

### 超参数调试

#### 调试处理

超参数一般有：学习率$\alpha$；动量梯度下降的$\beta$；Adma算法的$\beta_1,\beta_2,\epsilon$；神经网络的层数layers；隐藏层的数量；学习率衰减；`mini-bash size`等。

我们一般优先选择调试学习率$\alpha$，其次是隐藏层数量，`mini-batch size`和动量下降中的$\beta$。再其次调整的参数就是layers和学习率衰减了。Adma算法的三个参数一般设置为$\beta1=0.9,\beta_2 = 0.999,\epsilon=10^{-8}$，我们一般不会调整它。

在深度学习中，我们一般会通过矩阵随机取值的方式来调参，如下图：

 <img src="https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img38.jpg" style="zoom:50%;" />

长宽分别代表超参数1，2的取值。一般会在矩阵内`随机取25个点来查看效果`，因为这样会得到25个不同的超参数1和25个不同的超参数2。

但如果为3个参数，那么我们可以在一个立方体内随机选择点。

超参数调试的技巧是从`粗糙到精细的过程`：经过粗略的调整，发现在某区域内效果较好，那么我们要做的是，放大这块区域，更加密集的取值，来获取最优点。

#### 为超参数选择合适的范围

上面所说的随机取值并不是在有效值范围内的随机均匀取值，而是选择合适的步进值来探究超参数。对于隐藏层和隐藏单元的数量，随机均匀取值是合理的，但对某些参数的不合理的。

假设你认为学习率$\alpha$的取值范围是0.0001～1，取值范围内随机均匀取值会将90%的值集中在0.1到1里面，不合理。**对于这种情况，一般先按0.0001，0.001，0.01，0.1，1作为分界点，在分界点之间随机均匀取值。**

在Python中你可以这样做：

```python
import numpy as np

r = -4 * np.random.rand()
a = np.power(10, r)
# 更多的取值情况为10^a~10^b
# 通过计算得到a和b的值，然后我们将r取值变为a~b之间的随机取值。
```

还有就是动量梯度下降$\beta$的取值，它意味着指数加权平均的大小，一般通过$1-\beta$来取范围。但因为是$1-\beta$，需要将排列顺序颠倒。

```python
import numpy as np

# r在[-3, -1]内
h = -1
l = -3
r = (h - l) * np.random.rand() + l
beta = 1 - np.power(10, r)
```

#### 超参数训练方式：Panda VS Caviar

如果没有很大的算力的情况下，可以根据天数来调整超参数来观察效果（每次只能运行一个模型）。（Panda）

在算力足够的情况下，同时训练不同的几个模型。（Caviar）

#### 归一化网络

前面介绍过的输入归一化，可以使梯度下降时，更加不容易偏离方向，增大步长，加快学习速度。但这只针对了输入层，如果我们对所有隐藏层进行归一化，可以大大加快学习速率，这就是我们平常所说的`batch归一化`。实际做法一般是对$Z^{[l]}$进行归一化处理，做法如下：
$$
对于某一层的Z^{[l]}，有z^{(1)},z^{(2)},\dots,z^{(m)}\\
u=\frac1m\sum_{i=1}^mz^{(i)}\\
\sigma^2=\frac1m\sum_{i=1}^m(z^{(i)}-u)^2\\
z_{norm}^{(i)}=\frac{z^{(i)}-u}{\sqrt{\sigma^2+\epsilon}}\\
这样Z的每一个分量z值都已经标准化了，平均值为0，方差为1;\\
但我们不想让隐藏单元总是含有平均值0和方差1，也许隐藏单元有不同的分布会有意义。\\
\tilde{z}^{(i)}=\gamma z^{(i)}_{norm}+\beta\\
这里的\gamma和\beta的作用是让你可以构造其他平均值和方差的隐藏单元值\\
如果\gamma=\sqrt{\sigma^2+\epsilon},\beta=u\quad则\tilde{z}^{(i)}=z^{(i)}_{normal}
$$
*已经知道了如何对Z进行归一化，那么如何将其应用于神经网络中呢？*

BatchNormal神经网络参数计算的过程如下：
$$
X \xrightarrow{W^{[1]},b^{[1]}}Z^{[1]} \xrightarrow[Batch Normal(BN)]{\gamma^{[1]},\beta^{[1]}}\tilde{z}^{[i]}\rightarrow a^{[1]}=g^{[1]}(\tilde{z}^{[i]}) \xrightarrow{W^{[2]},b^{[2]}}Z^{[2]}\rightarrow\dots
$$
*需要注意的是这里的$\gamma$和$\beta$是和$W$，$b$一样的参数，而不是超参数。所以在反向传播时，也需要计算$d\beta,d\gamma$来更新$\beta$和$\gamma$的值。而且常数项$b$代表变化后的Z平均值离0有多远，但是标准化之后的$z^{(i)}_{norm}$均值必定为0，所以我们可以直接将$b$去掉，转而使用后面的$\beta$参数来定义。*

*Batch Normal为什么奏效？*

> 用一个例子来解释就是：你的训练集都是黑猫或者其他动物，假设他们都分布于某一侧，将训练出来的模型去预测花色猫，效果就不好。花猫分布于另一侧，这样相当于不同于黑猫的x分布。你不能期待分布在左边的数据训练出来的模型能预测右边的数据。假设将左侧的数据进行Batch Normal，相当于人为将其分布均匀，比较能适应新的数据集，防止了Covarite shift。
>
> 放在深层神经网络来看就是前层参数的变化会影响后层的参数，归一化降低了这种影响，尽量只保持特征值带来的波动。

*Batch Normal和dropout正则化*

>dropout正则化对隐藏单元进行随机删除，从而引入噪声，防止对某个神经单元过于依赖。而Batch Normal在每个mini-batch的均值方差与整体的均值方差不一致，引入加性噪声和乘性噪声，导致轻微的正则化。如果想减少这种影响，可以将mini-batch的值设置的更大，从而减少带来的噪声，减少正则化的效果！

*测试时的Batch Normal*

> 因为测试时是单个单个进行测试的，不能直接进行batch normal，因为无法知道均值和方差。所以我们使用训练集来估算均值和方差。通常估算的方法是通过指数加权平均来粗略估算均值和方差。



#### softmax回归

假设我们的分类类别是多个而不是两个，相当于我们的输出层单元个数变成了多个，数量就是你要分出的类别数C。这时，输出层会变成C个概率值，输入为$x$的情况下，输出为$p(类别|x)$，所以输出层是维度为（C，1）的矩阵。

softmax激活函数公式如下：
$$
假设Z^{[L]}=W^{[L]}a^{[L-1]}+b^{[L]},L层有C个隐藏单元，所以Z^{[L]}的维度为（C，1）\\
t=e^{(Z^{[L]})}\\
a^{[L]}=\frac{t}{\sum_{j=1}^{C}t_j},a_j^{[L]}=\frac{t_i}{\sum_{j=1}^{C}t_j}\\
a^{[L]}的维度为（C，1），可以看出softmax激活函数的特点为输入和输出都为（C，1）
$$
*使用softmax训练一个softmax分类器*

> 需要注意的是Softmax回归其实就是Logistic回归的推广，当Softmax回归的类C=2，就变成了Logistic回归。

softmax网络的损失函数为：
$$
L(\hat{y},y)=-\sum_{j=1}^Cy_j\log{\hat{y_j}}
$$
代价函数为：
$$
J=\frac1m\sum_{i=1}^{m}L(\hat{y}^{(i)},y^{(i)})
$$
**特别需要注意的是，softmax因为是一个多分类的问题，输入的$y$标签也是一个向量，$Y$会变成一个矩阵。比如有猫，狗，牛，蛇，那么蛇的样本标签$y=[0,0,0,1]^{T}$，向量化技术后，整体样本标签会成为一个（C，m）的矩阵。**

后向传播的公式：
$$
dz^{[L]}=\hat{y}-y\\
$$

#### 深度学习框架

深度学习框架在市面上有很多，我们该如何选择？

* 1.便于编程
* 2.运行速度
* 3.是否开源

#### 作业六

* tensorflow练习

导入需要的库：

```python
import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.python.framework import ops
from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict

%matplotlib inline
np.random.seed(1)
```

计算损失函数的例子：

```python
y_hat = tf.constant(36, name='y_hat')            # Define y_hat 常量. Set to 36.
y = tf.constant(39, name='y')                    # Define y. Set to 39

loss = tf.Variable((y - y_hat)**2, name='loss')  # Create a variable for the loss

init = tf.global_variables_initializer()         # When init is run later (session.run(init)),
                                                 # the loss variable will be initialized and ready to be computed
with tf.Session() as session:                    # Create a session and print the output
    session.run(init)                            # Initializes the variables
    print(session.run(loss))                     # Prints the loss
```

会话机制：

```python
a = tf.constant(2)
b = tf.constant(10)
c = tf.multiply(a,b)
print(c)

sess = tf.Session()
print(sess.run(c))
```

往函数喂数据:

```python
# Change the value of x in the feed_dict

x = tf.placeholder(tf.int64, name = 'x')
print(sess.run(2 * x, feed_dict = {x: 3}))
sess.close()
```

初始化神经网络模型参数：

```python
# GRADED FUNCTION: linear_function

def linear_function():
    """
    Implements a linear function: 
            Initializes W to be a random tensor of shape (4,3)
            Initializes X to be a random tensor of shape (3,1)
            Initializes b to be a random tensor of shape (4,1)
    Returns: 
    result -- runs the session for Y = WX + b 
    """
    
    np.random.seed(1)
    
    ### START CODE HERE ### (4 lines of code)
    X = tf.constant(np.random.randn(3, 1), name="X")
    W = tf.constant(np.random.randn(4, 3), name="W")
    b = tf.constant(np.random.randn(4, 1), name="b")
    Y = tf.add(tf.matmul(W, X), b)
    ### END CODE HERE ### 
    
    # Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate
    
    ### START CODE HERE ###
    sess = tf.Session()
    result = sess.run(Y)
    ### END CODE HERE ### 
    
    # close the session 
    sess.close()

    return result
```

激活函数：

```python
# GRADED FUNCTION: sigmoid

def sigmoid(z):
    """
    Computes the sigmoid of z
    
    Arguments:
    z -- input value, scalar or vector
    
    Returns: 
    results -- the sigmoid of z
    """
    
    ### START CODE HERE ### ( approx. 4 lines of code)
    # Create a placeholder for x. Name it 'x'.
    x = tf.placeholder(tf.float32, name="x")

    # compute sigmoid(x)
    sigmoid = tf.sigmoid(x)

    # Create a session, and run it. Please use the method 2 explained above. 
    # You should use a feed_dict to pass z's value to x. 
    with tf.Session() as sess:
        # Run session and call the output "result"
        result = sess.run(sigmoid, feed_dict={x:z})
    
    ### END CODE HERE ###
    
    return result
```

计算代价函数：

```python
# GRADED FUNCTION: cost

def cost(logits, labels):
    """
    Computes the cost using the sigmoid cross entropy
    
    Arguments:
    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)
    labels -- vector of labels y (1 or 0) 
    
    Note: What we've been calling "z" and "y" in this class are respectively called "logits" and "labels" 
    in the TensorFlow documentation. So logits will feed into z, and labels into y. 
    
    Returns:
    cost -- runs the session of the cost (formula (2))
    """
    
    ### START CODE HERE ### 
    
    # Create the placeholders for "logits" (z) and "labels" (y) (approx. 2 lines)
    z = tf.placeholder(tf.float32, name="z")
    y = tf.placeholder(tf.float32, name="y")
    
    # Use the loss function (approx. 1 line)
    cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = z, labels = y)
    
    # Create a session (approx. 1 line). See method 1 above.
    sess = tf.Session()
    
    # Run the session (approx. 1 line).
    cost = sess.run(cost, feed_dict={z:logits, y:labels})
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    
    return cost
```

独热编码：

```python
# GRADED FUNCTION: one_hot_matrix

def one_hot_matrix(labels, C):
    """
    Creates a matrix where the i-th row corresponds to the ith class number and the jth column
                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) 
                     will be 1. 
                     
    Arguments:
    labels -- vector containing the labels 
    C -- number of classes, the depth of the one hot dimension
    
    Returns: 
    one_hot -- one hot matrix
    """
    
    ### START CODE HERE ###
    
    # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)
    C = tf.constant(value = C, name="C")
    
    # Use tf.one_hot, be careful with the axis (approx. 1 line)
    one_hot_matrix = tf.one_hot(labels, C, axis=0)
    
    # Create the session (approx. 1 line)
    sess = tf.Session()
    
    # Run the session (approx. 1 line)
    one_hot = sess.run(one_hot_matrix)
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    
    return one_hot
```

初始化零一向量：

```python
# GRADED FUNCTION: ones

def ones(shape):
    """
    Creates an array of ones of dimension shape
    
    Arguments:
    shape -- shape of the array you want to create
        
    Returns: 
    ones -- array containing only ones
    """
    
    ### START CODE HERE ###
    
    # Create "ones" tensor using tf.ones(...). (approx. 1 line)
    ones = tf.ones(shape)
    
    # Create the session (approx. 1 line)
    sess = tf.Session()
    
    # Run the session to compute 'ones' (approx. 1 line)
    ones = sess.run(ones)
    
    # Close the session (approx. 1 line). See method 1 above.
    sess.close()
    
    ### END CODE HERE ###
    return ones
```



