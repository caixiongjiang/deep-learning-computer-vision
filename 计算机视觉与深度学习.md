# 计算机视觉与深度学习

## 深度学习

### 深度学习介绍

#### 线性整流函数（ReLU函数）

通常意义下，线性整流函数指代数学中的斜坡函数，即
$$
f(x)=max(0,x)
$$
而在神经网络中，线性整流作为神经元的激活函数，定义了该神经元在线性变换$W^Tx+b$之后的非线性输出结果。换言之，对于进入神经元的来自上一层神经网络的输入向量$x$，使用线性整流激活函数的神经元会输出
$$
max(0,W^Tx+b)
$$
到下一层神经元或作为整个神经网络的输出。

#### 神经网络介绍

神经网络的基本模型是神经元，由输入层，隐藏层，输出层组成。最基本的神经网络是计算映射的，输入层为$x$，在实际上一般表现为特征，输出层为y，一般为结果，隐藏层其实就是上面所说的权向量$W^t$。

#### 监督学习

监督学习也称为带标签的学习方式。监督学习是`从标记的训练数据`来推断一个功能的机器学习任务。训练数据包括一套训练示例。在监督学习中，每个实例都是由一个输入对象（通常为矢量）和一个期望的输出值（也称为监督信号）组成。

#### 结构化数据vs非结构化数据

结构化数据指传统数据库中的数据，非结构化数据库是指音频，图片，文本等数据。

#### 深度学习的准确率

取决于你的神经网络复杂度以及训练集的大小，一般来说神经网络越复杂时，需要的训练数据也越多，这样训练出来的模型效果也更好。

#### Sigmoid函数

sigmoid函数也叫`Logistic`函数，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：

优点：平滑、易于求导。

缺点：激活函数计算量大，反向传播求误差梯度时，求导涉及除法；反向传播时，很容易就会出现`梯度消失`的情况，从而无法完成深层网络的训练。

Sigmoid函数的公式如下：
$$
S(x)=\frac{1}{1+e^{-x}}
$$
函数图形如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img3.jpg)



### 深度学习基础

> 为了方便学习：
>
> 1.使用$(x,y)$来表示一个单独的样本
>
> 2.$x\in \R^{n_x}$代表$x$是$n_x$维的特征向量，$y\in \{0,1\}$代表标签$y$值为0或1
>
> 3.训练集由m个训练样本构成，$(x^{(1)},y^{(1)})$代表样本一，$(x^{(m)},y^{(m)})$代表最后一个样本m
>
> 4.$m=m_{train}+m_{test}$
>
> 5.构建神经网络时使用矩阵$X=\left[ \begin{matrix}|&|&&|\\ x^{\left( 1\right)  }&x^{\left( 2\right)  }&\cdots &x^{\left( m\right)  }\\ |&|&&|\end{matrix} \right]  $，$m$是训练集样本的个数。
>
> 6.输出标签时，为了方便，也将y标签放入列中，$Y=\left[ \begin{matrix} y^{\left( 1\right)  }&y^{\left( 2\right)  }&\cdots &y^{\left( m\right)  }\end{matrix} \right]  $,$Y\in\R^{1\times m}$

#### Logistic回归

Logistic回归通常用于二元分类问题。

它通常的做法是将`sigmoid函数`作用于线性回归：
$$
\hat{y} =\sigma\left( W^{T}x+b\right)\quad \quad \text{其中} \sigma(z)=\frac{1}{1+e^{-z}}
$$
这会使得$\hat{y}$的范围在0~1之间

梯度下降法中的`损失函数`如下：
$$
L(\hat{y},y)=\frac12(\hat{y}-y)^2
$$
Logistic回归中使用的`损失函数`如下：
$$
L(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))
$$
当$y=1$时，$L(\hat{y},y)=-y\log\hat{y}$，为了使损失函数较小，$\hat{y}$必须比较大，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于1；当$y=0$，$L(\hat{y},y)=-\log(1-\hat{y})$，$\hat{y}$必须比较小，而$\hat{y}$的取值范围在0~1之间，所以$\hat{y}$要接近于0。

>损失函数是在单个训练样本中定义的，在全体训练样本上的表现是由代价函数来定义的。



代价函数的定义：
$$
\begin{split}
J(W,b)&=\frac{1}{m}\sum^{m}_{i\  =\  1} L\left( \hat{y}^{\left( i\right)  } ,y^{\left( i\right)  }\right) \\&=-\frac{1}{m}\sum^m_{i=1}[y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)})]\quad \quad \quad
\text{其中}\hat{y}^{(i)}\text{代表的是预测值}，y^{(i)}代表的是真实值
\end{split}
$$

#### 梯度下降法

我们可以将梯度下降法用下图来表示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img1.jpg)

梯度下降法所做的事就是从初始点开始让$J(W,b)$朝`最陡的下坡方向`走一步,迭代次数不定。

其中$W$的迭代更新公式如下：
$$
W:=W-\alpha\frac{\partial J(W,b)}{\partial W} \quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial W} 代表该点的W对应的导数
\\b:=b-\alpha\frac{\partial J(W,b)}{\partial b}\quad \quad 其中\alpha 代表学习率，\frac{\partial J(W,b)}{\partial b} 代表该点的b对应的导数
$$
这样会使$W$和$b$一步一步得接近使得$J(W,b)$最小的值。

> 那么m个样本的梯度下降如何来表示呢？

其实就是对$J(W,b)$函数分别对$W$和$b$求偏导,得到全局的梯度值。

> $W$和$b$的迭代过程：
>
> 1.对$W$和$b$设定初值，计算$J(W,b)$
>
> 2.通过$J(W,b)$对参数求偏导
>
> 3.使用$W$和$b$的原值减去学习率乘以偏导来迭代更新值
>
> 4.重复1~3步骤

#### 向量化技术

如果不使用向量化技术，在面对巨大的数据集时，你会用非常多的循环去解决迭代的问题，这往往会降低代码运行的速度。向量化技术使得这种计算过程变得更加快速。

比如$f=W^T$,如果$W$有n个维度，不使用向量化一般需要用长度为n的for循环遍历求解，向量化之后则用矩阵来求解，看一段`Python`代码：

```python
import numpy as np
import time

a = np.random.rand(1000000)
b = np.random.rand(1000000)

# 非向量化使用循环
c = 0
tic = time.time()
for i in range(1000000):
  c += a[i]*b[i]
toc = time.time()
print("使用循环做法花费的时间为" + str(1000*(toc - tic)) + "ms")

# 使用向量化技术
tic = time.time()
c = np.dot(a, b)
toc = time.time()
print("使用向量化技术花费的时间为" + str(1000*(toc - tic)) + "ms")

# 运行结果如下：（保留一位小数）
# 使用循环做法花费时间为474.3ms
# 使用向量化技术花费的时间为1.5ms
```

> 我们在编写神经网络的时候，尽量要避免使用for循环

再举个例子：
$$
v=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  ==>u=\left[ \begin{matrix}v_{1}\\ \vdots \\ v_{n}\end{matrix} \right]  
$$
可以这样编程（尽量使用numpy）:

```python
import numpy as np

# np.random.randint(a, b, size=(c, d)):
# 注：a-b表示生成[a,b]数的范围，后面size表示生成矩阵的大小
n = 10000
v = np.random.randint(10,11,(1,n))

# 原始方法（for循环）
u = np.zero((n,1))
for i in range(n):
  u[i]=math.exp(v[i])
  
# 使用numpy的内置函数，能比原来快很多
u = np.exp(v)
# 同样还有np.log() np.abs() np.maximum(v,0) v**2 1/v
```

使用numpy简易表示Logistic回归的一轮迭代：

```python
# Z = w^T*X+b
Z = np.dot(w.T,X)+b
# A = sigmoid(Z) 
def sigmoid_func(Z):
	return 1/(1+np.exp(-z))
A = sigmoid_func(Z)
dZ = A - Y
dw = 1/m * dZ
db = 1/m * np.sum(dZ)
w = w - a * dw # a代表学习率
b = b - a * db 
```

#### Python中的广播

|            | 苹果 | 牛肉  | 鸡蛋 | 土豆 |
| ---------- | ---- | ----- | ---- | ---- |
| 碳水化合物 | 56.0 | 0.0   | 4.4  | 68.0 |
| 蛋白质     | 1.2  | 104.0 | 52.0 | 8.0  |
| 脂肪       | 1.8  | 135.0 | 99.0 | 0.9  |

求每种食物的每项指标占比：

```python
import numpy as np

A = np.array([[56.0, 0.0, 4.4, 68.0],
              [1.2, 104.0, 52.0, 8.0],
              [1.8, 135.0, 99.0 0.9]])

# 对矩阵进行竖直方向求和
cal = A.sum(axis=0)
# reshape是O(1)操作，放心使用
# 这里的广播是将3*4的矩阵除以1*4的矩阵，然后进行自动广播
percentage = 100*A/cal.reshape(1,4)
```

再举一个特殊的例子：

```python
import numpy as np

A = np.array([[1],
       				[2],
              [3],
              [4]])
# 广播
A = A + 100
print(A)
# 结果：
# [[101]
#  [102]
#  [103]
#  [104]]
```

如上所示，广播的规则如下：

一个$m\times n$的矩阵`加减乘除`一个$1\times n$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times n$的矩阵`加减乘除`一个$m\times 1$的矩阵，python就会自动把它复制成$m\times n$的矩阵

一个$m\times 1$的矩阵`加减乘除`一个常数，python就会自动把它复制成$m\times 1$的矩阵

一个$1\times m$的矩阵`加减乘除`一个常数，python就会自动把它复制成$1\times m$的矩阵

#### numpy的使用

```python
import numpy as np

# 并不是一个向量，而是一个秩为1的数组
a = np.random.randn(5)
print(a)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(a.shape)
# (5,)
print(a.T)
# [-1.20936449  0.67825543  1.92816046 -0.55383946 -0.53203701]
print(np.dot(a,a.T))
# 6.23019719213342

b = np.random.randn(5,1)
print(b)
# [[ 1.83847239]
#  [ 0.43958321]
#  [-0.87437944]
#  [ 0.70296355]
#  [-0.1833722 ]]
print(b.T)
# [[ 1.83847239  0.43958321 -0.87437944  0.70296355 -0.1833722 ]]
print(np.dot(b,b.T))
# [[ 3.37998075  0.8081616  -1.60752246  1.29237908 -0.33712473]
#  [ 0.8081616   0.1932334  -0.38436252  0.30901097 -0.08060734]
#  [-1.60752246 -0.38436252  0.7645394  -0.61465687  0.16033688]
#  [ 1.29237908  0.30901097 -0.61465687  0.49415775 -0.12890397]
#  [-0.33712473 -0.08060734  0.16033688 -0.12890397  0.03362536]]
```

从上面的例子可以看出，我们构建向量时尽量构建b这种类型的向量，不要使用数组，可以避免不必要的错误。



为了我们程序的运行正确，少点bug，可以使用assert声明函数。Python assert（断言）用于判断一个表达式，在表达式条件为 false 的时候触发异常。断言可以在条件不满足程序运行的情况下直接返回错误，而不必等待程序运行后出现崩溃的情况。语法为`assert (表达式)`。

其中`np.squeeze()`可以将数组变成一个向量。

#### 作业一

使用numpy手写Logistic回归，这里只写回归部分，数据处理部分略过：

两个偏导数公式如下：

$$ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{7}$$

$$ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{8}$$

```python
import numpy as np
# ----------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# ----------------------------------
def initialize_with_zeros(dim):
    """
    Argument:
    dim -- 输入数据的维度
    
    Returns:
    w -- 初始化维度为(dim, 1)的向量
    b -- 初始化标量
    """
    w = np.zeros((dim,1))
    b = 0
  
    assert(w.shape == (dim, 1))
    assert(isinstance(b, float) or isinstance(b, int))
  
    return w, b
# ----------------------------------
def propagate(w, b, X, Y):
    """
    代价函数
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
  
    Returns:
    cost -- 代价
    dw -- 损失相对于 w 的梯度，因此维度与 w 相同
    db -- 损失相对于 b 的梯度，因此维度与 b 相同
    """
  
    m = X.shape[1] # 样本数量
    A = sigmoid(np.dot(w.T, X) + b) # 预测值
    cost = -(1.0/m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))
  
    dw = (1.0/m) * np.dot(X, (A - Y).T)
    db = (1.0/m) * np.sum(A - Y)
  
    assert(dw.shape == w.shape)
    assert(db.shape == b.shape)
    cost = np.squeeze(cost) # 将数组转化为向量（这里为防止bug）
    assert(cost.shape == ())
  
    grads = {"dw": dw,
             "db": db}
    
    return grads, cost
# ----------------------------------
def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):
    """
    w和b的迭代优化
  
    Argument:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 1)维度的数据
    Y -- 维度为(1, 样本数量)标签
    num_iterations -- 优化迭代的次数
    learning_rate -- 学习率
    print_cost -- 如果为true，每迭代100次打印一次损失
  
    Returns:
    params -- 包含权重 w 和偏差 b 的字典
    grads -- 包含权重梯度和相对于成本函数的偏差梯度的字典
    costs -- 优化期间计算的所有成本的列表，这将用于绘制学习曲线。
    """
  
    costs = []
    for i in range(num_iterations):
        grads, cost = propagate(w, b, X, Y)
    
        dw = grads["dw"]
        db = grads["db"]

        w = w - learning_rate * dw
        b = b - learning_rate * db

        if i % 100:
            costs.append(cost)

        if print_cost and i % 100 == 0:
            print("Cost after iteration %i:%f" %(i, cost))

    params = {"w": w,
              "b": b}

    grads = {"dw": dw,
             "db": db}

    return params, grads
# ----------------------------------
def predict(w, b, X):
    """
    使用学习的逻辑回归参数 (w, b) 预测标签是 0 还是 1

    Arguments:
    w -- 权重
    b -- 偏移量
    X -- (num_px*num_px*3, 样本数量)维度的数据
    
    Returns:
    Y_prediction -- 一个numpy数组（向量），包含X中示例的所有预测（0/1）
    """
    m = X.shape[1]
    Y_prediction = np.zeros((1,m))
    w = w.reshape(X.shape[0], 1)

    A = sigmoid(np.dot(w.T, X) + b)

    for i in range(A.shape[1]):
        if A[0, i] > 0.5:
            Y_prediction[0, i] = 1
        else:
            Y_prediction[0, i] = 0
    
    assert(Y_prediction.shape == (1, m))

    return Y_prediction
# ----------------------------------
def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):
    """
    构建逻辑回归模型

    Arguments:
    X_train -- 训练样本 shape:(num_px * num_px * 3, m_train)
    Y_train -- 训练标签 shape:(1, m_train)
    X_test -- 测试样本 shape:(num_px * num_px * 3, m_test)
    Y_test -- 测试标签 shape:(1, m_test)
    num_iterations -- 迭代次数 默认为2000
    learning_rate -- 学习率 默认为0.5
    print_cost -- 是否打印代价
    
    Returns:
    d -- 包含模型信息的字典
    """

    w, b = initialize_with_zeros(X_train.shape[0])
    # 训练
    parameter, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)
    # 训练结果
    w = parameter["w"]
    b = parameter["b"]
    # 预测
    Y_prediction_test = predict(w, b, X_test)
    Y_prediction_train = predict(w, b, X_train)
    # 打印预测结果
    print("训练集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))
    print("测试集 预测准确率：{} %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))

    d = {"cost": costs,
         "测试集预测正确个数": Y_prediction_test,
         "训练集预测正确个数": Y_prediction_train,
         "w": w,
         "b": b,
         "学习率": learning_rate,
         "迭代轮数": num_iterations}

    return d
  
## 最后就可以使用model函数对已经经过数据处理的训练集和测试集进行训练和预测了

```

### 神经网络编写

#### 神经网络表示

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img2.jpg)

如上图所示，这是一个`双层神经网络`（一般输入层不作为层数）。最左边为输入层，代表单个样本的输入特征数；中间为隐藏层；最右边为输出层，一般代表预测值。中间的隐藏层是代表特征与预测值关系的一些表达式，类似于机器学习中的$W$和$b$。在这个图中，$W$是一个$4\times 3$的矩阵，$b$是一个$4\times 1$的矩阵，4代表隐藏层的个数，3代表输入的特征。

*需要注意的是这里的W和Logistic中讲的W是不一样的：因为这里的W是指整个隐藏层的W，计算时不用转置（4$\times$

3）；而Logostic中的W相当于只有一个节点的W，且计算时需要转置(3$\times$1)。*

#### 神经网络的计算

将每个隐藏层分开单独和左边的输入层结合在一起看，神经网络其实就是多个类似于Logistic回归的结构。

所以上图隐藏层的计算过程如下：
$$
z^{[1]}_1 = {w^{[1]}_1}^{T}x+b^{[1]}_1,a^{[1]}_1=\sigma(z^{[1]}_1)\\
z^{[1]}_2 = {w^{[1]}_2}^{T}x+b^{[1]}_2,a^{[1]}_2=\sigma(z^{[1]}_2)\\
z^{[1]}_3 = {w^{[1]}_3}^{T}x+b^{[1]}_3,a^{[1]}_3=\sigma(z^{[1]}_3)\\
z^{[1]}_4 = {w^{[1]}_4}^{T}x+b^{[1]}_4,a^{[1]}_4=\sigma(z^{[1]}_4)\\
其中[]里代表的数字是第几层,这里是从隐藏层算起\\下标的值代表的是该层的第几个节点\\
\sigma(z)代表激活函数
$$
所以将上面的双层神经网络整个计算过程合并起来就变成了：
$$
第一层：隐藏层\\
z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[1]}=\sigma(z^{[1]})\\
第二层：输出层\\
z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}\\
a^{[2]}=\sigma(z^{[2]})
$$

#### 多个样本的向量化

上面讲的计算过程是单个样本的计算过程，如果是多个样本，就要使用一个循环来计算。但是前面讲过样本的遍历可以使用向量化技术来加快运算速度。

所以我们把z和a关于样本的多个列合并在一起：
$$
Z^{[1]}=[z^{[1](1)},z^{[1](2)},\dots z^{[1](m)}]\\
A^{[1]}=[a^{[1](1)},a^{[1](2)},\dots a^{[1](m)}]\\
Z^{[2]}=[z^{[2](1)},z^{[2](2)},\dots z^{[2](m)}]\\
A^{[2]}=[a^{[2](1)},a^{[2](2)},\dots a^{[2](m)}]\\
m代表样本的数量,[]的值代表不同的层，行代表不同的节点(也叫隐藏单元)，列代表不同的样本
$$
所以计算过程变为了：
$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})\\
这里的b不需要变是因为python自带的广播技术
$$

#### 多种激活函数

上面我们使用的激活函数为$\sigma(z)$也就是`sigmoid函数`。现在我们要介绍多种激活函数来进行对比：

* $tanh(z)$:

$$
a=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\quad \quad 它的取值范围在[-1,1]\\
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img4.jpg)

该函数的特点是所有的数据平均值接近0，如果需要进行该种数据中心化可以使用该函数。

通常来说激活函数选取$tanh(z)$都比使用`sigmoid`函数更好。但有一个例外是输出层，输出层经常使用`sigmoid函数`，或者使用二元分类时，使用`sigmoid函数`。为了表示不同的层之间使用不同的激活函数，我们通常会将激活函数用$g$来表示，使用$g^{[i]}$表示第i层的激活函数。

`sigmoid函数`和$tanh(z)$函数共同的缺点是当$z$的值很大或者很小的时候，函数的斜率很接近0，也就是我们经常会说的梯度消失，拖慢梯度下降算法。

* ReLU：

$$
a=max(0,z)
$$

图像如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img5.jpg)

* Leaky ReLU：
  $$
  a=max(cz,z)\quad \quad  c在这里是一个常数，通常取一个比较小的数，比如0.01或者0.001
  $$
  图像如下：

  ![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img6.jpg)

  ReLU的缺点是，当$z$的值为负数的时候，它没有导数值。而Leaky ReLU解决了这个问题。

> 激活函数如何选择？

激活函数的选择经验：

1.如果你在做二元分类时，输出一般为0或1，那么该网络的输出层激活函数选择sigmoid函数较好，其他所有单元都使用ReLU函数。使用ReLU函数最大的好处就是梯度下降比较快，也就是收敛的比较快。

2.有时候特定情况下会使用tanh(z)函数。

3.ReLU函数是最常用的激活函数

> 为什么神经网络需要使用激活函数？

我们来做一个公式推导：
$$
如果不使用激活函数:\\
a^{[1]}=z^{[1]}=W^{[1]}x+b^{[1]}\\
a^{[2]}=z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}=(W^{[2]}W^{[1]})x+(W^{[2]}b^{[1]}+b^{[2]})\\
=W^{'}x+b^{'}
$$
可以看到如果不使用激活函数，无论你使用多庞大的神经网络，都始终在做线性激活函数，这就退化成了线形回归的内容。

#### 激活函数的导数

当你使用神经网络进行反向传播时，需要计算激活函数的斜率或者导数。

* sigmoid函数

$$
a=g(z)=\frac{1}{1+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=g(z)(1-g(z))=a(1-a)
$$

* tanh函数

$$
a=g(z)=tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}\\
g^{'}(z)=\frac{dg(z)}{dz}=1-(\frac{e^z-e^{-z}}{e^z+e^{-z}})^2=1-g(z)^2=1-a^2
$$

* ReLU

$$
a=g(z)=max(0,z)\\
g^{'}(z)=\begin{cases}
0,& \text{如果}z<0\\
1,& \text{如果}z>0\\
undefined,&\text{如果}z=0
\end{cases}
$$

* Leaky ReLU

$$
g(z)=max(0.01z,z)\\
g^{'}(z)=\begin{cases}
0.01& \text{如果}z<0\\
1& \text{如果}z>0
\end{cases}
$$

#### 神经网络的梯度下降法

以单隐藏层为例，写出它们的正向传播和反向传播的过程：

* 正向传播：

$$
Z^{[1]}=W^{[1]}X+b^{[1]}\\
A^{[1]}=\sigma(Z^{[1]})\\
Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}\\
A^{[2]}=\sigma(Z^{[2]})
$$

* 反向传播：

$$
dZ^{[2]}=A^{[2]}-Y\\
dW^{[2]}=\frac{1}{m}dZ^{[2]}{A^{[1]}}^T\\
db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)\\
dZ^{[1]}={W^{[2]}}^TdZ^{[2]}*{g^{[1]}}^{'}(Z^{[1]})\\
这里的*代表逐个元素乘积\\
dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T\\
db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)
$$

> np.sum的axis不同时：
>
> 1.np.sum(axis = 0)代表矩阵最外维度相加（如果最外维度为n，可以理解为n个二维矩阵直接相加）
>
> 2.np.sum(axis = 1)代表矩阵中间维度相加（相当于是二维矩阵内部对每列求和）
>
> 3.np.sum(axis = 2)代表矩阵最内维度相加（相当于是二维矩阵内部对每行求和）
>
> keepdims=True是为了保证不输出秩为1的数组

#### 随机初始化

在Logistic回归中，我们把$w$和$b$都初始化为0向量，在神经网络中$W$不能这么初始化为0矩阵。因为这样会导致第一层在做计算时，每个隐藏单元所做的计算都是一模一样的，在反向传播时，不同隐藏单元激活函数的导数$dz^{[1]}_1$和$dz^{[1]}_2$是一样的。

我们的做法一般是对$W$随机初始化:
$$
W^{[1]}=np.random.randn((x,y))\times 0.01\\
b^{[2]}=np.zeros((y,1))\\
0.01代表权重，一般取比较小的值,这样能使梯度下降更快一些\\这在使用sigmoid作为激活函数的网络更为明显（z值太大,导数接近0）；\\x代表输入层的特征数；\\y代表隐藏层的隐藏单元数目。
$$

#### 作业二

写一个双层神经网络(没有数据处理的部分)：

```python
import numpy as np
import matplotlib.pyplot as plt

# -----------------------------------------
def sigmoid(z):
    """
	sigmoid激活函数
	"""
    s = 1.0 / (1.0 + np.exp(-1.0 * z))
    
    return s
# -----------------------------------------
def layer_sizes(X, Y):
    """
    Arguments:
    X -- 输入数据 (输入层大小, 样本数量)
    Y -- 标签 (输出层大小, 样本数量)
    
    Returns:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小
    """

    n_x = X.shape[0] # 输入层的大小
    n_h = 4
    n_y = Y.shape[0] # 输出层的大小

    return (n_x, n_h, n_y)


# -----------------------------------------
def initialize_parameters(n_x, n_h, n_y):
    """
    Arguments:
    n_x -- 输入层的大小
    n_h -- 隐藏层的大小
    n_y -- 输出层的大小

    Returns:
    params -- 初始化参数的字典:
              W1 -- weight matrix of shape (n_h, n_x)
              b1 -- bias vector of shape (n_h, 1)
              W2 -- weight matrix of shape (n_y, n_h)
              b2 -- bias vector of shape (n_y, 1)
    """

    np.random.seed(2) # 设置随机种子

    W1 = np.random.randn((n_h, n_x))
    b1 = np.zeros((n_h, 1))
    W2 = np.random.rand((n_y, n_h))
    b2 = np.zeros((n_y, 1))

    assert(W1.shape == (n_h, n_x))
    assert(b1.shape == (n_h, 1))
    assert(W2.shape == (n_y, n_h))
    assert(b2.shape == (n_y, 1))

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def forward_propagation(X, parameters):
    """
    前向传播计算

    Argument:
    X -- 输入数据 (n_x, m)
    parameters -- 初始化参数的字典 (output of initialization function)
    
    Returns:
    A2 -- 第二层sigmoid激活函数输出的结果
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    """    

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1) # 隐藏层使用tanh激活函数
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2) # 输出层使用sigmoid激活函数

    assert(A2.shape == (1, X.shape[1])) # X.shape[1]代表样本数量

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache

# -----------------------------------------
def compute_cost(A2, Y, parameters):
    """
    计算代价函数 (13)
    
    Arguments:
    A2 -- 第二层sigmoid激活函数输出的结果 维度(1, number of examples)
    Y -- 正确的标签 维度(1, number of examples)
    parameters -- 初始化参数的字典 W1, b1, W2 and b2
    
    Returns:
    cost -- 代价函数结果
    """

    m = Y.shape[1] # 样本数量

    # 计算代价函数
    # np.multiply(X, Y)是指X和Y对应位置两两相乘
    logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1 - A2), 1 - Y)
    cost = -np.sum(logprobs) / m

    cost = np.squeeze(cost) # 确保代价为我们期望的维度
    
    # isinstance() 函数来判断一个对象是否是一个已知的类型
    assert(isinstance(cost, float))

    return cost

# -----------------------------------------
def backward_propagation(parameters, cache, X, Y):
    """
    后向传播
    
    Arguments:
    parameters -- 参数初始化字典 
    cache -- 中间权向量的字典 "Z1", "A1", "Z2" and "A2"
    X -- 输入数据 维度(2, number of examples)
    Y -- 正确标签 维度(1, number of examples)
    
    Returns:
    grads -- 参数渐变的字典
    """

    m = X.shape[1]

    W1 = parameters["W1"]
    W2 = parameters["W2"]

    A1 = cache["A1"]
    A2 = cache["A2"]

    # 后向传播计算
    # tanh()函数的导数为 g'(a) = 1 - a^2
    
    dZ2 = A2 - Y
    dW2 = np.dot(dZ2, A1.T) / m
    db2 = np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2)) # 1-np.power(A1, 2)为tanh的导数
    dW1 = np.dot(dZ1, X.T) / m
    db1 = np.sum(dZ1, axis=1, keepdims=True)/ m

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads

# -----------------------------------------
def update_parameters(parameters, grads, learning_rate = 1.2):
    """
    中间权重向量更新
    
    Arguments:
    parameters -- 更新前的参数 
    grads -- 用于参数更新的逆向传播参数
    
    Returns:
    parameters -- 更新后的参数
    """

    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]
    
    dW1 = grads["dW1"]
    db1 = grads["db1"]
    dW2 = grads["dW2"]
    db2 = grads["db2"]

    # 权重向量更新
    W1 = W1 - dW1 * learning_rate
    b1 = b1 - db1 * learning_rate
    W2 = W2 - dW2 * learning_rate
    b2 = b2 - db2 * learning_rate

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters

# -----------------------------------------
def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):
    """
    Arguments:
    X -- dataset of shape (2, number of examples)
    Y -- labels of shape (1, number of examples)
    n_h -- size of the hidden layer
    num_iterations -- 循环迭代的次数
    print_cost -- 如果为True,每1000次打印一次代价
    
    Returns:
    parameters -- 训练好的参数，用于预测
    """

    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]

    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters["W1"]
    b1 = parameters["b1"]
    W2 = parameters["W2"]
    b2 = parameters["b2"]

    # 循环
    for i in range(0, num_iterations):
        # 计算前向传播
        A2, cache = forward_propagation(X, parameters)
        
        # 计算代价
        cost = compute_cost(X, parameters)

        # 计算后向传播
        grads = backward_propagation(parameters, cache, X, Y)

        # 更新权向量
        parameters = update_parameters(parameters, grads) # 学习率直接设置为默认值1.2

        if print_cost and (i % 1000):
            print("第i次迭代之后的代价为:" + str(cost))

    return parameters


# ----------------------------------------- 
def predict(parameters, X):
    """
    通过训练好的权重来预测X的类型
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    X -- 输入数据 维度 (n_x, m)
    
    Returns
    predictions -- 模型预测的结果 (red: 0 / blue: 1)
    """

    A2, cache = forward_propagation(X, parameters)
    prediction = (A2 > 0.5) # sigmoid函数的判别方式

    return prediction

# -----------------------------------------
# 使用：

# 构建双层神经网络模型
parameters = nn_model(X, Y, n_h=4, num_iterations=10000, print_cost=True)

# 绘制决策边界
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y[0, :])
plt.title("Decision Boundary for hidden layer size " + str(4))
```

### 多层的深层神经网络

#### 神经网络的表示

1.L代表神经网络的层数（layers），不包括输入层，比如一个4层网络称为L-4

2.$n^{[l]}$代表$l$层上节点的数量，也可以说是隐藏单元的数量

3.$a^{[l]}$代表$l$层中的激活函数，$a^{[l]}=g^{[l]}(z^{[l]})$

#### 深层网络中的前向传播

神经网络中每层的前向传播过程：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})\\
l代表层数
$$
**如果需要计算前向传播的层数过多，可以使用for循环将它们串起来。**

#### 核对矩阵中的维数

如果我们在实现一个非常复杂的矩阵时，需要特别注意矩阵的维度问题。

通过一个具体的网络来手动计算一下维度：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img7.jpg)

可以写出该网络的部分参数如下：
$$
n^{[0]}=n_x=2\quad n^{[1]}=3\quad n^{[2]}=5\quad n^{[3]}=4\quad n^{[4]}=2\quad n^{[5]}=1
$$
由于前向传播的公式为：
$$
Z^{[l]}=W^{[l]}a^{[l-1]}+b^{[l]}\\
a^{[l]}=g^{[l]}(z^{[l]})
$$

> 需要说明的是这里的维度都是只在一个样本的情况下。如果在m个样本的情况下，1都要变成m，但b的维度可以不变，因为通过python中的广播技术，b会自动扩充。

1.$b^{[1]}$的维度为$3\times 1$，所以$Z^{[1]}$的维度也是一样的，为$n^{[1]}\times 1$也就是$3\times 1$。

2.$X$的维度为$n^{[0]}\times 1$，也就是$2\times 1$

所以通过1,2两条可以推出$W^{[1]}$的维度为$n^{[1]}\times n^{[0]}$，也就是$3\times 2$。

*可以总结出来的是：*
$$
W^{[l]}的维度一定是n^{[l]}\times n^{[l-1]}\\
b^{[l]}的维度一定是n^{[l]}\times 1
$$
*同理在反向传播时：*
$$
dW和W的维度必须保持一致，db必须和b保持一致
$$
因为$Z^{[l]}=g^{[l]}(a^{[l]})$，所以$z$和$a$的维度应该相等。

#### 参数vs超参数

参数（Parameters）：$W^{[1]},b^{[1]},W^{[2]},b^{[2]},\dots$

超参数：学习率$a$；迭代次数$i$ ；隐层数$L$；隐藏单元数$n^{[l]}$；激活函数的选择。

#### 作业三

一个神经网络工作原理的模型如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img8.jpg)

多层网络模型的前向传播和后向传播过程如下：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img9.jpg)

* 实现一个L层神经网络

```python
from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward
import numpy as np


# ------------------------------------------
def initialize_parameters_deep(layer_dims):
    """
    Arguments:
    layer_dims -- 包含网络中每一层的维度的Python List
    
    Returns:
    parameters -- Python参数字典 "W1", "b1", ..., "WL", "bL":
                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])
                    bl -- bias vector of shape (layer_dims[l], 1)
    """
    
    np.random.seed(3)
    parameters = {}
    L = len(layer_dims) # 网络层数

    for i in range(1, L):
        parameters["W" + str(i)] = np.random.randn(layer_dims[i], layer_dims[i - 1]) * 0.01
        parameters["b" + str(i)] = np.zeros(layer_dims[i], 1)

        assert(parameters["W" + str(i)].shape == (layer_dims[i], layer_dims[i - 1]))
        assert(parameters["b" + str(i)].shape == (layer_dims[i], 1))

    return parameters

# ------------------------------------------
def linear_forward(A, W, b):
    """
    实现前向传播的线性部分

    Arguments:
    A -- 来自上一层的激活结果 (或者为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)

    Returns:
    Z -- 激活函数的输入或称为预激活参数 
    cache -- Python参数字典包含"A", "W" and "b" ; stored for computing the backward pass efficiently
    """

    Z = np.dot(W, A) + b

    assert(Z.shape == (W.shape[0], A.shape[1]))

    cache = (A, W, b)

    return Z, cache

# ------------------------------------------
def linear_activation_forward(A_prev, W, b, activation):
    """
    实现 线性——>激活层 的前向传播

    Arguments:
    A_prev -- 来自上层的激活结果 (或为初始输入数据): (前一层的隐藏单元数, 样本数)
    W -- 权重矩阵: numpy array of shape (当前层的隐藏单元数, 前一层的隐藏单元数)
    b -- 偏置向量, numpy array of shape (当前层的隐藏单元数, 1)
    activation -- 当前隐藏层使用的激活函数: "sigmoid" or "relu"

    Returns:
    A -- 激活函数的输出,也称为激活后值 
    cache -- Python字典包含 "线性缓存" and "激活缓存";
             stored for computing the backward pass efficiently
    """

    if activation == "sigmoid":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = sigmoid(Z)

    elif activation == "relu":
        Z, linear_cache = linear_forward(A_prev, W, b)
        A, activation_cache = relu(Z)

    assert(A.shape == (W.shape[0], A.shape[1]))
    cache = (linear_cache, activation_cache)

    return A, cache

# ------------------------------------------
# 为了实现L层神经网络更加方便，需要将前L-1层的激活函数设置为ReLU，最后一层输出层激活函数设置为Sigmoid
def L_model_forward(X, parameters):
    """
    实现前向传播： the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation
    
    Arguments:
    X -- 初始数据, numpy array of shape (输入层大小, 样本数量)
    parameters -- 初始化deep网络的参数输出
    
    Returns:
    AL -- 上一层激活后的值
    caches -- cache的列表:
                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)
                the cache of linear_sigmoid_forward() (there is one, indexed L-1)
    """

    caches = []
    A = X
    L = len(parameters) // 2
    # 前L-1层为relu激活函数
    for i in range(1, L):
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters["W" + str(i)], parameters["b" + str(i)], "relu")
        caches.append(cache)
    # 最后一层为sigmoid激活函数
    AL, cache = linear_activation_forward(A, parameters["W" + str(L)], parameters["b" + str(L)], "sigmoid")
    caches.append(cache)

    assert(AL.shape == (1, X.shape[1]))

    return AL, caches

# ------------------------------------------
def compute_cost(AL, Y):
    """
    计算代价函数 使用Logistic回归中使用的代价函数

    Arguments:
    AL -- 对应于标签的预测概率向量, shape (1, 样本数)
    Y -- 正确的样本 (for example: containing 0 if non-cat, 1 if cat), shape (1, 样本数)

    Returns:
    cost -- 交叉熵代价
    """ 

    m = Y.shape[1]
    cost = -(np.dot(np.log(AL), Y.T) + np.dot(np.log(1 - AL), (1 - Y).T)) / m

    cost = np.squeeze(cost) # 使得cost的维度是我们想要的（比如将[[17]]变成17）
    assert(cost.shape == ())

    return cost

# ------------------------------------------
def linear_backward(dZ, cache):
    """
    单层实现反向传播的线性部分(l层)

    Arguments:
    dZ -- 代价函数对于线性输出的梯度 (l层)
    cache -- 元组(A_prev, W, b) 来自当前层的前向传播

    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    A_prev, W, b = cache
    m = A_prev.shape[1]

    dW = np.dot(dZ, A_prev.T) / m
    db = np.sum(dZ, axis=1, keepdims=True) / m 
    dA_prev = np.dot(W.T, dZ)

    assert(dA_prev.shape == A_prev.shape)
    assert(dW.shape == W.shape)
    assert(db.shape == b.shape)

    return dA_prev, dW, db

# ------------------------------------------
def linear_activation_backward(dA, cache, activation):
    """
    实现 线性——>激活 过程的反向传播
    
    Arguments:
    dA -- 当前层l激活后的梯度
    cache -- 元组 (linear_cache, activation_cache) 为了有效计算后向传播而存储
    activation -- 当前层的激活函数, stored as a text string: "sigmoid" or "relu"
    
    Returns:
    dA_prev -- 代价函数对于激活的梯度(l-1层), 和A_prev相同的维度,和A_prev相同的维度
    dW -- 代价函数对于W的梯度 (l层), 和W相同的维度
    db -- 代价函数对于b的梯度 (l层), 和b相同的维度
    """

    linear_cache, activation_cache = cache

    if activation == "relu":
        dZ = relu_backward(dA, activation)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    elif activation == "sigmoid":
        dZ = sigmoid_backward(dA, activation_cache)
        dA_prev, dW, db = linear_backward(dZ, linear_cache)

    return dA_prev, dW, db

# ------------------------------------------
def L_model_backward(AL, Y, caches):
    """
    前L-1层为ReLU激活函数,最后一层为sigmoid函数的后向传播实现
    
    Arguments:
    AL -- 前向传播输出的概率向量 (L_model_forward())
    Y -- 真实值的向量 (containing 0 if non-cat, 1 if cat)
    caches -- 包含cache的列表:
                every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)
                the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1])
    
    Returns:
    grads -- 带有渐变值的字典
             grads["dA" + str(l)] = ... 
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ... 
    """

    grads = {}
    L = len(caches)
    m = AL.shape[1]
    Y = Y.reshape(AL.shape) # 经过这一行转化，Y的维度和AL维度相同

    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))

    current_cache = caches[L - 1]
    grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = linear_activation_backward(dAL, current_cache, activation="sigmoid")

    for i in reversed(range(L-1)):
        current_cache = caches[i]
        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads["dA" + str(i + 2)], current_cache, activation="relu")
        grads["dA" + str(i + 1)] = dA_prev_temp
        grads["dW" + str(i + 1)] = dW_temp
        grads["db" + str(i + 1)] = db_temp

    return grads

# ------------------------------------------
def update_parameters(parameters, grads, learning_rate):
    """
    使用梯度下降更新参数
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients, output of L_model_backward
    
    Returns:
    parameters -- 更新后的参数字典
                  parameters["W" + str(l)] = ... 
                  parameters["b" + str(l)] = ...
    """   

    L = len(parameters) // 2 # 神经网络中的层数

    for i in range(1, L + 1):
        parameters["W" + str(i)] -= learning_rate * grads["dW" + str(i)]
        parameters["b" + str(i)] -= learning_rate * grads["db" + str(i)]

    return parameters



```



### 有效运行神经网络

深度学习网络是一个需要迭代得到结果的模型。它的超参数调整过程：想法->编码->实验->修改想法，需要不断地尝试，才能学习到调参的经验。

#### 训练集和测试集划分

我们一般将数据分为三个部分：

1.训练集：为训练模型准备的数据

2.验证集：通过交叉验证集选择最优模型

3.测试集：对模型进行评估

> 划分训练测试集最常见的比例是什么？

如果明确指定验证集，一般训练和测试集的比例为7:3；如果指定验证集，那么一般比例为6:2:2。这一般在数据量在10000条以下都是最好的划分比例。但在大数据时代，如果数据总量比较大，比如是百万条，那么验证集和测试集的比例还需要减少，比如98:1:1，也是合理的。

*需要特别注意的是：要保证验证集和测试集来自同一分布，这样会使得你的机器学习算法变得更快。*

#### 偏差和方差

根据数据集的分布状况，可以分为以下三种：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img10.jpg)

如上图，最左边使用Logistic回归，没有大部分正确分类，属于“欠拟合”的状况；最右边使用比较复杂的神经网络，完整地分出了两个类别，属于”过拟合“的情况（因为部分输入数据是不合理的）；中间只有和别数据分类错误，这叫“适度拟合”，是我们比较追求的一种状态。

上图这种只有一个或者两个特征的二维数据中，可以绘制数据，将偏差和方差可视化。但在多维空间数据中，绘制数据和可视化分割边界无法实现。

> 我们在多维空间通常通过两个个指标,来研究偏差和方差：训练集误差和测试集误差。为了便于研究，假设人眼分辨的错误率为0，这也被称为基本误差或者最优误差；假设训练集和验证集来自同一分布。如果训练集误差为0.01，测试集误差为0.11，这种情况很有可能是我们过度拟合了训练集，称之为高方差，对应于上图最右边的情况；如果训练集误差为0.15，测试集误差为0.16，这种情况很有可能是我们训练数据的时候欠拟合，称之为高偏差，对应于上图最左边的情况；如果训练集误差为0.15，测试集误差为0.3，偏差和方差都比较高，这种情况是因为你的算法模型并不适合这个任务，需要改变模型；如果训练集误差为0.005，测试集误差为0.01，偏差和方差都很低，是分类效果比较好的情况，对应上图中间的情况。

#### 模型评估调优的过程

* 首先，将进行训练之后的模型用于评估训练集的性能，如果误差高，代表欠拟合。那么你要采取的方法可能是增加训练时间或者增大网络结构或者是选择一个新网络，先将偏差降下来，拟合训练数据。（这在基本误差不同的情况下会有所不同）
* 如果模型的训练集性能很高后，可以将其用于评估测试集，如果误差高，代表过拟合。那么你要采取的方法最好是采用更多的训练数据，如果无法获得更多数据，可以通过正则化来减少过拟合，降低方差，有时候也会通过替换网络结构来实现。

#### 正则化

如果你的模型在评估是，由比较高的方差，然而你又不能拿到更多的数据集，那么我们最先想到的办法可能是正则化。

正则化有助于避免数据过度拟合，减少网络误差。

* Logistic正则化

假设我们的目标是找到$w,b$来使得$J(w,b)$达到最小值，且使用逻辑回归的代价函数，那么
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})
$$
我们在该函数中加入正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2m}{\Vert w \Vert_2}^2\\
{\Vert w \Vert_2}^2=\sum^{n_x}_{j=1}w_{j}^2=w^Tw \quad 这被称为L2正则化
$$
同理也会有L正则化
$$
J(w,b)=\frac1m\sum^m_{i=1}L(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{m}{\Vert w \Vert_1}\\
{\Vert w \Vert_1}=\sum^{n_x}_{j=1}\lvert w_j \rvert \quad 这被称为L1正则化
$$
通常我们都会使用`L2正则化`来实现降低方差的效果。

>那么$\lambda$的值我们需要如何确定呢？

我们通常使用验证集或者交叉验证来配置$\lambda$参数，不过首先要考虑训练集之间的权衡，吧参数$w,b$正常值设为较小的值，避免过拟合，不断调整超参数$\lambda$的值来减小方差。

*需要特别说明的是在编写代码的时候，python语言中lambda是一个保留字段，所以编程时我们通常使用lambd来代替lambda。*

* 神经网络正则化

$$
J(W^{[1]},b^{[1]},\dots,W^{[L]},b^{[L]})=\frac1m\sum^n_{i=1}L(\hat{y}^{(i)},y^{(i)})+ \frac{\lambda}{2m}\sum^L_{l=1}{\Vert W^{[l]} \Vert}^2\\
{\Vert W^{[l]} \Vert}^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(W_{i,j}^{[l]})^2\quad 该矩阵范数被称为是弗罗贝尼乌斯范数\\
W的维度是(n^{[l]},n^{[l-1]})
$$

如果$J(W,b)$发生了改变，那么反向传播的$dW^{[l]}$也要发生变化：

在`backprop`计算出的$dW$的基础上加一个$\frac{\lambda}{m}W^{[l]}$,然后用此更新$W^{[l]}$的值，这样做的结果是的$W^{[l]}$会比没有正则化之前更小。因此`L2范数`也被称为`权重衰减`。

#### 正则化如何预防过拟合

如果正则化的参数$\lambda$如果设置的过大，$W^{[l]}$会变得更小，就会导致每层上的部分$w$的权重 会接近0，这相当于将部分隐藏单元给去除了，复杂的神经网络会退化称为一 个很深但是又很简单的网络，导致从过拟合直接变成欠拟合的状态。

 一个合适的$\lambda$值能够使模型的性能从过拟合到适度拟合。

>$\lambda$越大，得到迭代的$W$就越小，这相当于让部分隐藏单元的影响变小，降低模型的拟合程度，方差减小。 
>
>总体来说，正则化的效果其实就是将复杂的网络线性化，如果$\lambda$的取值设置的比较好，能达到适度拟合的效果。

#### dropout正则化

`dropout正则化`是指如果一个网络存在过拟合的情况下，可以将所有节点（隐藏单元）设置一个删除概率为0.5，那么保留该隐藏单元的概率`keep-prob`也为0.5，这个值是可以改变的，然后随机消除一些节点并删除该节点进出的连线，得到一个节点更少，规模更小的网络。如下图所示：

![](https://blog-1311257248.cos.ap-nanjing.myqcloud.com/imgs/deep-learning%26computer-vision/img11.jpg)

实现Dropout的方法很多，最常用的是`inverted dropout`。以一个深层神经网络的某一个隐藏层为例来解释怎么进行Dropout正则化。首先假设对于第$l$层，其激活函数为$a^{[l]}$，我们设置的保留概率`keep_prob`等于0.8，这意味着该隐藏层的所有神经元以0.8的概率得到保留。

可以将`inverted dropout`方法归纳为四步：

>1.根据`keep_prob`生成和 $a^{[l]}$相同的随机概率矩阵$d^{[l]}$，`Dl = np.rndom.randn(Al.shape[0], Al.shape[1])`
>
>2.将$d^{[l]}$转化为0-1矩阵， `Dl = Dl < keep_prob`
>
>3.将$a^{[l]}$和$d^{[l]}$ 中的元素一一对应相乘，$d^{[l]}$为1代表对应的神经元被保留，$d^{[l]}$为0则代表舍弃， `Al = np.muiltiply(Al, Dl)`
>
>4.为了确保$a^{[l]}$的期望值不变，将$a^{[l]}$除以`keep_prob`,`Al = Al / keep_prob`

需要注意的是，在反向传播的时候，也需要像上面一样进行dropout操作，和前向传播关闭相同的神经元。即对于某一层的$da^{[l]}$，应该进行以下计算：

> 1.`dAl =dAl * Dl` 
>
> 2.`dAl = dAl / keep_prob`

另一点是，Dropout正则化只在训练阶段实施，在测试阶段只需要利用训练好的参数进行正向预测，而不需要进行神经元的随机失活。

三层网络的前向传播`dropout`示例代码：

```python
keep_prob = 0.8
def foward(X):
    # 3层neural network的前向传播
    A1 = np.maximum(0, np.dot(W1, X) + b1)  # 计算第一层网络的输出
    D1 = (np.random.rand(*A1.shape) < keep_prob)  # 以keep_prob为标准，判断该层结点哪些可以保留
    Z1 = np.multiply(A1, D1)     #dropout
    Z1 /= keep_prob   # 为了期望的一致，除以keep_prob

    A2 = np.maximum(0, np.dot(W2, Z1) + b2)
    D2 = (np.random.rand(*A2.shape) < keep_prob)
    Z2 = np.multiply(A2, D2) 
    Z2 /= keep_prob

    out = np.dot(W3, Z2) + b3

```

